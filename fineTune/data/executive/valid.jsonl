{"messages": [{"role": "user", "content": "Give me an executive summary:\ncapture fundamen- **0**\ntal properties that must hold for valid reasoning, including logical coherence (not simultaneously believing \"the design is stable\" and \"the design will collapse\"), compositionality (understanding \"red LEGO\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||||~~Score~~<br>~~Present~~<br>Partially Present|~~Score~~<br>~~Present~~<br>Partially Present|~~Score~~<br>~~Present~~<br>Partially Present|~~Score~~<br>~~Present~~<br>Partially Present|~~Score~~<br>~~Present~~<br>Partially Present|\n||||||||||||||||||||\n||||||||||||||||||||\n\ncockpit\" by combining concepts of color, Figure 2: Distribution of cognitive element presence across\nmaterial, and function). **2)** **Meta-** 1,598 arXiv LLM Reasoning papers. _Partially present_ indi**cognitive controls** encompass the execu- cates that there is evidence that the behavior was considered\ntive functions that select and monitor rea- in the design (motivation, method, evaluation) of the paper but\nsoning strategies, such as recognizing when was not the primary focus. _Present_ indicates that there is eviyou lack necessary pieces (self-awareness), dence that the behavior was a conscious and significant design\ndeciding whether to plan the entire de- decision. Details provided in Section 5.\nsign upfront versus building exploratively\n(strategy selection). **3) Reasoning representations** desrcibe how knwoledge is organized: hierarchically\n(decomposing \"spaceship\" into \"body,\" \"wings,\" \"cockpit\"), causally (understanding that inadequate support\ncauses structural collapse), or spatially (tracking how pieces connect in 3D space). **4) Reasoning oper-**\n**ations** specify the procedures that construct and transform these representations, such as verifying each\nconnection works, backtracking when a wing design fails, or abstracting the principle that heavier components need more support points. Organizing these theories through Marr\u2019s (1982) levels of analysis (focusing\non computational and algorithmic levels) yields 28 specific cognitive elements spanning the space of human\n\nDistribution of Cognitive Elements Considered by LLM Reasoning Papers"}, {"role": "assistant", "content": "**Verdict:** Essential framework for assessing LLM robustness; current LLMs likely fall short, indicating a clear development opportunity.\n\n**Summary:** This section defines critical cognitive properties\u2014logical consistency, compositional understanding, self-monitoring (metacognition), flexible knowledge organization, and systematic operational procedures\u2014that are fundamental for reliable and robust LLM reasoning. The accompanying data (Figure 2) strongly suggests these capabilities are often either 'partially present' or entirely absent in current LLM designs. This deficiency means existing models are less reliable for complex, multi-step tasks, increasing the need for human oversight and validation. Implementing these properties will necessitate more sophisticated model architectures, increased computational resources for internal state management and explicit reasoning steps, and potentially novel training paradigms. However, the utility is significant: achieving these would yield LLMs capable of more autonomous, verifiable, and generalizable reasoning, reducing operational costs associated with errors and manual intervention, and unlocking higher-value applications currently out of reach. This outlines a clear roadmap and a substantial investment opportunity for next-generation AI."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n_\ud835\udc5e_ \u2265 2 \u00d7 _\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52_\ud835\udc51\ud835\udc52\ud835\udc59\ud835\udc4e\ud835\udc66\ud835\udc60_ [ _\ud835\udc5f_ ] **then** /* cwnd backoff once queuing delay exceeds threshold */\n\n**23** _\ud835\udc4f\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc5c\ud835\udc53\ud835\udc53_ _ _\ud835\udc53\ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc5c\ud835\udc5f_ \u2190 0 _._ 5 \u00d7 _\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc45\ud835\udc47\ud835\udc47_ [ _\ud835\udc5f_ ]/ _\ud835\udc5f\ud835\udc61\ud835\udc61_\n\n**24** _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ [ _\ud835\udc5f_ ] \u2190 _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ [ _\ud835\udc5f_ ] \u00d7 _\ud835\udc4f\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc5c\ud835\udc53\ud835\udc53_ _ _\ud835\udc53\ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc5c\ud835\udc5f_\n\n**25** _\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52_ _ _\ud835\udc51\ud835\udc52\ud835\udc59\ud835\udc4e\ud835\udc66\ud835\udc60_ [ _\ud835\udc5f_ ] \u2190 0\n\n**26** _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ [ _\ud835\udc5f_ ] \u2190 max{2 _,\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ [ _\ud835\udc5f_ ]} // lower bound\n\n**Adjust_Weights():**\n\n**27** _\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59_ _ _\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52_ \u2190 [\ufffd] _\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc56\ud835\udc59\ud835\udc56\ud835\udc4f\ud835\udc5f\ud835\udc56\ud835\udc62\ud835\udc5a_ _ _\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc60_ ;"}, {"role": "assistant", "content": "**Verdict:** Standard, critical component. Not a differentiator, but non-negotiable for system stability.\n\n**Summary:** This section outlines a fundamental congestion control algorithm. Its utility is extremely high, as it prevents network saturation and ensures fair resource allocation, which is essential for stable system performance under varying load. It dynamically throttles data throughput when queuing delays exceed acceptable thresholds, using round-trip time variations to calibrate the backoff intensity. There are no significant hardware costs or unusual requirements; it's a software-only feature. While not a novel concept that would drive investment on its own, its robust and efficient implementation is a prerequisite for any reliable high-performance network system and directly impacts user experience and operational reliability."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n_each new unit of product i added from exogenous shock_ **do**\n\n**8** assign this new unit into batch _Bi,hi_ +1"}, {"role": "assistant", "content": "**Verdict:** Foundational operational logic, not a value-add innovation on its own.\n\n**Summary:** This describes a basic, low-overhead mechanism for categorizing and sequentially allocating new units of a product type into distinct batches. It ensures that each new arrival of a specific product type receives a unique, incremented batch identifier, preventing collisions and enabling clear chronological or logical grouping. This is critical for data integrity, tracking, and efficient downstream processing or inventory management. Hardware requirements are negligible, and computational costs are minimal, involving simple counter increments and data assignment. Its utility lies in providing a robust, simple system for managing dynamic incoming data streams by type."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\nWe evaluate our method on various datasets, including ImageNet-1k [12] and ImageNet-100 [42] for\nour primary results, Spawrious [26] and Waterbirds [38] for a study on adversarial datasets, Stanford\nDogs [21] and CUB-200-2011 [45] for fine-grained visual classification, and ArtBench [24] to test\nthe method\u2019s out-of-distribution capabilities. The majority of our experiments use four pre-trained\nself-supervised feature extractors as backbones: CLIP [34], DINO-v2 [31], EVA-02 [14], and MoCov3 [10]. In all our experiments, we distill the given dataset for 5000 iterations before training linear\nprobes to convergence on the resulting synthetic images. All experiments are conducted at 224 _\u00d7_ 224\nresolution and use the \u201cViT-B\u201d version of the given model. All distilled datasets by default use 10\nsets of augmentations per batch except for ImageNet-1k, for which only 3 sets of augmentations are\nused due to compute constraints. For further implementation details, please see the Appendix.\n\n**Evaluation and Baselines.** To measure our method\u2019s performance on a given feature extractor,\nwe randomly initialize a linear classifier and optimize it to convergence using the distilled images\nbefore evaluating on the test set. The same procedure is used to evaluate real-image baselines. In\nprior works on self-supervised learning [31], the evaluation consists of a grid search across a number\nof hyper-parameters, including from which layer(s) the features used to train the linear probe should\nbe taken from. For simplicity, we instead use only the features from the backbone\u2019s _last layer_ and\nkeep the training hyper-parameters consistent across the training of all linear probes.\n\nWe compare our method against three real-image baselines. For the **Neighbors** baseline, we choose\nthe real image for each class whose _distill-model_ embedding is closest to that of the corresponding\nsynthetic image produced by our method. Similarly, for **Centroids**, we take the real training image\nwith embedding closest to the _mean_ embedding for each class. Lastly, for **Random**, we simply select\na random image for each class and average the performance over 10 different seeds.\n\n5\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-4-0.png)\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-5-1.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-5-0.png)\n\nFigure 3: Performing more rounds of differentiable\naugmentation on the synthetic data during each distillation step improves both the single-model and crossmodel performance of the distilled images.\n\n|Distil<br>Centr|Col2|Col3|Distil<br>Centr|\n|---|---|---|---|\n|30<br>20<br>10|0|10<br>20|10<br>20|\n\nFigure 4: We distill ImageNet-Fruits and observe the\nPCA of the training image embeddings. Each color\nrepresents a class. Note that the distilled images typically lie on the edge or outside of their class\u2019s cluster.\n\n**4.1** **Linear Gradient Matching Out-Performs Real-Image Baselines**\n\nFirst, we evaluate our method on the original dataset distillation task: using a backbone model\nto distill a dataset and then training a model _of the same architecture_ on the resulting synthetic\nimages. Note that this setting differs from previous dataset distillation works in that we use a\n_pre-trained backbone_ distillation and reuse that backbone during evaluation by training a new\nrandomly-initialized linear classifier on top of it.\n\nIn Table 1, we see results for the single-model setting when distilling ImageNet-100 and ImageNet-1k.\nAcross both datasets and all four models, **our method convincingly out-performs all real-image**\n**baselines** . In particular, our method enables a linear probe trained on top of DINO-v2 [31] to reach\n**75% test accuracy** on ImageNet-1k while only ever having seen a **single labeled image per class** .\nIn comparison, training on the full dataset of 1.3 million real images reaches just 7 points higher at\n83%. Similar results are seen for the other models and datasets as well.\n\nIn Figure 4 we visualize the _embeddings_ of the distilled images relative to the real training data\nby plotting the 2D principal component analysis (PCA). For the sake of visual clarity, we use\nImageNet-Fruits [6, 7], a toy dataset of 10 fruit classes from ImageNet-1k. The embeddings in this\nfigure are from the same model used to distill the dataset (DINO-v2). We observe that the embeddings\nof the distilled images tend to lie far away from their respective class\u2019s centroid, often falling on\nthe outside edge of the class\u2019s distribution. We hypothesize that this is due in-part to the distillation\nembedding _highly-discriminative_ features within the synthetic images.\n\n**4.2** **Distilled Images Generalize Across Models**\n\nNext, we investigate the cross-model generalization capabilities of our distilled datasets. That is, we\nfirst distill a dataset using one backbone model (e.g., CLIP) and then evaluate the synthetic images\u2019 performance on the _other_ models (DINO-v2, EVA-02, MoCo-v3) that were not seen during distillation.\n\nWe visualize these results in Table 2. The diagonal elements are equivalent to the first row of Table 1\nand represent single-model performance while the off-diagonals show cross-model capabilities. We\nsee that the distilled datasets tend to generalize well to unseen models save for an outlier between\nCLIP and Moco, possibly due to poor model _alignment_ (Section 4.6). We also observe that the highestaccuracy model on the full dataset (DINO) also has the distilled dataset with the best cross-model performance, suggesting that a model\u2019s quality affects the generalization capabilities of its distilled data.\n\n**4.3** **Regularization and Augmentation Enable High Performance**\n\nIn this section, we quantitatively (Figure 3) and qualitatively (Table 5) analyze the effects of the\nvarious regularization and augmentation techniques included in our distillation process. Table 3 shows\nthe effects of ablating various components when evaluating using the same model used to distill ( **top** )\n\n6\n\nTrain Set ImageNet-100\n(1 Img/Cls)\n\nCLIP DINO-v2 EVA-02 MoCo-v3 Average\n\nFull (Ours) **84.9** _\u00b1_ **0.1 91.5** _\u00b1_ **0.1 89.0** _\u00b1_ **0.0 83.4** _\u00b1_ **0.1 87.2** _\u00b1_ **0.1**\n-Decorrelate 82.6 _\u00b1_ 0.1 **91.3** _\u00b1_ **0.2 89.0** _\u00b1_ **0.1** 83.2 _\u00b1_ 0.1 86.5 _\u00b1_ 0.1\n\n-Pyramid 83.5 _\u00b1_ 0.2 91.0 _\u00b1_ 0.3 87.9 _\u00b1_ 0.1 80.5 _\u00b1_ 0.1 85.7 _\u00b1_ 0.1\n-Augment 58.4 _\u00b1_ 0.2 82.6 _\u00b1_ 0.4 74.0 _\u00b1_ 0.3 59.6 _\u00b1_ 0.5 68.6 _\u00b1_ 0.4\n\nFull (Ours) **75.4** _\u00b1_ **0.3 80.9** _\u00b1_ **0.1** 76.6 _\u00b1_ 0.1 78.2 _\u00b1_ 0.1 **77.8** _\u00b1_ **0.2**\n-Decorrelate 69.4 _\u00b1_ 0.4 79.5 _\u00b1_ 0.1 **76.8** _\u00b1_ **0.1 79.9** _\u00b1_ **0.2** 76.4 _\u00b1_ 0.2\n\n-Pyramid 57.3 _\u00b1_ 0.2 74.5 _\u00b1_ 0.2 68.5 _\u00b1_ 0.2 68.1 _\u00b1_ 0.2 67.1 _\u00b1_ 0.2\n-Augment 35.3 _\u00b1_ 0.4 31.6 _\u00b1_ 0.2 34.3 _\u00b1_ 0.5 32.2 _\u00b1_ 0.5 33.3 _\u00b1_ 0.4\n\nFull Dataset 92.5 _\u00b1_ 0.0 95.2 _\u00b1_ 0.1 94.1 _\u00b1_ 0.1 89.4 _\u00b1_ 0.3 92.8 _\u00b1_ 0.1\n\nTable 3: **Evaluating Ablations:** While all three components\nprovide improvements, the Augmentation has the most dramatic effect, especially in the cross-model setting. Likewise,\nthe Pyramid optimization seems to matter more in the crossmodel setting than the same-model setting by mitigating overfitting to the model used during distillation.\n\nFigure 5: **Visualizing Ablations:** Removing\nvarious components of our pipeline causes\nvisual degradation in the distilled images.\n\nand averaging performances across the other three models ( **bottom** ). Figure 5 visualizes the distilled\nPineapple class from ImageNet-100 under the same ablations.\n\n**Color Decorrelation** Inspired by prior work in feature visualization [30] and motivated by a desire\nto spare our synthetic images from any color-related biases imposed by the model used to distill\nthem, we apply a color decorrelation technique to our distillation process. In Figure 5, we see that the\nimages distilled without the color decorrelation (column 2) look less realistic than those using our\nfull method (column 1); they are over saturated and contain high levels of incorrect colors (blue in\nthis example). Quantitatively, however, this component has the lowest effect of the three analyzed in\nthis section. While generally slightly helpful, it only offers a large improvement in the cross-model\nsetting when distilling using CLIP.\n\n**Pyramid Representation** It has been shown in prior work [7] that pixel-based optimization for\ndataset distillation does not scale well to higher resolutions; the synthetic images tend to become\nriddled with high-frequency patterns that inhibit their usefulness as training data. As discussed in\nSection 3.2, we instead adopt a pyramid representation for the distillation process. In Figure 5, we\nsee samples distilled from ImageNet-100 optimized using either the pyramid (column 1) or na\u00efve\npixel representation (column 3). We encourage the reader to zoom in and observe the high-frequency\npatterns and overall lack of coherence in the pixel-based images. In Table 3, we see that the images\ndistilled without the pyramid representation do indeed make far worse training data in the cross-model\nsetting, causing the model to overfit to the high-frequency patterns.\n\n**Differentiable Augmentations** Since its first proposal [53], the incorporation of differentiable\naugmentations during the distillation process has proven critical to the efficacy of the synthesized\nimages as training data. As such, we also perform differentiable augmentations in this work, as\ndescribed in Section 3.3. In Figure 5, we see samples from ImageNet-100 distilled both with (column 1) and without (column 4) augmentations. Visually, we observe that the images distilled _without_\naugmentations lack much meaningful structure and appear to simply be blobs of color with just a hint\nof geometry. In Table 3, we see that the lack of augmentations _severely_ limits the synthetic images\u2019\nusefulness as training data, even when evaluation on the same model used to distill.\n\nAs earlier discussed, we also apply _multiple rounds_ of augmentations at each distillation step and\nconcatenate the multiple augmented versions together to form the synthetic batch. Figure 3 illustrates\nhow the number of _augmentations per batch_ affects both the single-model and cross-model performance of the distilled data. Specifically, we see that when distilling ImageNet-100 with DINO-v2,\nraising the number of augmentations per batch from 1 to 5 and eventually 10 steadily increases the\ndistilled images\u2019 effectiveness as training data both on the backbone model used to distill (DINO-v2)\nand the other unseen models (CLIP, EVA-02, MoCo-v3).\n\n7\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-3.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-4.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-5.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-6.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-7.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-8.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-9.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-10.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-14.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-15.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-16.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-17.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-18.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-19.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-20.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-21.png)\n\nFigure 6: **Distilling Datasets with Spurious Correlations:** The 4-class \u201cSpawrious\u201d dataset contains spurious\nbackground correlations in the training set that are then subverted in the test set ( **left** ). A DINO linear\nprobe trained on the full training set performs well, reaching 78% test accuracy, while a MoCo probe fails\ncatastrophically, only reaching 36%. The distilled images ( **right** ) hint as to why: those distilled with DINO-v2\nstill contain mostly decipherable dog breeds while the MoCo-v3 counterparts focus almost entirely on the\nbackground environments. These images likely reflect the same biases held by the models used to distill them.\n\nTrain Set Spawrious WaterBirds\n(1 Img/Cls)\n\nCLIP DINO-v2 EVA-02 MoCo-v3 Average CLIP DINO-v2 EVA-02 MoCo-v3 Average\n\nDistilled (Ours) **43.1** _\u00b1_ **6.4 80.8** _\u00b1_ **3.1 36.5** _\u00b1_ **3.1 32.7** _\u00b1_ **2.3 48.3** _\u00b1_ **3.7 77.9** _\u00b1_ **0.2 82.1** _\u00b1_ **2.9 78.0** _\u00b1_ **0.8 77.8** _\u00b1_ **0.0 79.0** _\u00b1_ **1.0**\nNeighbors **41.7** _\u00b1_ **4.6 76.9** _\u00b1_ **1.2 40.7** _\u00b1_ **2.7 33.0** _\u00b1_ **3.4 48.1** _\u00b1_ **3.0 74.6** _\u00b1_ **6.5** 67.3 _\u00b1_ 5.8 **74.3** _\u00b1_ **3.3** 45.2 _\u00b1_ 11.4 65.3 _\u00b1_ 6.8\nCentroids **44.8** _\u00b1_ **5.0 80.2** _\u00b1_ **2.7 38.1** _\u00b1_ **3.3 30.5** _\u00b1_ **1.6 48.4** _\u00b1_ **3.2 69.8** _\u00b1_ **8.6** 65.7 _\u00b1_ 3.4 64.6 _\u00b1_ 7.9 62.0 _\u00b1_ 5.2 65.5 _\u00b1_ 6.3\n\nRandom **46.2** _\u00b1_ **4.4** 68.1 _\u00b1_ 8.1 **33.3** _\u00b1_ **5.3 31.8** _\u00b1_ **2.5 44.9** _\u00b1_ **5.1** 71.9 _\u00b1_ 4.5 57.1 _\u00b1_ 8.6 59.8 _\u00b1_ 13.3 67.4 _\u00b1_ 5.7 64.0 _\u00b1_ 8.0\n\nFull Dataset 50.4 _\u00b1_ 0.3 78.1 _\u00b1_ 0.1 50.1 _\u00b1_ 0.4 35.8 _\u00b1_ 0.0 53.6 _\u00b1_ 0.2 86.0 _\u00b1_ 0.1 95.5 _\u00b1_ 0.1 90.4 _\u00b1_ 0.2 74.3 _\u00b1_ 0.1 86.5 _\u00b1_ 0.1\n\nTable 4: **Performance on Datasets with Spurious Background Correlations:** The Spawrious ( **left** ) and Waterbirds ( **right** ) training sets contain intentionally adversarial background correlations that are then subverted in the\ntest sets. On Spawrious, our method no longer out-performs the real-image baselines as with the standard datasets\n(Table 1). This is perhaps due to the synthetic data adopting the models\u2019 biases and overfitting to the backgrounds\non the training sets. We also see interesting interpretability results in the images themselves (Figure 6).\n\n**4.4** **Distilling Adversarial Datasets Reveals Interpretable Model Weaknesses**\n\nThe \u201cSpawrious\u201d dataset [26] consists of four classes of dog breeds and is designed to evaluate a\nmodel\u2019s ability to focus on the relevant content of an image rather than spurious correlations. In the\ntraining data, each breed is paired with its own unique environment: (Bulldog, Forest), (Corgi, Desert),\n(Dachshund, Mountain), and (Labrador, Snow). However, in the test set, the environments are intentionally different: (Bulldog, Mountain), (Corgi, Forest), (Dachshund, Snow), and (Labrador, Desert).\nWe also experiment with a similarly constructed data, \u201cWaterbirds\u201d [38] consisting of just two classes\n(land birds and water birds) with similarly spurious background correlations.\n\nQuantitative results on these datasets are shown in Table 4. Our method generally outperforms the\nreal-image baselines, but to a significantly smaller extent than on the standard datasets (Table 1). We\nalso observe that when training on the full Spawrious dataset, DINO-v2 [31] _far_ out-performs the\nrest of the models (81% test accuracy) with MoCo-v3 [10] doing dramatically worse (37%), both to\ndegrees far greater than seen when training on standard datasets (as in Table 1).\n\nBy _distilling_ Spawrious with our method, we gain potential insights as to why we might see such\ndramatic results. As seen in Figure 6, the images distilled with DINO-v2 contain clear (albeit\nabstract) depictions of the correct dog breeds. On the other hand, those yielded by MoCo-v3 are\nalmost entirely indecipherable, with the only clear portions being the spurious backgrounds from the\ntraining set. This interpretability result gives a clue towards explaining MoCo-v3\u2019s poor performance\non this dataset; the model clearly seems to focus on the spurious background correlations present\nin the training set rather than the relevant subjects, causing catastrophic overfitting.\n\n**4.5** **Distillation Excels in Fine-Grained Visual Classification**\n\nWhile computer vision methods are typically benchmarked against datasets containing broad categories of objects, such as ImageNet [12], much research also focuses on the task of Fine-Grained\nVisual Classification (FGVC) wherein datasets consist of many closely related classes that can be\n\n8\n\nFigure 7: **Distilling Fine-Grained Datasets:** Our distillation method captures the details necessary to teach a\nclassifier to distinguish between highly similar classes. Pictured above are just 10 of the 120 classes distilled\nfrom the Stanford Dogs [21] dataset ( **top** ) and 10 of the 200 classes distilled from Caltech-UCSD Birds [45]\n[using DINO-v2. For the full distilled datasets, please see the Appendix or our project page.](http://georgecazenavette.github.io/linear-gm)\n\nTrain Set Stanford Dogs CUB-2011\n(1 Img/Cls)\n\nCLIP DINO-v2 EVA-02 MoCo-v3 Average CLIP DINO-v2 EVA-02 MoCo-v3 Average\n\nDistilled (Ours) **52.1** _\u00b1_ **0.2 83.0** _\u00b1_ **0.1 74.8** _\u00b1_ **0.1 69.6** _\u00b1_ **0.2 69.9** _\u00b1_ **0.2 62.2** _\u00b1_ **0.2 86.0** _\u00b1_ **0.1 74.1** _\u00b1_ **0.2 42.5** _\u00b1_ **0.2 66.2** _\u00b1_ **0.2**\nNeighbors 33.4 _\u00b1_ 0.1 71.3 _\u00b1_ 0.2 58.5 _\u00b1_ 0.2 56.3 _\u00b1_ 0.1 54.9 _\u00b1_ 0.2 39.4 _\u00b1_ 0.1 76.9 _\u00b1_ 0.0 52.6 _\u00b1_ 0.3 28.1 _\u00b1_ 0.0 49.2 _\u00b1_ 0.1\nCentroids 43.3 _\u00b1_ 0.1 73.0 _\u00b1_ 0.2 60.9 _\u00b1_ 0.2 55.2 _\u00b1_ 0.2 58.1 _\u00b1_ 0.2 54.3 _\u00b1_ 0.2 78.5 _\u00b1_ 0.2 59.9 _\u00b1_ 0.3 30.2 _\u00b1_ 0.1 55.7 _\u00b1_ 0.2\n\nRandom 23.3 _\u00b1_ 1.5 51.9 _\u00b1_ 1.8 38.3 _\u00b1_ 1.8 36.6 _\u00b1_ 1.4 37.5 _\u00b1_ 1.6 37.5 _\u00b1_ 1.6 64.4 _\u00b1_ 1.5 44.3 _\u00b1_ 1.5 19.1 _\u00b1_ 0.5 41.3 _\u00b1_ 1.3\n\nFull Dataset 76.9 _\u00b1_ 0.1 88.6 _\u00b1_ 0.1 82.6 _\u00b1_ 0.1 72.3 _\u00b1_ 0.5 80.1 _\u00b1_ 0.2 77.5 _\u00b1_ 0.7 90.2 _\u00b1_ 0.2 84.0 _\u00b1_ 0.3 43.7 _\u00b1_ 0.8 73.8 _\u00b1_ 0.5\n\nTable 5: **Performance on Fine-Grained Datasets:** Our distillation method captures the most discriminative\naspects of each class, thereby enabling a down-stream classifier to correctly identify samples from datasets\nwhere all classes are closely related. In particular, the performance gap between our method and the real-image\nbaselines is even higher on these fine-grained datasets (Stanford Dogs [21] and CUB-200-2011 [45]) than on the\nstandard ImageNet benchmarks.\n\nchallenging for even human experts to distinguish. Such datasets require models to learn fine-grained\nfeatures in order to identify the subtle differences between each class.\n\nWe apply our linear method to two common FGVC datasets: Stanford Dogs [21] and Caltech-UCSD\nBirds-200-2011 [45]. Samples of these datasets distilled using DINO-v2 can be seen in Figure 7. As\nseen in Table 5, in this more challenging fine-grained setting, our method outperforms the real-image\nbaselines to a _significantly higher_ degree than on the standard ImageNet benchmarks (Table 1). It\nseems as though the distillation process\u2019s ability to store didactic information in a single image\nper class is even more relevant in this setting since any one real image is less likely to contain the\ninformation necessary to distinguish its entire respective class from the other highly similar classes.\n\n**4.6** **Cross-Model Performance Predicts Model Alignment**\n\nRecent research has suggested an \u201calignment\u201d of various self-supervised models despite large\ndifferences in their training methods or even data modalities. The notion that all these models are\nindeed converging to a unified embedding space has been dubbed the \u201cPlatonic Representation\nHypothesis\u201d [20]. While the representations of today\u2019s models are not yet perfectly synchronized, it\nis still of interest to measure the degree to which they are aligned.\n\nIn the hypothesis\u2019s titular work, the authors introduce a method of measuring the alignment of two\nmodels called \u201cmutual _k_ nearest neighbors.\u201d In short, this method finds each sample\u2019s _k_ nearest\nneighbors in the embedding spaces of each model and then computes the fraction of neighbors that\nare shared between the two.\n\nWhen we distill images with a given model A and measure the 1 nearest neighbor performance\n(since there is just one image per class) on a different model B, we find that the normalized test\naccuracy is strongly correlated with the mutual _k_ -nn alignment between models A and B, as seen\nin Table 6. This is an interesting interpretability result showing that the cross-model performance of\na distilled dataset highly depends on the alignment between the two models. Given this dependency,\nour distillation method then acts as a method of _visualizing_ how well these models align by backing\nthe discrepancies in the embedding space out to image space. For example, given the two least\naligned models (CLIP and MoCo), we can see in Figure 1 that even for different sets of classes, the\ntwo models clearly distill very different _styles_ of images, alluding towards their misalignment.\n\n9\n\nDistill Normalized _k_ -NN Accuracy\nModel\n\nCLIP DINO-v2 EVA-02 MoCo-v3\n\nSource Model Alignment\nModel\n\nCLIP DINO-v2 EVA-02 MoCo-v3\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-9-0.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-9-1.png)\n\nTable 6: **Distilled datasets predict model alignment.** We distill ImageNet-1k and find the synthetic data\u2019s\n_cross-model_ performance ( **left** ) by evaluating on a model other than the one used during distillation. We find that\nthis cross-model performance correlates well with the _alignment_ between models\u2019 embedding spaces ( **right** ).\nNote the similarity of the per-row trends between the two tables. Rows are colored from highest to lowest.\n\nFigure 8: **Distilling Out-of-Distribution Datasets:** Despite only ever being trained on the real-world images\nin ImageNet-1k [12], our method can still use DINO-v1 [5] to distill out-of-distribution datasets such as\nArtBench [24] that have no overlapping content. We see that the distillation is not simply \u201ccopying\u201d samples\nfrom the dataset by comparing to the nearest real neighbor in the model\u2019s embedding space and observing the\nstark differences. This interpretability insight granted by our method highlights DINO\u2019s remarkable ability to\ngeneralize beyond its training distribution.\n\n**4.7** **Self-Supervised Backbones can Distill Out-of-Distribution Datasets**\n\nNote that while we have thus far been discussing DINO-v2, this section in particular will reference\nDINO-v1 since this version\u2019s training set is open-sourced. Since DINO-v1 [5] was only trained on\nImageNet [12], a dataset of nearly all \u201creal-world\u201d images, we can easily test its ability to distill an\n**out-of-distribution** dataset such as ArtBench [24] that consists of 10 classes of art styles. In Figure 8,\nwe visualize the distilled images ( **top** ) as well as the nearest neighbor in DINO-v1\u2019s embedding\nspace. Surprisingly, despite having previously only ever seen \u201creal-world\u201d images from ImageNet,\nDINO-v1 is still able to effectively distill ArtBench into a single image per class. Furthermore, we\nconfirm that the distilling is not simply \u201ccopying\u201d real images from the training set by observing\nthe stark differences between the synthetic images and their nearest neighbors. This is especially\napparent in the cases of Realism, Renaissance, and Romanticism where the nearest neighbors are\nblack and white while the distilled images are colorful."}, {"role": "assistant", "content": "**Verdict:** SIGNIFICANT REDUCTION IN DATA-RELATED COSTS AND TRAINING TIME, UNLOCKING NEW EFFICIENCIES. Highly valuable for fine-grained classification and model interpretability.\n\n**Summary:** This method dramatically reduces the data footprint required for training downstream linear classifiers on pre-trained self-supervised vision models, from millions of images to a single synthetic image per class. This translates directly to substantial savings in data storage, transfer, and the computational resources (CPU/GPU hours) needed for training these classifiers. The utility is high because it offers: 1. Cost Reduction: Training a linear classifier on one synthetic image per class achieves comparable performance to using the full dataset for a fraction of the cost. 2. Scalability: Enables efficient deployment in environments with limited storage or bandwidth. 3. Enhanced Fine-Grained Classification: Shows superior performance on challenging datasets with highly similar classes. 4. Interpretability & Debugging: The generated synthetic images serve as a powerful diagnostic tool, revealing model biases or weaknesses. 5. Out-of-Distribution Generalization: Demonstrates the ability to distill effective training data for novel domains even when the backbone was trained on disparate data. While distillation itself requires compute, the overall reduction in downstream training costs and requirements makes this a significant efficiency gain for practical AI applications."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\nVideo understanding benchmarks should test whether models build coherent, predictive world models from long video\nstreams. This requires accumulating visual evidence, maintaining state, and tracking entities over time\u2014abilities\nthat humans use naturally but current models largely lack. In practice, many benchmarks do not meet this goal.\nPrior work shows that models can often ignore most of the temporal structure in the video, rely on a single frame,\nor even use text-only inputs and still match or surpass state-of-the-art _video_ models (Yang et al., 2025b; Tangemann\net al., 2020; Buch et al., 2022; Lei et al., 2023; Li et al., 2018; Goyal et al., 2017; Brown et al., 2025). Much of what\nwe call _video understanding_ reduces to image captioning or language modeling with minimal temporal reasoning.\n\nThis is known as shortcut learning (Geirhos et al., 2020). Benchmark designers aim for tasks that are hard to solve\nvia shortcuts, so that the easiest way to improve performance is to develop the intended capability (Brown et al.,\n2025). When this fails, the design can unintentionally nudge practitioners into baking those same shortcuts into\ntheir models (Hammoud et al., 2023; Dehghani et al., 2021). Large models often similarly exploit spurious cues\ninstead (Geirhos et al., 2020). A recent work, Cambrian-S (Yang et al., 2025b) proposes to address these issues.\n\nCambrian-S argues that benchmarks should target specific capabilities: streaming event cognition, implicit 3D\nreasoning, and _spatial supersensing_ i.e., whether models form predictive world models from raw video. To resist\nbrute-force context expansion, Cambrian-S introduces the VSI-Super benchmarks which need true spatial reasoning\nabilities. The tasks use long, multi-environment videos and questions designed to require accumulating evidence,\ntracking objects, and maintaining state over time. The promise is clear: strong performance on these benchmarks\nshould require genuine spatial cognition and memory. This motivates our central questions about VSI-Super:\n\n1. Does success on _VSI-Super-Recall_ (VSR) truly require supersensing or even spatial cognition, or can a simple\nSigLIP model which discards almost all temporal structure still perfectly solve the benchmark?\n\n2. Does the Cambrian-S inference method perform the intended event-level reasoning and spatial supersensing, or\ndoes it rely on shortcuts specific to the benchmark like _VSI-Super-Counting_ (VSC)?\n\nTo investigate these questions, we conduct two complementary analyses:\n\n1. **VSR: A Simple Baseline.** We take SigLIP-2, select the four frames most relevant to the object mentioned\nin the question, and select the most correlated answer option. We call the resulting baseline _NoSense_, short\nfor _No Supersensing_ . NoSense uses no explicit 3D representation, does not track objects, and does not use\nlanguage models. Despite this simplicity, NoSense nearly perfectly solves VSR, even on 4-hour videos, while\ndiscarding almost all temporal structure in the video. This suggests that VSI-Super-Recall, like many prior video\nbenchmarks, primarily measures image-level semantic cues and object\u2013context associations rather than spatial\nsupersensing, and only needs a few informative frames rather than long-range temporal understanding.\n\n2. **VSC: A Sanity Check.** Does the inference method of Cambrian-S, designed for each benchmark, simply\nexploit shortcuts in the benchmark? We evaluate this by creating _VSC-Repeat_ : We take each video clip in\nthe VSC benchmark and concatenate it with itself 1-5 times, creating a sequence in which every room and\nobject is revisited repeatedly. A model performing genuine supersensing should recognize that these are repeated\nobservations of the same scene; therefore, the predicted object counts should remain unchanged. However,\nrepeating the video a few times reduces the mean relative accuracy of Cambrian-S from 42% to 0%. This indicates\nthe inference algorithm inadvertently exploits the benchmark shortcuts rather than improving supersensing.\n\nOur findings do not imply that there has been no progress in improving spatial supersensing. Gemini 2.5 Flash\nachieves around 41.5% accuracy on 60-minute VSR videos, illustrating that substantial VSR performance can arise\nfrom generic long-context video training. While VSI-Super is a comprehensive, thoughtful effort, it still fails to\n\n                                                       reliably enforce the long-horizon spatial supersensing it aims to elicit. Cambrian S\u2019 VSR inference pipeline and\n_NoSense_ share a similar high-level pattern \u2013 streaming frame encoding, a compact memory of salient frames, and\nquery-guided retrieval \u2013 suggesting that a much simpler instantiation of this pattern already goes a long way on\nVSR. Our simple sanity check, _VSC-Repeat_ shows the degree to which VSC-tailored inference algorithms rely on\nshortcuts. Our two-fold analysis underscores the need for caution when interpreting performance on VSI-Super:\nrather than reflecting genuine spatial cognition, high scores may simply mean exploiting heuristic shortcuts. We\nfurther include a response from the authors (in Appendix A) to provide a balanced perspective alongside our claims.\n\n2\n\nOverall, our tools \u2013 _NoSense_ for VSR and _VSC-Repeat_ for VSC \u2013 act as simple stress tests for benchmarks and\ntraining methods. We view this kind of meta-evaluation as analogous to ablation studies for models and believe\nit should be standard for benchmarks. For any proposed spatial supersensing task, we should ask: what is the\nsimplest streaming baseline that exploits obvious regularities? Which invariances should hold, and do methods\nrespect them? We hope our analysis fosters more rigorous testing and design of superspatial sensing in the future."}, {"role": "assistant", "content": "**Verdict:** VSI-Super, a new benchmark designed to promote genuine long-horizon spatial reasoning in video models, is largely ineffective as a reliable measure. Its performance metrics can be easily gamed by models employing simple, shortcut-based strategies rather than true spatial cognition or long-term memory.\n\n**Summary:** This analysis exposes a significant weakness in video understanding benchmarks, including the recently proposed VSI-Super. The \"NoSense\" baseline, a computationally inexpensive approach leveraging a pre-trained image model (SigLIP-2) and simple frame selection, nearly perfectly solves VSI-Super-Recall (VSR) without any explicit 3D representation or temporal tracking. This indicates that high performance on VSR does not guarantee genuine spatial supersensing but often reflects image-level semantic cues. Furthermore, the \"VSC-Repeat\" stress test demonstrates that even Cambrian-S's own inference method for VSI-Super-Counting (VSC) relies on benchmark-specific shortcuts, failing to maintain object identity over repeated observations.The primary utility of this work is its contribution to *benchmark meta-evaluation*. It provides two practical, low-cost tools (NoSense and VSC-Repeat) that can be immediately adopted by benchmark designers and researchers to validate whether a video understanding task truly requires the intended capabilities. This could prevent wasted R&D efforts and significant compute costs on developing complex models that merely exploit superficial patterns in benchmarks. By exposing these vulnerabilities, the paper guides future research towards developing models that genuinely understand long-form video streams, fostering more robust and meaningful progress in the field. There are no direct hardware requirements or high costs associated with implementing these diagnostic tools; they are simple analytical methods."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n_each tree \ud835\udf0f_ \u2208 _\ud835\udc47_ _that contains edge_ ( _\ud835\udc62, \ud835\udc63_ ) **do**\n\n**18** _\ud835\udc64\ud835\udc5c\ud835\udc5f\ud835\udc54_ \u2190 _\ud835\udc64\ud835\udc5c\ud835\udc5f\ud835\udc54_ + _\ud835\udc64_ ( _\ud835\udf0f_ );\n\n**19** _\ud835\udf0f_ [\u2032] \u2190 cut-link ( _\ud835\udf0f,_ ( _\ud835\udc62, \ud835\udc63_ ));\n\n**20** Update _\ud835\udc53_ ( _\ud835\udf0f_ [\u2032] );\n\n**21** Calculate weight as _\ud835\udc64_ ( _\ud835\udf0f_ [\u2032] ) = _[\ud835\udc51]_ _\ud835\udc51_ [(] ( _[\ud835\udf0f]_ _\ud835\udf0f_ [\u2032] ) [)] _[\ud835\udc64]_ [(] _[\ud835\udf0f]_ [)][;]"}, {"role": "assistant", "content": "**Verdict:** This section describes a core, computationally intensive process for graph decomposition and metric propagation, likely forming a critical bottleneck.\n\n**Summary:** This code details an iterative procedure that processes a collection of graph structures (trees) by identifying specific edges, performing a 'cut-link' operation to decompose the structures (creating new sub-trees), and subsequently re-calculating and accumulating weights. The `worg` variable tracks a global sum of weights, indicating an aggregated measure. The `cut-link` operation and subsequent weight recalculation `w(\ud835\udf0f') = d(\ud835\udf0f', \ud835\udf0f) * w(\ud835\udf0f)` implies recursive or hierarchical processing of structural components. This will incur significant computational overhead due to repeated graph traversals, dynamic structure modifications, and feature updates. The primary costs are high CPU cycles for iterative processing and potential memory demands for storing evolving tree structures and associated metadata. Its utility lies in enabling fine-grained structural analysis and weighted aggregation, but at a high operational expense that must be considered for scalability."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n**3.1** **Evolutionary Algorithms**\n\nEvolutionary algorithms have long been a compelling alternative to backpropagation-based training methods.\nDespite encompassing a broad range of algorithms (e.g., genetic algorithms (Such et al., 2018) or symbolic\nevolution (Koza, 1994)), most contemporary research on evolution has shifted towards algorithms that scale\nbetter to large numbers of neural network parameters (Jaderberg et al., 2017; Hansen & Ostermeier, 2001;\nSalimans et al., 2017).\n\nOur work focuses on evolving the weights of predetermined architectures, building up from the NES (Wierstra\net al., 2011) family of methods. Such an approach has recently grown in prominence, following the application\nof NES to policy learning in conventional RL environments (Salimans et al., 2017) to alleviate some of the\nchallenges that policy-gradient methods struggle with, like long-horizon environments. Since then, evolution\nhas been widely applied in other domains, such as meta-learning (e.g., (Lu et al., 2022; Metz et al., 2022; Lange\net al., 2023; Goldie et al., 2024; 2025)), hyperparameter tuning (e.g., (Parker-Holder et al., 2021; Tani et al.,\n2021; Vincent & Jidesh, 2023)) and drug discovery (Towers et al., 2025). Here, we consider the limitations\nand solutions to applying ES at significant scale, beyond the small networks and population sizes of these prior\nworks, with a broad focus on policy-learning. In particular, Salimans et al. (2017) uses a maximum population\nsize of 1440, whereas our maximum population size is on the order of hundreds of thousands.\n\nES is limited by its requirement for full, potentially expensive evaluations of the fitness function due to\nsimulating policies in long-horizon environments and potentially high memory usage. Persistent Evolution\nStrategies (Vicol et al., 2021) demonstrate a significant speedup by updating networks _online_ (i.e., during the\nunroll), with followups offering further variance reduction (Li et al., 2023b; Vicol et al., 2023). We note that\nthese works are orthogonal to our focus on scaling up the population size of ES; we leave the application of\nthese techniques on top of EGGROLL to future work.\n\n**3.2** **Evolution Strategies for LLMs**\n\nAlthough gradient backpropagation is typically used for LLM training and fine-tuning, prior work has explored\nES variants for fine-tuning. In particular, zeroth order optimization (Zhang et al., 2024), which is analogous to\nES with a population size of 1, is used by Malladi et al. (2023) for memory-efficient LLM fine-tuning. Yu et al.\n(2025) extend this approach by projecting perturbations to a low-rank subspace, improving the convergence\nof zeroth order optimization. Jin et al. (2024) performs ES directly on LoRA matrices. These works focus\non the supervised fine-tuning setting, finding comparable performance to full fine-tuning, but they do not\ndetermine whether pretraining is possible with zeroth order methods; we find that large population sizes are\nnecessary for pretraining performance, indicating that zeroth order optimization methods would be unsuitable\nfor pretraining.\n\nRecent work has also explored ES in the context of LLM reasoning. Korotyshova et al. (2025) first train\nLoRA adapters using supervised fine-tuning (SFT) before decomposing them into fixed SVD bases alongside\nsingular values that are trained using CMA-ES. They achieve comparable performance to GRPO (Shao et al.,\n2024) in significantly less wall-clock time on math reasoning benchmarks. Qiu et al. (2025) directly use ES\nto optimize all LLM parameters for reasoning, with stronger performance than GRPO on the countdown\nreasoning task. However, both of these approaches use relatively small population sizes, on the order of a\nhundred unique perturbations per update, and instead collect hundreds of rollouts per perturbation to efficiently\nuse GPUs. By contrast, our approach allows all generations to use different perturbations, such that our\nmaximum population size per update is orders of magnitude larger (equal to the maximum inference batch\nsize), without compromising token generation throughput."}, {"role": "assistant", "content": "**Verdict:** Potentially a significant breakthrough for Large Language Model (LLM) pretraining, offering a viable alternative to backpropagation, but comes with extreme computational demands.\n\n**Summary:** This section introduces a method for scaling Evolutionary Algorithms (EAs) to unprecedented population sizes (hundreds of thousands), specifically targeting policy-learning and, critically, LLM pretraining. Its utility lies in enabling EA for tasks like pretraining where smaller populations or zeroth-order methods failed, potentially overcoming challenges faced by policy-gradient methods in long-horizon environments. The core innovation is demonstrating that large population sizes are 'necessary for pretraining performance,' suggesting a new avenue for training foundational models. However, this comes with a substantial cost: the requirement for 'significant scale,' demanding massive computational resources for evaluating 'hundreds of thousands' of population members simultaneously. While prior work focused on speedups or memory efficiency, this paper intentionally scales up population size, meaning hardware requirements (e.g., GPU clusters for parallel inference) will be orders of magnitude higher than existing ES approaches for LLMs, despite not compromising 'token generation throughput' for inference."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\nRecent advances in natural language processing have been driven\nby the rapid scaling of large language models (LLMs) [20, 27, 28].\nAmong them, Mixture-of-Experts (MoE) architectures [36] provide\nan attractive path to efficiency by activating only a small subset of\nexperts per token while preserving a high overall parameter capacity. Modern MoE models, such as Qwen-MoE [34] and DeepSeekMoE [19], consistently outperform dense counterparts [7], yet their\ndeployment remains constrained by memory footprint. To support\ndynamic routing, all experts must reside in GPU memory, causing\ninactive experts to dominate the total parameter load. For example,\nQwen3-30B-A3B requires roughly 57GB of weights to be present on\ndevice despite using only about 3B (6.1GB) parameters per token. As\nMoE layers contribute more than 98% of the model size, compressing experts is essential for running such models on memory-limited\nconsumer GPUs like the RTX-5090 or A6000.\n\nPost-training quantization (PTQ) provides a practical means to\nreduce memory usage via low-bit representations. Methods such as\nAWQ [18] and GPTQ [9] demonstrate strong compression on dense\n\n\u2217Corresponding author. Email: wei.13.zhang@uconn.edu\n\nLLMs, and recent efforts extend PTQ to MoE experts. Techniques including MoQE [15], MxMoE [6], EaQuant [10], and QMoE [8] push\nprecision down to 2\u20134 bits or even sub-1-bit regimes. Although it\nincurs about a 6 _._ 7% accuracy drop compared with the full-precision\nmodel. In addition, all prior MoE quantization remains _static_ : expert\nbit-widths are fixed offline, based on limited calibration data, and\ncannot adjust to the highly dynamic activation patterns observed\nduring inference. As a result, hot experts may suffer unnecessary\ndegradation under aggressive quantization, while cold experts retain oversized representations that waste scarce HBM capacity.\nThis work introduces **DynaExq** (Dynamic Expert Exchange\nQuantization), a runtime system that treats precision as a _first-class,_\n_dynamically managed resource_ . DynaExq is motivated by the observation that expert activation patterns shift substantially across\ninputs and workloads. As shown in Fig. 1, only a small subset\nof experts remains consistently active within each layer, whereas\nmost experts are rarely selected. By exploiting this temporal sparsity, DynaExq assigns precision adaptively: frequently activated\n(hot) experts retain high precision to maintain routing accuracy,\nwhereas infrequently used (cold) experts are aggressively quantized\nor offloaded to reduce memory pressure. This runtime precision realignment enables substantial compression without compromising\nmodel fidelity.\nDynaExq comprises two core mechanisms and two supporting\ncomponents. The _Expert Precision Controller_ maintains a slidingwindow profile of expert activity and issues promotion or demotion\ndecisions based on workload trends and memory feasibility. The\n_Asynchronous Swapping Pipeline_ performs precision transitions in\nthe background by staging weights across SSD, DRAM, and HBM\nusing dedicated CUDA streams, ensuring that expert switching\nnever interrupts the forward path. To guarantee allocator determinism, DynaExq adopts a dual-pool memory layout that isolates\nhigh- and low-precision experts, combined with a static threshold\ninitialization procedure that respects per-layer HBM budgets.\nTogether, these mechanisms allow DynaExq to deliver adaptive\nquantization, stable routing behavior, and high-throughput inference for large MoE models on a single commodity GPU. The key\ncontributions of this work are:\n\n - We present **DynaExq**, a runtime system that tightly couples\nexpert quantization with activation-aware precision scheduling for single-GPU MoE inference, managing expert precision\nonline based on router statistics and memory budgets.\n\n![](papers/images/Dynamic-Expert-Quantization-for-Scalable-MixtureofExperts-Inference.pdf-1-6.png)\n\n**(a) Wikitext**\n\n**(d) Wikitext**\n\n![](papers/images/Dynamic-Expert-Quantization-for-Scalable-MixtureofExperts-Inference.pdf-1-5.png)\n\n**(b) GSM8K**\n\n**(e) GSM8K**\n\nKexin Chu, Dawei Xiang, Zixu Shen, Yiwei Yang, Zecheng Lin, and Wei Zhang\n\n![](papers/images/Dynamic-Expert-Quantization-for-Scalable-MixtureofExperts-Inference.pdf-1-4.png)\n\n**(c) HumanEval**\n\n**(f) HumanEval**\n\n![](papers/images/Dynamic-Expert-Quantization-for-Scalable-MixtureofExperts-Inference.pdf-1-14.png)\n\n![](papers/images/Dynamic-Expert-Quantization-for-Scalable-MixtureofExperts-Inference.pdf-1-15.png)\n\n![](papers/images/Dynamic-Expert-Quantization-for-Scalable-MixtureofExperts-Inference.pdf-1-16.png)\n\n**Figure 1: Expert activation distributions across layers and workloads. Each subplot shows routing frequency for 128 experts in**\n**layer 1 and layer 47.**\n\n - We introduce a multi-stage **asynchronous swapping pipeline**\nthat overlaps data movement, quantization, and computation\nto ensure non-blocking precision transitions.\n\n - We develop a **structured memory pool** that eliminates fragmentation and enables efficient concurrent expert promotions\nand demotions under tight memory constraints."}, {"role": "assistant", "content": "**Verdict:** Strategically valuable. This directly addresses a critical bottleneck for deploying large Mixture-of-Experts (MoE) models on commodity hardware, significantly improving their economic viability and accessibility.\n\n**Summary:** The primary challenge with MoE models is their enormous memory footprint, as all experts, regardless of activity, must reside in expensive GPU memory. This paper introduces DynaExq, a system that dynamically quantizes MoE experts based on their real-time usage patterns. It keeps frequently accessed ('hot') experts in high precision for accuracy while aggressively quantizing or offloading rarely accessed ('cold') experts. This allows for substantial memory reduction, enabling deployment of massive MoE models (e.g., 57GB Qwen3) on single consumer-grade GPUs like an RTX-5090 or A6000, which traditionally lack sufficient HBM. The solution leverages an asynchronous swapping pipeline and structured memory management to ensure non-blocking precision transitions and prevent performance degradation. The utility is high: it transforms MoE from a specialized, high-cost architecture into one more suitable for wider adoption and cost-effective inference by reducing hardware requirements, particularly memory-intensive GPUs, and maintaining model fidelity. While there's an implied computational overhead for expert profiling and memory management, the system is designed to deliver high-throughput inference."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\nIn this paper, we introduce EGGROLL, a powerful method for blackbox optimisation that scales evolutionary\nstrategies to billion-parameter models and beyond using low-rank search matrices. Our experiments demonstrate that EGGROLL is effective with rank as small as _r_ = 1, which represents substantial computational and\nmemory savings for negligible decrease in performance when compared to the full-rank ES update. Empirically,\nEGGROLL delivers large speedups over na\u00efve ES in tabula rasa and multi-agent RL, and can power end-to-end\ntraining pipelines for large language models. Our theoretical analysis reveals that the low-rank EGGROLL\nupdate quickly converges with rank _r_, however further theoretical analysis is needed to explain the success of\nour method when _r_ = 1.\n\nLooking forward, we are working on applying EGGROLL for other problems beyond the reach of modern\ngradient-based techniques. In particular, EGGROLL can enable the training of large scale end-to-end neurosymbolic systems (Sarker et al., 2021) which have nondifferentiable components. For instance, we can\ntrain neural networks that directly interface with symbolic modules for specialized functions, like memory\nor calculations. We can also optimize end-to-end systems of language models, training them to be aware of\ninference-time harnesses and interactions with other agents in complex systems.\n\n**Acknowledgements**\n\nCompute for this project is graciously provided by the Isambard-AI National AI Research Resource, under\nthe projects \u201cFLAIR 2025 Moonshot Projects\u201d and \u201cRobustness via Self-Play RL.\u201d Some experiments also\n[used compute generously given by JASMIN, the UK\u2019s collaborative data analysis environment (https:](https://www.jasmin.ac.uk)\n[//www.jasmin.ac.uk).](https://www.jasmin.ac.uk)\n\n11\n\nBidipta Sarkar is supported by the Clarendon Fund Scholarship in partnership with a Department of Engineering\nScience Studentship for his Oxford DPhil. Mattie Fellows is funded by a generous grant from the UKRI\nEngineering and Physical Sciences Research Council EP/Y028481/1. Jakob Nicolaus Foerster is partially\nfunded by the UKRI grant EP/Y028481/1 (originally selected for funding by the ERC). Jakob Nicolaus Foerster\nis also supported by the JPMC Research Award and the Amazon Research Award. Juan Agustin Duque is\nsupported by the St-Pierre-Larochelle Scholarship at the University of Montreal and by Aaron Courville\u2019s\nCIFAR AI Chair in Representations that Generalize Systematically. Lukas Seier is supported by the Intelligent\nEarth CDT with funding from the UKRI grant number EP/Y030907/1.\n\n**References**\n\nShun-ichi Amari. Natural gradient works efficiently in learning. _Neural Computation_, 10(2):251\u2013276, 1998.\ndoi: 10.1162/089976698300017746.\n\nA. B. Basset. _A Treatise on Hydrodynamics: with numerous examples_, volume 2. Deighton, Bell, and Co.,\nCambridge, UK, 1892.\n\nYoshua Bengio, R\u00e9jean Ducharme, and Pascal Vincent. A neural probabilistic language model. In T. Leen,\nT. Dietterich, and V. Tresp (eds.), _Advances in Neural Information Processing Systems_, volume 13.\n[MIT Press, 2000. URL https://proceedings.neurips.cc/paper_files/paper/2000/](https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf)\n[file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf)\n\nHans-Georg Beyer. Toward a theory of evolution strategies: Self-adaptation. _Evolutionary Computation_, 3:\n[311\u2013347, 1995. URL https://api.semanticscholar.org/CorpusID:17416734.](https://api.semanticscholar.org/CorpusID:17416734)\n\nHans-Georg Beyer and Hans-Paul Schwefel. Evolution strategies \u2013a comprehensive introduction. _Natural_\n_Computing_, 1(1):3\u201352, 2002.\n\nR. N. Bhattacharya and R. Ranga Rao. _Normal approximation and asymptotic expansions_ . Wiley series in\nprobability and mathematical statistics. Wiley, New York, 1976. ISBN 047107201X.\n\nCl\u00e9ment Bonnet, Daniel Luo, Donal Byrne, Shikha Surana, Sasha Abramowitz, Paul Duckworth, Vincent\nCoyette, Laurence I. Midgley, Elshadai Tegegn, Tristan Kalloniatis, Omayma Mahjoub, Matthew Macfarlane,\nAndries P. Smit, Nathan Grinsztajn, Raphael Boige, Cemlyn N. Waters, Mohamed A. Mimouni, Ulrich\nA. Mbou Sob, Ruan de Kock, Siddarth Singh, Daniel Furelos-Blanco, Victor Le, Arnu Pretorius, and\nAlexandre Laterre. Jumanji: a diverse suite of scalable reinforcement learning environments in jax, 2024.\n[URL https://arxiv.org/abs/2306.09884.](https://arxiv.org/abs/2306.09884)\n\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George\nNecula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable\n[transformations of Python+NumPy programs, 2018. URL http://github.com/jax-ml/jax.](http://github.com/jax-ml/jax)\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\n[are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165.](https://arxiv.org/abs/2005.14165)\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, ..., Barret Zoph, Liam Fedus, and ...\n(full list). Palm: Scaling language modeling with pathways. _J. Mach. Learn. Res._, 24(1144), 2023. URL\n[https://jmlr.org/papers/volume24/22-1144/22-1144.pdf.](https://jmlr.org/papers/volume24/22-1144/22-1144.pdf)\n\nA. P. Dawid. Some matrix-variate distribution theory: Notational considerations and a bayesian application.\n_Biometrika_, 68(1):265\u2013274, 1981. ISSN 0006-3444.\n\n12\n\nNan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\nYanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P. Bosma, Zongwei Zhou,\nTao Wang, Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju\nDuke, Lucas Dixon, Kun Zhang, Quoc Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. Glam: Efficient\nscaling of language models with mixture-of-experts. In _Proceedings of the 39th International Conference_\n_on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pp. 5547\u20135569, Jul 2022.\n[URL https://proceedings.mlr.press/v162/du22c.html.](https://proceedings.mlr.press/v162/du22c.html)\n\nRichard Durrett. _Probability: Theory and Examples_ . Cambridge University Press, Cambridge, UK, 4th edition,\n2010.\n\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: scaling to trillion parameter models\nwith simple and efficient sparsity. _J. Mach. Learn. Res._, 23(1):1\u201339, January 2022. ISSN 1532-4435. URL\n[https://jmlr.org/papers/volume23/21-0998/21-0998.pdf.](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf)\n\n[Jakob Nicolaus Foerster. Nonlinear computation in deep linear networks, sep 2017. URL https://blog.](https://blog.openai.com/nonlinear-computation-in-linear-networks/)\n[openai.com/nonlinear-computation-in-linear-networks/. Accessed: 2025-11-20.](https://blog.openai.com/nonlinear-computation-in-linear-networks/)\n\nGerald B. Folland. _Real Analysis: Modern Techniques and Their Applications_ . John Wiley & Sons, New York,\n2nd edition, 1999. See Theorem 8.22 (Riemann\u2013Lebesgue Lemma).\n\nC. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem. Brax \u2013 a\n[differentiable physics engine for large scale rigid body simulation, 2021. URL https://arxiv.org/](https://arxiv.org/abs/2106.13281)\n[abs/2106.13281.](https://arxiv.org/abs/2106.13281)\n\nAlexander D. Goldie, Chris Lu, Matthew T. Jackson, Shimon Whiteson, and Jakob N. Foerster. Can Learned\nOptimization Make Reinforcement Learning Less Difficult? In _Advances in Neural Information Processing_\n_Systems_, volume 37, pp. 5454\u20135497, 2024.\n\nAlexander David Goldie, Zilin Wang, Jaron Cohen, Jakob Nicolaus Foerster, and Shimon Whiteson. How\n[Should We Meta-Learn Reinforcement Learning Algorithms? May 2025. URL https://openreview.](https://openreview.net/forum?id=jKzQ6af2DU)\n[net/forum?id=jKzQ6af2DU.](https://openreview.net/forum?id=jKzQ6af2DU)\n\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep Learning_ [. MIT Press, 2016. http://www.](http://www.deeplearningbook.org)\n[deeplearningbook.org.](http://www.deeplearningbook.org)\n\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes,\nN. Lawrence, and K.Q. Weinberger (eds.), _Advances in Neural Information Processing Systems_, volume 27. Curran Associates, Inc., 2014. [URL https://proceedings.neurips.cc/paper_](https://proceedings.neurips.cc/paper_files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Paper.pdf)\n[files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Paper.pdf)\n\nI. S. (Izrail Solomonovich) Gradshte\u02d8\u0131n, I. M. (Iosif Moiseevich) Ryzhik, Daniel Zwillinger, Victor Moll, and\nInc Scripta Technica. _Table of integrals, series, and products_ . Academic Press, San Diego ; Tokyo, 8 edition,\n2015. ISBN 0123849330.\n\nG R Grimmett and D R Stirzaker. Probability and random processes. _Journal of the Royal Statistical Society._\n_Series A, Statistics in society_, 156(3):503\u2013503, 1993. ISSN 0964-1998.\n\nPeter Hall. _The bootstrap and Edgeworth expansion_ . Springer series in statistics. Springer-Verlag, New York,\n1992. ISBN 9780387945088.\n\n[Nikolaus Hansen. The cma evolution strategy: A tutorial, 2023. URL https://arxiv.org/abs/1604.](https://arxiv.org/abs/1604.00772)\n\n[00772.](https://arxiv.org/abs/1604.00772)\n\nNikolaus Hansen and Andreas Ostermeier. Completely Derandomized Self-Adaptation in Evolution Strategies.\n_Evolutionary Computation_, 9(2):159\u2013195, June 2001. ISSN 1063-6560. doi: 10.1162/106365601750190398.\n[URL https://ieeexplore.ieee.org/document/6790628.](https://ieeexplore.ieee.org/document/6790628)\n\n13\n\nJoel Heck and Fathi M. Salem. Simplified minimal gated unit variations for recurrent neural networks, 2017.\n[URL https://arxiv.org/abs/1701.03452.](https://arxiv.org/abs/1701.03452)\n\nSepp Hochreiter and J\u00fcrgen Schmidhuber. Lstm can solve hard long time lag problems. In M.C. Mozer,\nM. Jordan, and T. Petsche (eds.), _Advances in Neural Information Processing Systems_, volume 9.\n[MIT Press, 1996. URL https://proceedings.neurips.cc/paper_files/paper/1996/](https://proceedings.neurips.cc/paper_files/paper/1996/file/a4d2f0d23dcc84ce983ff9157f8b7f88-Paper.pdf)\n[file/a4d2f0d23dcc84ce983ff9157f8b7f88-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/1996/file/a4d2f0d23dcc84ce983ff9157f8b7f88-Paper.pdf)\n\nMark Horowitz. 1.1 computing\u2019s energy problem (and what we can do about it). In _2014 IEEE International_\n_Solid-State Circuits Conference Digest of Technical Papers (ISSCC)_, pp. 10\u201314, 2014. doi: 10.1109/ISSCC.\n2014.6757323.\n\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. In _ICLR_ . OpenReview.net, 2022.\n\nMax Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi,\nOriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, and Koray Kavukcuoglu.\n[Population Based Training of Neural Networks, November 2017. URL http://arxiv.org/abs/](http://arxiv.org/abs/1711.09846)\n[1711.09846. arXiv:1711.09846 [cs].](http://arxiv.org/abs/1711.09846)\n\nFeihu Jin, Yifan Liu, and Ying Tan. Derivative-free optimization for low-rank adaptation in large language\nmodels. _IEEE/ACM Trans. Audio, Speech and Lang. Proc._, 32:4607\u20134616, October 2024. ISSN 2329-9290.\n[doi: 10.1109/TASLP.2024.3477330. URL https://doi.org/10.1109/TASLP.2024.3477330.](https://doi.org/10.1109/TASLP.2024.3477330)\n\nJean Kaddour. The minipile challenge for data-efficient language models. _arXiv preprint arXiv:2304.08442_,\n2023.\n\nSham M Kakade. A natural policy gradient. In T. Dietterich, S. Becker, and Z. Ghahramani (eds.), _Advances in Neural Information Processing Systems_, volume 14. MIT Press,\n2001. [URL https://proceedings.neurips.cc/paper_files/paper/2001/file/](https://proceedings.neurips.cc/paper_files/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf)\n[4b86abe48d358ecf194c56c69108433e-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf)\n\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun\n(eds.), _2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April_\n_14-16, 2014, Conference Track Proceedings_ [, 2014. URL http://arxiv.org/abs/1312.6114.](http://arxiv.org/abs/1312.6114)\n\nDaria Korotyshova, Boris Shaposhnikov, Alexey Malakhov, Alexey Khokhulin, Nikita Surnachev, Kirill\nOvcharenko, George Bredis, Alexey Gorbatovski, Viacheslav Sinii, and Daniil Gavrilov. Essa: Evolutionary\n[strategies for scalable alignment, 2025. URL https://arxiv.org/abs/2507.04453.](https://arxiv.org/abs/2507.04453)\n\nJohn R. Koza. Genetic programming as a means for programming computers by natural selection. _Statistics_\n_and Computing_ [, 4(2):87\u2013112, June 1994. ISSN 1573-1375. doi: 10.1007/BF00175355. URL https:](https://doi.org/10.1007/BF00175355)\n[//doi.org/10.1007/BF00175355.](https://doi.org/10.1007/BF00175355)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep\nconvolutional neural networks. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger\n(eds.), _Advances in Neural Information Processing Systems_, volume 25. Curran Associates,\n[Inc., 2012. URL https://proceedings.neurips.cc/paper_files/paper/2012/file/](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n[c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with\npagedattention. In _Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles_,\n2023.\n\nRobert Tjarko Lange, Tom Schaul, Yutian Chen, Tom Zahavy, Valentin Dallibard, Chris Lu, Satinder Singh,\nand Sebastian Flennerhag. Discovering Evolution Strategies via Meta-Black-Box Optimization, March\n[2023. URL http://arxiv.org/abs/2211.11260. arXiv:2211.11260 [cs].](http://arxiv.org/abs/2211.11260)\n\n14\n\nPierre-Simon Laplace. M\u00e9moire sur les int\u00e9grales d\u00e9finies et leur application aux probabilit\u00e9s, et sp\u00e9cialement\n\u00e0 la recherche du milieu qu\u2019il faut choisir entre les r\u00e9sultats des observations. _M\u00e9moires de la Classe des_\n_Sciences Math\u00e9matiques et Physiques de l\u2019Institut Imp\u00e9rial de France,_ 1 [re] s\u00e9rie, 11(1 [re] partie):297\u2013347,\n1811.\n\nV. P. Leonov and A. N. Shiryaev. On a method of calculation of semi-invariants. _Theory of Probability and Its_\n_Applications_ [, 4(3):342\u2013355, 1959. URL https://www.mathnet.ru/eng/tvp4894.](https://www.mathnet.ru/eng/tvp4894)\n\nOscar Li, James Harrison, Jascha Sohl-Dickstein, Virginia Smith, and Luke Metz. Variance-reduced gradient\nestimation via noise-reuse in online evolution strategies. In _Thirty-seventh Conference on Neural Information_\n_Processing Systems_, 2023a.\n\nOscar Li, James Harrison, Jascha Sohl-Dickstein, Virginia Smith, and Luke Metz. Noise-Reuse in Online\n[Evolution Strategies, April 2023b. URL http://arxiv.org/abs/2304.12180. arXiv:2304.12180](http://arxiv.org/abs/2304.12180)\n\n[cs].\n\n[Jarek Liesen, Chris Lu, and Robert Lange. rejax, 2024. URL https://github.com/keraJLi/rejax.](https://github.com/keraJLi/rejax)\n\nRyan Lowe, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for\nmixed cooperative-competitive environments. _Advances in neural information processing systems_, 30, 2017.\n\nChris Lu, Jakub Kuba, Alistair Letcher, Luke Metz, Christian Schroeder de Witt, and Jakob Foerster. Discovered policy optimisation. _Advances in Neural Information Processing Systems_, 35:16455\u201316468, 2022.\n\nH. M. Macdonald. Zeroes of the bessel functions. _Proceedings of the London Mathematical Society_, 30:\n165\u2013179, 1899. doi: 10.1112/plms/s1-30.1.165.\n\nSadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, and Sanjeev Arora.\nFine-tuning language models with just forward passes. In _Proceedings of the 37th International Conference_\n_on Neural Information Processing Systems_, NIPS \u201923, Red Hook, NY, USA, 2023. Curran Associates Inc.\n\nMichael Matthews, Michael Beukman, Benjamin Ellis, Mikayel Samvelyan, Matthew Jackson, Samuel\nCoward, and Jakob Foerster. Craftax: A lightning-fast benchmark for open-ended reinforcement learning.\n_arXiv preprint arXiv:2402.16801_, 2024.\n\nMichael T. Matthews, Michael Beukman, Chris Lu, and Jakob Nicolaus Foerster. Kinetix: Investigating\nthe training of general agents through open-ended physics-based control tasks. In _ICLR_, 2025. URL\n[https://openreview.net/forum?id=zCxGCdzreM.](https://openreview.net/forum?id=zCxGCdzreM)\n\n[P. McCullagh. Cumulants (lecture notes). Lecture notes, University of Chicago (pdf), 2013. URL https:](https://www.stat.uchicago.edu/~pmcc/courses/stat306/2013/cumulants.pdf)\n[//www.stat.uchicago.edu/~pmcc/courses/stat306/2013/cumulants.pdf.](https://www.stat.uchicago.edu/~pmcc/courses/stat306/2013/cumulants.pdf)\n\nPeter McCullagh. _Tensor Methods in Statistics_ . Monographs on Statistics and Applied Probability. Chapman\n[& Hall / CRC, London, 1987. URL https://www.stat.uchicago.edu/~pmcc/tensorbook/](https://www.stat.uchicago.edu/~pmcc/tensorbook/DoverEdition.pdf)\n[DoverEdition.pdf. Discusses multivariate moments and cumulants (cumulant tensors).](https://www.stat.uchicago.edu/~pmcc/tensorbook/DoverEdition.pdf)\n\nWilliam Merrill, Jackson Petty, and Ashish Sabharwal. The illusion of state in state-space models. In\nRuslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and\nFelix Berkenkamp (eds.), _Proceedings of the 41st International Conference on Machine Learning_, volume\n235 of _Proceedings of Machine Learning Research_, pp. 35492\u201335506. PMLR, 21\u201327 Jul 2024. URL\n[https://proceedings.mlr.press/v235/merrill24a.html.](https://proceedings.mlr.press/v235/merrill24a.html)\n\nLuke Metz, James Harrison, C. Daniel Freeman, Amil Merchant, Lucas Beyer, James Bradbury, Naman\nAgrawal, Ben Poole, Igor Mordatch, Adam Roberts, and Jascha Sohl-Dickstein. VeLO: Training Versatile\n[Learned Optimizers by Scaling Up, November 2022. URL http://arxiv.org/abs/2211.09760.](http://arxiv.org/abs/2211.09760)\narXiv:2211.09760 [cs, math, stat].\n\nJack Parker-Holder, Vu Nguyen, and Stephen Roberts. Provably Efficient Online Hyperparameter Opti[mization with Population-Based Bandits, June 2021. URL http://arxiv.org/abs/2002.02518.](http://arxiv.org/abs/2002.02518)\narXiv:2002.02518 [cs].\n\n15\n\nBo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Xingjian Du, Haowen Hou, Jiaju Lin, Jiaxing\nLiu, Janna Lu, William Merrill, Guangyu Song, Kaifeng Tan, Saiteja Utpala, Nathan Wilce, Johan S. Wind,\nTianyi Wu, Daniel Wuttke, and Christian Zhou-Zheng. Rwkv-7 \"goose\" with expressive dynamic state\n[evolution, 2025. URL https://arxiv.org/abs/2503.14456.](https://arxiv.org/abs/2503.14456)\n\n[K. B. Petersen and M. S. Pedersen. The matrix cookbook, October 2008. URL http://www2.imm.dtu.](http://www2.imm.dtu.dk/pubdb/p.php?3274)\n[dk/pubdb/p.php?3274. Version 20081110.](http://www2.imm.dtu.dk/pubdb/p.php?3274)\n\nEduardo Pignatelli, Jarek Liesen, Robert Tjarko Lange, Chris Lu, Pablo Samuel Castro, and Laura Toni. Navix:\n[Scaling minigrid environments with jax, 2024. URL https://arxiv.org/abs/2407.19396.](https://arxiv.org/abs/2407.19396)\n\nXin Qiu, Yulu Gan, Conor F. Hayes, Qiyao Liang, Elliot Meyerson, Babak Hodjat, and Risto Miikkulainen.\n[Evolution strategies at scale: Llm fine-tuning beyond reinforcement learning, 2025. URL https://](https://arxiv.org/abs/2509.24372)\n[arxiv.org/abs/2509.24372.](https://arxiv.org/abs/2509.24372)\n\nI. Rechenberg. Evolutionsstrategien. In Berthold Schneider and Ulrich Ranft (eds.), _Simulationsmethoden_\n_in der Medizin und Biologie_, pp. 83\u2013114, Berlin, Heidelberg, 1978. Springer Berlin Heidelberg. ISBN\n978-3-642-81283-5.\n\nV. K. Rohatgi. _An introduction to probability theory and mathematical statistics_ . Wiley series in probability\nand mathematical statistics. Wiley, New York, 1976. ISBN 0471731358.\n\nFrank. Rosenblatt. _Principles of neurodynamics : perceptrons and the theory of brain mechanisms._ Spartan\nBooks, Washington, 1962.\n\nAlexander Rutherford, Benjamin Ellis, Matteo Gallici, Jonathan Cook, Andrei Lupu, Gardar Ingvarsson,\nTimon Willi, Akbir Khan, Christian Schroeder de Witt, Alexandra Souly, et al. Jaxmarl: Multi-agent rl\nenvironments in jax. _arXiv preprint arXiv:2311.10090_, 2023.\n\nTim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable\n[alternative to reinforcement learning, 2017. URL https://arxiv.org/abs/1703.03864.](https://arxiv.org/abs/1703.03864)\n\nJohn K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw. Parallel random numbers: As easy as 1, 2,\n3. In _SC \u201911: Proceedings of 2011 International Conference for High Performance Computing, Networking,_\n_Storage and Analysis_, pp. 1\u201312, 2011. doi: 10.1145/2063384.2063405.\n\nMd Kamruzzaman Sarker, Lu Zhou, Aaron Eberhart, and Pascal Hitzler. Neuro-symbolic artificial intelligence:\n[Current trends, 2021. URL https://arxiv.org/abs/2105.05330.](https://arxiv.org/abs/2105.05330)\n\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang,\nY. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open\n[language models, 2024. URL https://arxiv.org/abs/2402.03300.](https://arxiv.org/abs/2402.03300)\n\nJasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learning\n[algorithms, 2012. URL https://arxiv.org/abs/1206.2944.](https://arxiv.org/abs/1206.2944)\n\nFelipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O. Stanley, and Jeff\nClune. Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural\n[Networks for Reinforcement Learning, April 2018. URL http://arxiv.org/abs/1712.06567.](http://arxiv.org/abs/1712.06567)\narXiv:1712.06567 [cs].\n\nLaurits Tani, Diana Rand, Christian Veelken, and Mario Kadastik. Evolutionary algorithms for hyperparameter\noptimization in machine learning for application in high energy physics. _The European Physical Journal C_,\n81(2):170, February 2021. ISSN 1434-6044, 1434-6052. doi: 10.1140/epjc/s10052-021-08950-y. URL\n[http://arxiv.org/abs/2011.04434. arXiv:2011.04434 [hep-ex].](http://arxiv.org/abs/2011.04434)\n\nNico M Temme. _Bessel Functions_, chapter 9, pp. 219\u2013255. John Wiley and Sons, Ltd, 1996. ISBN\n[9781118032572. doi: https://doi.org/10.1002/9781118032572.ch9. URL https://onlinelibrary.](https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118032572.ch9)\n[wiley.com/doi/abs/10.1002/9781118032572.ch9.](https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118032572.ch9)\n\n16\n\nSebastian Towers, Aleksandra Kalisz, Philippe A. Robert, Alicia Higueruelo, Francesca Vianello, MingHan Chloe Tsai, Harrison Steel, and Jakob N. Foerster. ADIOS: Antibody Development via Opponent\n[Shaping, June 2025. URL http://arxiv.org/abs/2409.10588. arXiv:2409.10588 [q-bio].](http://arxiv.org/abs/2409.10588)\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett (eds.), _Advances in Neural Information Processing Systems_,\n[volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n[files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n\nPaul Vicol, Luke Metz, and Jascha Sohl-Dickstein. Unbiased gradient estimation in unrolled computation\ngraphs with persistent evolution strategies. In Marina Meila and Tong Zhang (eds.), _Proceedings of the_\n_38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning_\n_Research_ [, pp. 10553\u201310563. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/](https://proceedings.mlr.press/v139/vicol21a.html)\n[v139/vicol21a.html.](https://proceedings.mlr.press/v139/vicol21a.html)\n\nPaul Vicol, Zico Kolter, and Kevin Swersky. Low-Variance Gradient Estimation in Unrolled Computation Graphs with ES-Single, April 2023. [URL http://arxiv.org/abs/2304.11153.](http://arxiv.org/abs/2304.11153)\narXiv:2304.11153 [cs, stat].\n\nAmala Mary Vincent and P. Jidesh. An improved hyperparameter optimization framework for AutoML\nsystems using evolutionary algorithms. _Scientific Reports_, 13(1):4737, March 2023. ISSN 2045-2322. doi:\n[10.1038/s41598-023-32027-3. URL https://doi.org/10.1038/s41598-023-32027-3.](https://doi.org/10.1038/s41598-023-32027-3)\n\nG. N. Watson. _A Treatise on the Theory of Bessel Functions_ . Cambridge University Press, Cambridge, 2\nedition, 1944. Reprinted with corrections, various later printings.\n\nG. B. Whitham. _Linear and nonlinear waves_ . Pure and applied mathematics. Wiley-Interscience, New York,\n1999. ISBN 9786613306241.\n\nDaan Wierstra, Tom Schaul, Jan Peters, and J\u00fcrgen Schmidhuber. Natural evolution strategies. pp. 3381\u20133387,\n06 2008. doi: 10.1109/CEC.2008.4631255.\n\nDaan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, and J\u00fcrgen Schmidhuber. Natural evolution\n[strategies, 2011. URL https://arxiv.org/abs/1106.4487.](https://arxiv.org/abs/1106.4487)\n\nC.S. Withers. A simple expression for the multivariate hermite polynomials. _Statistics and Probability Letters_,\n47(2):165\u2013169, 2000. ISSN 0167-7152. doi: https://doi.org/10.1016/S0167-7152(99)00153-4. URL\n[https://www.sciencedirect.com/science/article/pii/S0167715299001534.](https://www.sciencedirect.com/science/article/pii/S0167715299001534)\n\nKe Xue, Chao Qian, Ling Xu, and Xudong Fei. Evolutionary gradient descent for non-convex optimization.\nIn Zhi-Hua Zhou (ed.), _Proceedings of the Thirtieth International Joint Conference on Artificial Intelli-_\n_gence, IJCAI-21_, pp. 3221\u20133227. International Joint Conferences on Artificial Intelligence Organization,\n[8 2021. doi: 10.24963/ijcai.2021/443. URL https://doi.org/10.24963/ijcai.2021/443.](https://doi.org/10.24963/ijcai.2021/443)\nMain Track.\n\nZiming Yu, Pan Zhou, Sike Wang, Jia Li, Mi Tian, and Hua Huang. Zeroth-order fine-tuning of llms in random\nsubspaces. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pp.\n4475\u20134485, October 2025.\n\nYihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang, Wenqing Zheng, Pin-Yu Chen, Jason D.\nLee, Wotao Yin, Mingyi Hong, Zhangyang Wang, Sijia Liu, and Tianlong Chen. Revisiting zeroth-order\noptimization for memory-efficient llm fine-tuning: A benchmark, 2024.\n\n17\n\n**Appendix**\n\n**A Proofs** **19**\n\nA.1 ES Matrix Gradient Deviations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n\nA.2 Error Rate Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n\n**B** **Mean Field Score Function Approximator** **27**\n\nB.1 Bessel Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n\nB.2 Derivation of Mean-field Approximators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n\n**C EGG Architecture** **33**\n\nC.1 Notation and Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n\nC.2 Parameter Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n\nC.3 Matrix Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n\nC.4 Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n\nC.5 Layer Normalization (LN) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n\nC.6 MLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n\nC.7 GRU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n\nC.8 Fitness Calculation in Integer Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n\n**D EGG Pretraining with Integer EGGROLL** **35**\n\nD.1 Adding EGGROLL Perturbations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\nD.2 Fitness Shaping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\nD.3 Parameter Update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\n**E** **EGG Ablations** **36**\n\nE.1 Data Efficient Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\nE.2 Controlling For Data Across Population Sizes . . . . . . . . . . . . . . . . . . . . . . . . . 37\n\n**F** **EGGROLL Speed**"}, {"role": "assistant", "content": "**Verdict:** Highly impactful. This technology dramatically reduces the resource requirements for blackbox optimization, making previously intractable training scenarios feasible for multi-billion parameter models and non-differentiable systems.\n\n**Summary:** EGGROLL offers significant computational and memory savings by scaling evolutionary strategies with low-rank search matrices. It achieves large speedups over naive ES, demonstrating effectiveness even at rank r=1 with negligible performance loss. This enables end-to-end training for large language models, multi-agent reinforcement learning, and crucially, large-scale neuro-symbolic systems with non-differentiable components where gradient-based methods fail. Hardware resource usage, while still significant for billion-parameter models, is substantially lower than alternative ES methods, reducing costs and expanding the scope of problems that can be tackled. The primary utility is unlocking blackbox optimization for ultra-large and complex AI systems."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n_\ud835\udc5a\ud835\udc4e\ud835\udc65_ ( _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5d\ud835\udc5f\ud835\udc56\ud835\udc5c\ud835\udc5f_ _ _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51,\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ ) ;\n\n**181** BBR_Restore_Cwnd():\n\n**182** _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ = _\ud835\udc5a\ud835\udc4e\ud835\udc65_ ( _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51, \ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5d\ud835\udc5f\ud835\udc56\ud835\udc5c\ud835\udc5f_ _ _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ ) ;\n\nEvaluating the Impact of Packet Scheduling and Congestion Control Algorithms on MPTCP Performance over\nHeterogeneous Networks 57\n\n**Algorithm 13:** Coupled Multipath BBR (C-MPBBR) - [theoretical part in 2.3.7]\n\n**Fulfilling Goal 1 (MPTCP incentive):**\n\n**1** _\ud835\udefd_ = 40, _\ud835\udc47\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59_ _ _\ud835\udc37\ud835\udc52\ud835\udc59_ _ _\ud835\udc45\ud835\udc61_ = 0 ;\n\n**2** _\ud835\udc59\ud835\udc5c\ud835\udc64\ud835\udc52\ud835\udc60\ud835\udc61_ _ _\ud835\udc4f\ud835\udc64_ _ _\ud835\udc4e\ud835\udc5a\ud835\udc5c\ud835\udc5b\ud835\udc54_ _ _\ud835\udc4e\ud835\udc59\ud835\udc59_ _ _\ud835\udc46\ud835\udc39\ud835\udc60_ = 999999999 ;\n\n**3** _\u210e\ud835\udc56\ud835\udc54\u210e\ud835\udc52\ud835\udc60\ud835\udc61_ _ _\ud835\udc4f\ud835\udc64_ _ _\ud835\udc4e\ud835\udc5a\ud835\udc5c\ud835\udc5b\ud835\udc54_ _ _\ud835\udc4e\ud835\udc59\ud835\udc59_ _ _\ud835\udc46\ud835\udc39\ud835\udc60_ = 0 ;\n\n**4** _\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59_ _ _\ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f_ _ _\ud835\udc5c\ud835\udc53_ _ _\ud835\udc46\ud835\udc39\ud835\udc60_ = 0 ;"}, {"role": "assistant", "content": "**Verdict:** Foundational setup code for a multi-path BBR implementation. Low inherent value on its own, but essential for the algorithm's operational integrity.\n\n**Summary:** This section outlines basic congestion window recovery logic and initial variable setup for a Coupled Multipath BBR (C-MPBBR) algorithm. The `BBR_Restore_Cwnd()` function is a critical component for maintaining stable throughput by preventing unnecessary `cwnd` reductions, ensuring the connection always aims for an effective sending rate. The initialization of C-MPBBR variables (`beta`, bandwidth trackers, subflow counter) lays the groundwork for managing multiple network paths, which is key for MPTCP's goal of improved resource utilization. There are no significant hardware requirements or direct costs associated with this specific code snippet; it represents software-level logic. Its overall utility and value depend entirely on the C-MPBBR algorithm's ability to demonstrate performance gains in heterogeneous network environments, potentially improving MPTCP's efficiency and incentives for path usage."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n_\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc53\ud835\udc56\ud835\udc59\ud835\udc59\ud835\udc52\ud835\udc51_\ud835\udc5d\ud835\udc56\ud835\udc5d\ud835\udc52_ **or** _\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52_ _> \ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc56\ud835\udc5b\ud835\udc54_\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52_ **then**\n\n**137** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc56\ud835\udc5b\ud835\udc54_ _ _\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52_ = _\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52_ ;\n\n**138** BBR_Set_Send_Quantum():"}, {"role": "assistant", "content": "**Verdict:** This code provides a critical adaptive mechanism for network throughput, enabling faster, more efficient data transfer in dynamic network conditions.\n\n**Summary:** This segment outlines an essential, low-overhead logic update for BBR congestion control. Its primary utility is to rapidly adjust the data sending rate upward ('pacing_rate') whenever the network demonstrates a higher actual delivery capability ('rate'), or when the BBR algorithm has initially estimated the full network capacity ('filled_pipe'). This enables quicker throughput scaling and better utilization of available bandwidth. From a cost and hardware perspective, this is a software-only logic update, imposing negligible computational overhead. Its value lies in reducing latency and increasing effective bandwidth for applications, leading to improved user experience and more efficient use of network infrastructure without requiring any specialized hardware or significant processing power. It directly translates to faster downloads, smoother streaming, and more responsive cloud services."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n**HLS as Reference**\n\nThe overall workflow of the proposed HDL Design Agent is shown\nin Fig. 4. Given a C/C++ program with its specification and a corresponding C/C++ testbench from the HLS benchmark suite [52, 53],\nthe agent executes a three-stage process to produce a functionally\ncorrect HDL design. The agent interacts with a toolkit comprising tool APIs (HLS, RTL simulation, logic synthesis), a knowledge\nbase of HDL syntax rules, and automation scripts for simulation\nand comparison. In stage (1), described in Section 3.1, the C/C++\nprogram is decomposed into submodules according to the rules in\nSection 3.1.1. For each submodule, the LLM generates a corresponding HDL module (Design Under Test, DUT), and syntax errors are\nrepaired through the RAG mechanism. In stage (2), described in\nSection 3.2, each C/C++ submodule is synthesized by the HLS tool\ninto a functionally correct HDL as the golden reference. The DUT\nand the golden reference are then simulated under the same stimuli. During this process, HLS tools can automatically translate the\noriginal C/C++-based testbench into an equivalent HDL testbench\nto validate the HLS-generated HDL design, whereas the testbench\nfor the LLM-generated HDL design is adapted from the original\nC/C++ testbench using the LLM and then verified by humans. Any\ndetected functional mismatches are fed back to the LLM for iterative\n\nenhancement. In stage (3), described in Section 3.3, all verified LLMgenerated submodules are instantiated into a top-level design, and\ndifferential verification is repeated to ensure integration correctness.\n\n**3.1** **LLM-Assisted HDL Design Generation**\n\n_3.1.1_ _C/C++ Program Decomposition._ Directly feeding an entire\nC/C++ program with long context into the LLM tends to dilute attention, preventing it from capturing fine-grained hardware semantics,\ne.g., customized bit widths, type conversions, etc. This often leads\nto deep functional errors that are difficult to debug. To mitigate this\nissue, a divide-and-conquer strategy based on C/C++ decomposition\nis adopted before HDL generation. The original C/C++ program is\ndecomposed into smaller, LLM-friendly C/C++ submodules, thereby\nimproving the quality of the initial HDL design generation.\n\n**Figure 6: Syntax repair for LLM-generated HDL using RAG.**\n\nThe decomposition follows three strict rules: (1) Function-Level\nGranularity: Decomposition is performed along function boundaries,\nrather than by arbitrary line-level splitting. (2) Explicit I/O definition:\nEach submodule uses fixed-width scalars or static arrays as interfaces (e.g., int8_t state[16]), with all bit widths explicitly specified.\n(3) Single and Clear Semantics: Each submodule should have clear\nsemantics (e.g., SubBytes() performs only SBox substitution).\nTo ensure correctness after decomposition, a differential verification step is performed. All decomposed C/C++ submodules are\nre-integrated into a top-level C/C++ program and verified against\nthe original C/C++ testbench. The decomposition is considered successful only when the output results match exactly.\n_3.1.2_ _LLM-Assisted HDL Design Generation._ After decomposition,\nthe agent proceeds to the HDL generation phase as shown in Fig. 5.\nFor each C/C++ submodule, we generate two specification files in natural language using the LLM. (1) Functional Specification: A detailed\ndescription of the functionality and expected hardware behavior\nof the C/C++ submodule. (2) Interface Definition: Precisely maps\nC/C++ parameters to Verilog ports, including names, directions, and\nexact bit widths. With these two specification files, the LLM can\nbetter understand the design constraints of individual submodules\nand produce valid HDL designs from the C/C++ snippet effectively.\nTo guide the LLM toward producing effective HDL designs, we\nalso define the design, and formatting constraints as follows: (1)\nSeparating control logic from the datapath and using synchronous\nreset conventions; (2) Optimization strategies for the circuits, e.g.,\ninserting pipeline stages; (3) Formatting constraints such as enclosing the resulting HDL between triple backticks to facilitate HDL\nextraction from the output of the LLM using a script. The C/C++\nsubmodule code, functional specification, interface definition, design\nand formatting constraints are jointly provided to the LLM, guiding\nit to generate the corresponding HDL design of a submodule.\nDue to LLM hallucinations, the HDL design generated for a submodule often fails syntax compilation. As shown in Fig. 5, these\nerrors can be repaired by a retrieval-augmented generation (RAG)\nmechanism, which involves three stages: First, an external rule library is created containing HDL syntax rule templates. Each template includes error logs that may be triggered during syntax compilation, and a summary of the corresponding repair rule extracted\nfrom the user manual. Second, when an LLM-generated HDL design fails compilation, the error log is embedded and then matched\nagainst the templates in a rule library using a sentence transformer\n\nTABLE 1: Typical Functional Discrepancies in LLM-Generated HDL\nand the Corresponding Corrections Driven by _CorrectHDL_ .\n\n**Task** **DUT Output** **HLS Reference** **Functional Discrepancy Corresponding Correction**\n\n|< / > C/C++ Program SubBytes() \u2026 ShiftRows() \u2026 MixColumns() \u2026 AddRoundKey() \u2026<br>SameTestStimulus|Col2|\n|---|---|\n|LLM\u2013Generated HDL (DUT)<br>**DUT Path**|HLS\u2013Generated HDL (Golden Reference)<br>**Golden Path**|\n\nEdge [-3, 66, \u2026, -250, 108] [-31, 664, \u2026, -2479, 1081] Numeric scaling bug\n\nCorrect shift scaling\n\nMarkov [5, 22, 17, \u2026, 9, 14, 14] [5, 22, 17, \u2026, 9, 14, 15] Boundary write-back bug Commit delayed by\n\none cycle\n\n![](papers/images/CorrectHDL-Agentic-HDL-Design-with-LLMs-Leveraging-HighLevel-Synthesis-as-Reference.pdf-3-0.png)\n\n**Figure 7: Differential verification for LLM-generated HDL.**\n\n![](papers/images/CorrectHDL-Agentic-HDL-Design-with-LLMs-Leveraging-HighLevel-Synthesis-as-Reference.pdf-3-1.png)\n\n**Figure 8: Functional enhancement in LLM-generated HDL.**\n\n[50]. The retrieval process can be formalized as follows:\n\n_\ud835\udc38_ = embed(error_log) _,_ _\ud835\udc45\ud835\udc56_ = embed(rule_template _\ud835\udc56_ ) _,_ (1)\n\nwhere _\ud835\udc38_ and _\ud835\udc45\ud835\udc56_ denote the embedding vectors of the error log and\nthe _\ud835\udc56_ th rule template. Given an error log _\ud835\udc38_ and a template _\ud835\udc45_, the rule\ntemplate with the highest similarity is selected as follows:\n\n_\ud835\udc38_          - _\ud835\udc45_\nsim( _\ud835\udc38, \ud835\udc45_ ) = _\ud835\udc45_ [\u2217] = arg max sim( _\ud835\udc38, \ud835\udc45\ud835\udc56_ ) _,_ (2)\n\u2225 _\ud835\udc38_ \u2225\u2225 _\ud835\udc45_ \u2225 _[,]_ _\ud835\udc45\ud835\udc56_ \u2208R\n\nwhere R denotes all templates in the library, _\ud835\udc45_ [\u2217] represents the template with the highest similarity. Third, once the most relevant rule\ntemplate is retrieved, it is incorporated into the LLM prompt to guide\nthe repair of the HDL design into a syntactically correct version.\nAn example of repairing an LLM-generated HDL design is given in\nFig. 6. The agent uses compiler logs to retrieve repair rules and then\nprompts the LLM to repair the HDL, and repeats this process until\ncompilation succeeds or the predefined iteration limit is reached.\n\n**3.2** **Differential Verification and Functional**\n**Enhancement**\n\nAfter syntax errors in LLM-generated HDL designs are corrected as\ndescribed in Section 3.1.2, we leverage the HLS-generated HDL as a\ngolden functional reference to enhance its functional correctness, as\nshown in Fig. 7. For each C/C++ submodule, the HLS tool synthesizes\na bit-accurate and functionally correct HDL design as a reference.\nDespite its suboptimal area/power efficiency and limited readability,\nits functional behavior is highly reliable [2\u20135] and thus serves as an\neffective golden reference for verifying LLM-generated design.\n\nDMM [262144, 0, 4096, \u2026] [262144, 4096, 133120,\u2026] Reset or delay bug Decouple load and compute\n\nAES 1: [xx, xx, xx, ..., xx, xx] 1: [8e, a2, b7, ..., 60, 89] Lag due to synchronous Align output register with\n2: [8e, a2, b7, ..., 60, 89] 2: [6b, c1, be, ..., ef, 63] assignment timing bug next state or delay valid\n\nKMP [0, 8, 0, \u2026, 8, 0, 8] [7, 64, 9, \u2026, 59, 0, 560] Prefix skipped by slicing Compare full constant\n\nN-body [1, 0, 1, 1, 1,\u2026, 0, 1, 1] [1, 1, 0, 1, 1\u2026, 1, 0, 1] Inter-beat misalignment Use local blocking temps\n\nBFS [1, 0, 3, 0, 2, 1, 0, 1, \u2026] [1, 1, 4, 4, 6, 7, 7, 8\u2026] State retention across calls Add per-call initialization\n\nTo enable bit-accurate comparison, the LLM-generated HDL design as DUT and the HLS-generated HDL design as golden reference are simulated under identical test stimuli derived from the\n\nC/C++ testbench. Modern HLS tools can automatically translate\nthe C/C++ testbench into an equivalent HDL testbench to test the\nHLS-generated HDL. The corresponding HDL testbench for the LLMgenerated HDL is adapted from the same C/C++ testbench via the\nLLM with human verification to ensure its correctness in validating\nthe LLM-generated HDL design. The new testbench is examined by\nhuman engineers to ensure its quality.\nThe agent then initiates a parallel simulation flow shown in Fig. 7,\nwhere in the DUT Path, the LLM-generated HDL design is simulated\nin an RTL simulation environment, and in the Golden Path, the\nHLS-generated HDL design is simulated in the HLS simulation environment. During both simulations, all output ports are recorded and\ndumped into result files. A comparison script conducts automated\ndifferential checking between these files. If all outputs match, the\nDUT is considered functionally correct and collected for subsequent\nintegration. If any mismatch is observed, the DUT is marked as\nfunctionally incorrect, and the functional repair loop is triggered.\nThe repair loop uses the mismatched results as precise feedback\nto guide the LLM in the functional repair of the DUT. A structured\nprompt is constructed that contains the current (incorrect) Verilog\ncode of the DUT, the C/C++ program that defines the intended\nfunctional behavior, and a mismatch log produced by differential\ncomparison. This mismatch log explicitly records the test stimulus, the actual DUT output, and the expected golden reference for\neach failing test case. In addition, a step-wise reasoning method inspired by Chain-of-Thought is adopted to repair the LLM-generated\nHDL design. As shown in Fig. 8, the prompt explicitly instructs\nthe LLM to strengthen functional understanding by analyzing the\nC/C++ program and summarizing the intended behavior. It then\nconducts behavior and difference analysis through reviewing the\nHDL design, correlating it with the expected functionalities, and\nidentifying the source of the mismatches. The root cause of errors\nis identified and fixed by explaining the simulation discrepancy.\nWith such concrete feedback, the LLM can then perform targeted\ndebugging rather than blind editing. If mismatches remain, the verification\u2013feedback\u2013repair iteration is repeated until functional equivalence is achieved or the predefined iteration limit is reached.\nA concrete example of the LLM-driven reasoning chain is shown\nin Fig. 8. Guided by this structured prompt, the LLM first identifies\nthe small value differences stemming from inconsistent rounding\nsemantics between the DUT and the golden fixed-point implementation. It then proposes a revised HDL version that corrects the\nrounding behavior. In the second check, the LLM observes that the\nfirst test case is almost correct but the second is completely wrong,\nand traces this to a missing state retention in the control logic. It\nproposes latching the intermediate result in the ST_WAIT_LBL state\n\nTABLE 2: Comparison of _CorrectHDL_ with Baselines in Simulation Pass Rate, and with HLS in Area and Power Efficiency.\n\n|Benchmark|FunctionalSimulationPassRate(%)|Col3|Col4|Col5|Col6|Col7|HDL<br>Readability<br>(CodeLines)|Col9|AreaOverhead(\u00b5m\u00b2)|Col11|Col12|PowerConsumption(mW)|Col14|Col15|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|**Benchmark**|**LLM**<br>**\u2192**<br>**(NL as**<br>**HDL**<br>[29]<br>**Input)**|<br>**LLM\u2192**<br>**HDL**<br>**(C as**<br>**Input)**<br>**(**<br>**S**|**LLM\u2192HDL**<br>**w/ Task Decomp.**|**LLM\u2192HDL**<br>**w/ Task Decomp.**|** )**<br>**Proposed**<br>**CorrectHDL**<br>**(w/ HLS as Refer.)**|** )**<br>**Proposed**<br>**CorrectHDL**<br>**(w/ HLS as Refer.)**|** )**<br>**Proposed**<br>**CorrectHDL**<br>**(w/ HLS as Refer.)**|** )**<br>**Proposed**<br>**CorrectHDL**<br>**(w/ HLS as Refer.)**|** )**<br>**Proposed**<br>**CorrectHDL**<br>**(w/ HLS as Refer.)**|** )**<br>**Proposed**<br>**CorrectHDL**<br>**(w/ HLS as Refer.)**|** )**<br>**Proposed**<br>**CorrectHDL**<br>**(w/ HLS as Refer.)**|** )**<br>**Proposed**<br>**CorrectHDL**<br>**(w/ HLS as Refer.)**|** )**<br>**Proposed**<br>**CorrectHDL**<br>**(w/ HLS as Refer.)**|** )**<br>**Proposed**<br>**CorrectHDL**<br>**(w/ HLS as Refer.)**|\n|**Benchmark**|**LLM**<br>**\u2192**<br>**(NL as**<br>**HDL**<br>[29]<br>**Input)**|<br>**LLM\u2192**<br>**HDL**<br>**(C as**<br>**Input)**<br>**(**<br>**S**|**LLM\u2192HDL**<br>**w/ Task Decomp.**|**LLM\u2192HDL**<br>**w/ Task Decomp.**|** )**<br>**Proposed**<br>**CorrectHDL**<br>**(w/ HLS as Refer.)**|** )**<br>**Proposed**<br>**CorrectHDL**<br>**(w/ HLS as Refer.)**|**HLS-**<br>**Gen**<br>**HDL**|**Prop.**<br>**Correct-**<br>**HDL**|**HLS-Gen**<br>**HDL**[54]|**Prop.**<br>**Correct-**<br>**HDL**|**(%)**|**HLS-Gen**<br>**HDL**[54]|**Prop.**<br>**Correct-**<br>**HDL**|**(%)**|\n|**Benchmark**|**LLM**<br>**\u2192**<br>**(NL as**<br>**HDL**<br>[29]<br>**Input)**|<br>**LLM\u2192**<br>**HDL**<br>**(C as**<br>**Input)**<br>**(**<br>**S**|**ubmod.**|**Top**|**Submod.**|**Top**|**Top**|**Top**|**Top**|**Top**|**Top**|**Top**|**Top**|**Top**|\n|AdvancedEncryptionStandard|6.25|12.5|48.44|18.75|76.56|62.5|1879|386|9591|7836|18.30|4.30|2.92|32.09|\n|Breadth-First Search|12.5|6.25|28.13|12.5|65.63|56.25|1392|234|29181|24101|17.41|9.98|8.08|19.04|\n|Edge Detection Algorithm|0|0|27.08|6.25|64.58|37.5|4753|388|35422|30543|13.77|16.04|13.67|14.78|\n|Median Filter|18.75|25|53.13|31.25|90.63|75|1351|213|11207|8249|26.39|4.84|3.61|25.41|\n|Dense Matrix Multiplication|12.5|31.25|46.88|37.5|78.13|62.5|568|179|33044|21061|36.26|16.32|13.01|20.28|\n|Knuth-Morris-Pratt|25|18.75|40.63|31.25|81.25|56.25|966|238|32631|20320|37.73|3.84|2.39|37.76|\n|MIPS Processor|0|0|35.42|6.25|58.33|31.25|3033|397|30698|24483|20.25|14.35|10.53|26.62|\n|N-body Molecular Dynamics|6.75|18.75|39.58|18.75|75.08|50|1086|270|25444|16007|37.09|5.89|3.41|42.11|\n|Dynamic Sequence Alignment|12.5|12.5|33.33|25|60.42|43.75|2998|325|23138|18104|21.76|10.05|7.69|23.48|\n|Secure Hash Algorithm|0|6.25|41.67|18.75|79.17|50|4421|420|22729|20898|8.06|9.40|7.81|16.91|\n|Spam Filtering|12.5|18.75|43.75|31.25|84.38|56.25|1396|183|17571|11475|34.69|6.40|4.18|34.69|\n|Markov Model Probabilities|6.25|6.25|34.38|12.5|65.63|37.5|1388|292|38598|28487|26.20|18.86|13.1|30.54|\n\n - The simulation pass rate is calculated based on 16 independent generation rounds by the LLM for each task.\n\n![](papers/images/CorrectHDL-Agentic-HDL-Design-with-LLMs-Leveraging-HighLevel-Synthesis-as-Reference.pdf-4-0.png)\n\n**Figure 9: Design integration.**\nand reusing the stored value in subsequent calculations. After resimulation, the repaired DUT matches the golden outputs on all test\ncases, demonstrating how the agent can systematically analyze and\nrepair functional discrepancies with reliable functional references.\nTo better describe the types of functional errors handled by our\nframework, Table 1 summarizes several typical functional discrepancies observed between the DUT and the golden reference. For each\ntask, the table lists representative DUT and HLS golden outputs and\nhighlights how the LLM identifies the underlying root cause, such as\nnumeric scaling errors, reset or lag issues, or state-retention faults,\nand generates the corresponding fixes, including correcting fixedpoint scaling, decoupling load and compute operations, or adding\nper-call initialization. These examples indicate that the encountered\nbugs are not simple syntactic issues but functional errors that are\ndifficult to debug without a golden functional reference, highlighting\nits essential role in the repair process.\n\n**3.3** **Design Integration**\n\nAfter the LLM-generated HDL designs of all submodules pass differential verification, they are integrated into a functionally correct\ntop-level HDL design. To enable correct instantiation and wiring,\nthe LLM is prompted with the original C/C++ program from Section 3.1.1 and all verified HDL designs from Section 3.2 together\nwith their interface definitions from Section 3.1.2. Using this information, function calls and variable transfers in the C/C++ code are\nsystematically mapped to HDL module instances and signal connections, ensuring that data dependencies and interface contracts are\npreserved. The result is an LLM-generated top-level HDL design\nthat implements the behavior of the original C/C++ program.\nAs shown in Fig. 9, the integrated top-level HDL design may still\ncontain errors that are not exposed at the submodule level, such as\nmisconnected ports or misaligned valid signals. To validate the toplevel behavior, the original C/C++ program is then synthesized by\nthe HLS tool into an HLS-generated top HDL design, which serves\nas the golden reference. Differential verification is then performed\nat the top level, as in Section 3.2, to detect integration errors.\n\nTo efficiently locate integration errors, boundary signal instrumentation is inserted at all submodule interfaces in the top-level\ndesign. During verification, the agent compares not only the final\noutputs but also these internal boundary signals against the golden\nreference. Guided by this structured feedback, the LLM performs\ntargeted repair by using the backward slicing technique to trace the\norigin of the discrepancy [51]. The repair process is repeated until\nfunctional equivalence is achieved or the iteration limit is reached."}, {"role": "assistant", "content": "**Verdict:** This section outlines the core methodology for achieving functionally correct HDL designs from LLMs, a critical step for practical utility. The system significantly boosts simulation pass rates, making LLM-based hardware design viable for complex tasks.\n\n**Summary:** The proposed HDL Design Agent offers a three-stage workflow to produce functionally correct HDL from C/C++ using LLMs. Key utility is derived from addressing LLM limitations: program decomposition for context management, RAG for automated syntax repair, and differential verification using HLS-generated HDL as a reliable 'golden reference' for functional correctness. The iterative feedback loop, driven by precise mismatch logs, allows the LLM to debug and fix complex functional errors. This process dramatically improves the functional simulation pass rate of LLM-generated designs, turning unreliable initial outputs into robust ones (e.g., up to 90.63% pass rate for submodules). While requiring computational resources for LLM inference, HLS synthesis, and iterative simulations, the system delivers substantial value by automating a historically manual and error-prone process. The resulting LLM-generated HDL is also shown to be more readable (fewer code lines) and often more area and power-efficient compared to HLS-generated designs, providing a tangible benefit beyond just functional correctness."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n_\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc56\ud835\udc59\ud835\udc56\ud835\udc4f\ud835\udc5f\ud835\udc56\ud835\udc62\ud835\udc5a_\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc60_ [ _\ud835\udc5f_ ] \u2260 0 **then**\n\n**30** _\ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc60_ [ _\ud835\udc5f_ ] \u2190 _\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc56\ud835\udc59\ud835\udc56\ud835\udc4f\ud835\udc5f\ud835\udc56\ud835\udc62\ud835\udc5a_ _ _\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc60_ [ _\ud835\udc5f_ ]/ _\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59_ _ _\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52_ ;\n\n**On packet loss for subflow** _\ud835\udc5f_ **:**\n\n**31** _\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc56\ud835\udc59\ud835\udc56\ud835\udc4f\ud835\udc5f\ud835\udc56\ud835\udc62\ud835\udc5a_ _ _\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc60_ [ _\ud835\udc5f_ ] \u2190 0 ;\n\n**32** _\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52_ _ _\ud835\udc51\ud835\udc52\ud835\udc59\ud835\udc4e\ud835\udc66\ud835\udc60_ [ _\ud835\udc5f_ ] \u2190 0 ;\n\nThe theoretical aspects of _wVegas_ congestion control algorithm are provided in 2.3.5.\n\n52 D. Dimopoulos, et al.\n\n**Algorithm 12:** Bottleneck Bandwidth and Round-trip propagation time (BBR) - [theoretical part in 2.3.6]\n\n**1 Initialization:**\n\n**2** BBR_On_Connection_Init():\n\n**3** BBR_Init();\n\n**On ACK arrival:**\n\n**4** BBR_Update_On_ACK():\n\n**5** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc48\ud835\udc5d\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc52_ _ _\ud835\udc40\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59_ _ _\ud835\udc34\ud835\udc5b\ud835\udc51_ _ _\ud835\udc46\ud835\udc61\ud835\udc4e\ud835\udc61\ud835\udc52_ () ;\n\n**6** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc48\ud835\udc5d\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc52_ _ _\ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc5c\ud835\udc59_ _ _\ud835\udc43\ud835\udc4e\ud835\udc5f\ud835\udc4e\ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc60_ () ;\n\n**7** BBR_Update_Model_And_State():\n\n**8** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc48\ud835\udc5d\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc52_ _ _\ud835\udc35\ud835\udc61\ud835\udc59\ud835\udc35\ud835\udc64_ () ;\n\n**9** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc36\u210e\ud835\udc52\ud835\udc50\ud835\udc58_ _ _\ud835\udc36\ud835\udc66\ud835\udc50\ud835\udc59\ud835\udc52_ _ _\ud835\udc43\u210e\ud835\udc4e\ud835\udc60\ud835\udc52_ () ;\n\n**10** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc36\u210e\ud835\udc52\ud835\udc50\ud835\udc58_ _ _\ud835\udc39\ud835\udc62\ud835\udc59\ud835\udc59_ _ _\ud835\udc43\ud835\udc56\ud835\udc5d\ud835\udc52_ () ;\n\n**11** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc36\u210e\ud835\udc52\ud835\udc50\ud835\udc58_ _ _\ud835\udc37\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b_ () ;\n\n**12** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc48\ud835\udc5d\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc52_ _ _\ud835\udc45\ud835\udc47\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc5d_ () ;\n\n**13** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc36\u210e\ud835\udc52\ud835\udc50\ud835\udc58_ _ _\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc52_ _ _\ud835\udc45\ud835\udc47\ud835\udc47_ () ;\n\n**14** BBR_Update_Control_Parameters():\n\n**15** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc46\ud835\udc52\ud835\udc61_ _ _\ud835\udc43\ud835\udc4e\ud835\udc50\ud835\udc56\ud835\udc5b\ud835\udc54_ _ _\ud835\udc45\ud835\udc4e\ud835\udc61\ud835\udc52_ () ;\n\n**16** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc46\ud835\udc52\ud835\udc61_ _ _\ud835\udc46\ud835\udc52\ud835\udc5b\ud835\udc51_ _ _\ud835\udc44\ud835\udc62\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc62\ud835\udc5a_ () ;\n\n**17** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc46\ud835\udc52\ud835\udc61_ _ _\ud835\udc36\ud835\udc64\ud835\udc5b\ud835\udc51_ () ;\n\n**On packet transmission:**\n\n**18** BBR_On_Transmit():\n\n**19** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc3b\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc59\ud835\udc52_ _ _\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61_ _ _\ud835\udc39\ud835\udc5f\ud835\udc5c\ud835\udc5a_ _ _\ud835\udc3c\ud835\udc51\ud835\udc59\ud835\udc52_ () ;\n\n**Initialization function definitions:**\n\n**20** BBR_Init():\n\n**21** _\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61_ _ _\ud835\udc64\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc5c\ud835\udc64\ud835\udc52\ud835\udc51_ _ _\ud835\udc5a\ud835\udc4e\ud835\udc65_ _ _\ud835\udc53\ud835\udc56\ud835\udc59\ud835\udc61\ud835\udc52\ud835\udc5f_ ( _\ud835\udc53\ud835\udc56\ud835\udc59\ud835\udc61\ud835\udc52\ud835\udc5f_ = _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc35\ud835\udc61\ud835\udc59\ud835\udc35\ud835\udc64\ud835\udc39\ud835\udc56\ud835\udc59\ud835\udc61\ud835\udc52\ud835\udc5f, \ud835\udc63\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52_ = 0 _, \ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52_ = 0) ;\n\n**22** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5f\ud835\udc61\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc5d_ = _\ud835\udc46\ud835\udc45\ud835\udc47\ud835\udc47_ ? _\ud835\udc46\ud835\udc45\ud835\udc47\ud835\udc47_ : _\ud835\udc3c\ud835\udc5b\ud835\udc53_ ;\n\n**23** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5f\ud835\udc61\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc5d_ _ _\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5a\ud835\udc5d_ = _\ud835\udc41\ud835\udc5c\ud835\udc64_ () ;\n\n**24** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc52_ _ _\ud835\udc5f\ud835\udc61\ud835\udc61_ _ _\ud835\udc51\ud835\udc5c\ud835\udc5b\ud835\udc52_ _ _\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5a\ud835\udc5d_ = 0 ;\n\n**25** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc52_ _ _\ud835\udc5f\ud835\udc61\ud835\udc61_ _ _\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc51_ _ _\ud835\udc51\ud835\udc5c\ud835\udc5b\ud835\udc52_ = _\ud835\udc53\ud835\udc4e\ud835\udc59\ud835\udc60\ud835\udc52_ ;\n\n**26** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61_ _ _\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5f\ud835\udc63\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b_ = _\ud835\udc53\ud835\udc4e\ud835\udc59\ud835\udc60\ud835\udc52_ ;\n\n**27** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5d\ud835\udc5f\ud835\udc56\ud835\udc5c\ud835\udc5f_ _ _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ = 0 ;\n\n**28** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc56\ud835\udc51\ud835\udc59\ud835\udc52_ _ _\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61_ = _\ud835\udc53\ud835\udc4e\ud835\udc59\ud835\udc60\ud835\udc52_ ;\n\n**29** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc3c\ud835\udc5b\ud835\udc56\ud835\udc61_ _ _\ud835\udc45\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc51_ _ _\ud835\udc36\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5b\ud835\udc54_ () ;\n\n**30** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc3c\ud835\udc5b\ud835\udc56\ud835\udc61_ _ _\ud835\udc39\ud835\udc62\ud835\udc59\ud835\udc59_ _ _\ud835\udc43\ud835\udc56\ud835\udc5d\ud835\udc52_ () ;\n\n**31** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc3c\ud835\udc5b\ud835\udc56\ud835\udc61_ _ _\ud835\udc43\ud835\udc4e\ud835\udc50\ud835\udc56\ud835\udc5b\ud835\udc54_ _ _\ud835\udc45\ud835\udc4e\ud835\udc61\ud835\udc52_ () ;\n\n**32** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc38\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f_ _ _\ud835\udc46\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61\ud835\udc62\ud835\udc5d_ () ;\n\n**33** BBR_Init_Round_Counting():\n\n**34** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5b\ud835\udc52\ud835\udc65\ud835\udc61_ _ _\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc51_ _ _\ud835\udc51\ud835\udc52\ud835\udc59\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc51_ = 0 ;\n\n**35** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc51_ _ _\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61_ = _\ud835\udc53\ud835\udc4e\ud835\udc59\ud835\udc60\ud835\udc52_ ;\n\n**36** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc51_ _ _\ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc61_ = 0 ;\n\nEvaluating the Impact of Packet Scheduling and Congestion Control Algorithms on MPTCP Performance over\nHeterogeneous Networks 53\n\n**Initialization function definitions (cont.):**\n\n**37** BBR_Init_Full_Pipe():\n\n**38** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc53\ud835\udc56\ud835\udc59\ud835\udc59\ud835\udc52\ud835\udc51_ _ _\ud835\udc5d\ud835\udc56\ud835\udc5d\ud835\udc52_ = _\ud835\udc53\ud835\udc4e\ud835\udc59\ud835\udc60\ud835\udc52_ ;\n\n**39** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc53\ud835\udc62\ud835\udc59\ud835\udc59_ _ _\ud835\udc4f\ud835\udc64_ = 0 ;\n\n**40** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc53\ud835\udc62\ud835\udc59\ud835\udc59_ _ _\ud835\udc4f\ud835\udc64_ _ _\ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc61_ = 0 ;\n\n**41** BBR_Init_Pacing_Rate():\n\n**42** _\ud835\udc5b\ud835\udc5c\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc59_ _ _\ud835\udc4f\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc64\ud835\udc56\ud835\udc51\ud835\udc61\u210e_ = _\ud835\udc3c\ud835\udc5b\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc4e\ud835\udc59\ud835\udc36\ud835\udc64\ud835\udc5b\ud835\udc51_ /( _\ud835\udc46\ud835\udc45\ud835\udc47\ud835\udc47_ ? _\ud835\udc46\ud835\udc45\ud835\udc47\ud835\udc47_ : 1 _\ud835\udc5a\ud835\udc60_ ) ;\n\n**43** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc56\ud835\udc5b\ud835\udc54_ _ _\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52_ = _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc56\ud835\udc5b\ud835\udc54_ _ _\ud835\udc54\ud835\udc4e\ud835\udc56\ud835\udc5b_ \u2217 _\ud835\udc5b\ud835\udc5c\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc59_ _ _\ud835\udc4f\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc64\ud835\udc56\ud835\udc51\ud835\udc61\u210e_ ;\n\n**44** BBR_Enter_Startup():\n\n**45** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc61\ud835\udc52_ = _\ud835\udc46\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61\ud835\udc62\ud835\udc5d_ ;\n\n**46** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc56\ud835\udc5b\ud835\udc54_ _ _\ud835\udc54\ud835\udc4e\ud835\udc56\ud835\udc5b_ = _\ud835\udc35\ud835\udc35\ud835\udc45\ud835\udc3b\ud835\udc56\ud835\udc54\u210e\ud835\udc3a\ud835\udc4e\ud835\udc56\ud835\udc5b_ ;\n\n**47** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ _ _\ud835\udc54\ud835\udc4e\ud835\udc56\ud835\udc5b_ = _\ud835\udc35\ud835\udc35\ud835\udc45\ud835\udc3b\ud835\udc56\ud835\udc54\u210e\ud835\udc3a\ud835\udc4e\ud835\udc56\ud835\udc5b_ ;\n\n**ACK-arrival function definitions (1/4):**\n\n**48** BBR_Update_BtlBw():\n\n**49** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc48\ud835\udc5d\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc52_ _ _\ud835\udc45\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc51_ () ;"}, {"role": "assistant", "content": "**Verdict:** This section details the operational framework of BBR, a high-impact congestion control algorithm. Its core utility lies in maximizing network throughput and minimizing latency by proactively discovering network path characteristics, offering significant improvements over traditional loss-based methods.\n\n**Summary:** The technical content outlines the functional flow of the BBR congestion control algorithm, contrasting it briefly with a reactive packet loss handling mechanism from wVegas. BBR is designed for modern networks, focusing on identifying and utilizing the true bottleneck bandwidth and minimum round-trip time rather than reacting solely to packet loss. This leads to higher throughput and lower queueing delays, making it particularly valuable for cloud services, high-bandwidth applications, and global content delivery where efficiency and responsiveness are critical. Implementation of BBR is software-based, integrated within the operating system's network stack (e.g., Linux kernel). It requires standard compute resources for its calculations, posing negligible additional hardware or operational costs. The value proposition is a significant enhancement in network performance and user experience without requiring specialized hardware, representing a strong return on integration effort for network-dependent services."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n**2.1** **Reasoning and Reinforcement Learning**\n\n**Reasoning LLMs** . Large Language Models (LLMs) have\nrecently shown remarkable performance in complex reasoning domains such as mathematics, logic, and programming.\n\n20K\n\n15K\n\n10K\n\n2\n\n![](papers/images/Taming-the-LongTail-Efficient-Reasoning-RL-Training-with-Adaptive-Drafter.pdf-2-2.png)\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n|Target Model<br><br><br>|Target Model<br><br><br>|Target Model<br><br><br>|Target Model<br><br><br>|Target Model<br><br><br>|Target Model<br><br><br>|Target Model<br><br><br>|\n|Target Model<br><br><br>|Target Model<br><br><br>|Target Model<br><br><br>|Target Model<br><br><br>|Target Model<br><br><br>|Target Model<br><br><br>|Target Model<br><br><br>|\n|Target Model<br><br><br>|||\u2460|\u2460|\u2460|\u2460|\n\n![](papers/images/Taming-the-LongTail-Efficient-Reasoning-RL-Training-with-Adaptive-Drafter.pdf-2-1.png)\n\n|Open<br>s1|AI-o1|\n|---|---|\n|||\n|||\n|||\n\n**Figure 3. Test-time scaling of reasoning models** . (a) Performance of OpenAI-o1 [40] and Stanford s1-32B [38] on the\nAIME Competition-level Math Benchmark [34]. (b) Example\nof self-reflection correcting an error within the reasoning.\n\n|Col1|Col2|Col3|Col4|\n|---|---|---|---|\n|||||\n|||||\n|||Vanil<br>Spec|la Dec.<br>. Dec.|\n\n|Col1|Col2|Col3|Col4|\n|---|---|---|---|\n|le training step i<br>   s: (1)**_ Rollout St_**|le training step i<br>   s: (1)**_ Rollout St_**|le training step i<br>   s: (1)**_ Rollout St_**|le training step i<br>   s: (1)**_ Rollout St_**|\n|ndidate response|ndidate response|ndidate response|ndidate response|\n\n|Col1|Pha Dse c1 o: dR ino gllout<br>e<br>ReM spo od ne sl<br>es<br>R1<br>\ud83d\udd25<br>Model R2<br>\u2026<br>Rn|P Rh ea ss pe n2 s: I n Pf re er fe iln linc ge<br>o e<br>\u2744<br>Ref Model<br>\ud83d\udd25<br>Model<br>R Pe ow lia cr yd|Col4|Col5|Ph Mas oe e3 : T pr da ain teing<br>d l U<br>KL<br>Diverge nce Update<br>\ud83d\udd25<br>FL uo nss Model<br>c|\n|---|---|---|---|---|---|\n||R1<br>R2<br>\u2026<br>Rn<br>Model<br>Responses<br>Model<br>\ud83d\udd25<br>**Phase 1: Rollout**<br>_Decoding_|||||\n|Prompt<br>(Question)|Prompt<br>(Question)|||||\n|Prompt<br>(Question)|Prompt<br>(Question)||Reward<br>Policy|Reward<br>Policy|Reward<br>Policy|\n|Prompt<br>(Question)|Prompt<br>(Question)|||||\n\n**Figure 4.** Overview of the GRPO [51] RL training process.\n\nPowerful models such as OpenAI-o1 [40] and DeepSeekR1 [9] achieve impressive accuracy on challenging reasoning\nbenchmarks, including competition-level math and coding\nchallenges. A key factor in their success is _test-time scaling_\n\n[32, 62], an inference paradigm that enhances accuracy by\naffording models more generation time to explore complex\nreasoning paths. These reasoning LLMs typically feature:\n(1) _Long Chain-of-Thought_, generating extended intermediate reasoning steps before reaching a final answer; (2) _Self-_\n_Reflection_, the capacity to evaluate, refine, and correct their\nown reasoning during inference to improve accuracy. As\nillustrated in Figure 3 (a), allowing extended generation time\nenables these models to explore deeper and wider thinking\ntraces [6], thereby enhancing accuracy without additional\ntraining costs. More importantly, unlike traditional LLMs,\nthese reasoning models can automatically revisit their generated responses and exhibit self-reflection capabilities (e.g.,\n\" _wait_ \" in Figure 3 (b) reasoning trace), which are typically\nacquired during their RL training stage [9] and are key to\nperformance enhancements.\n**Reinforcement Learning for Reasoning** . Recently, RL algorithm variants such as Group Relative Policy Optimization\n(GRPO) [51] have proven their success in training reasoning LLMs. Unlike standard Policy Optimization (PPO) [50],\nwhich relies on an extensive critic model, GRPO employs a\ngroup-based baseline estimation to streamline the optimization process, significantly reducing training overhead while\npreserving the robustness of policy updates [58].\n\n**Figure 5. Overview of Speculative Decoding.** (a & b)\nComparison between Vanilla and Speculative Decoding. (c)\nSpeculative decoding achieves peak compute throughput\n(TFLOPS) at significantly smaller batch sizes (gray arrow).\n\n![](papers/images/Taming-the-LongTail-Efficient-Reasoning-RL-Training-with-Adaptive-Drafter.pdf-2-3.png)\n\nFigure 4 illustrates a single training step in GRPO, which\ncomprises three main stages: (1) _**Rollout Stage**_, the target\nmodel generates multiple candidate responses for each input\nprompt; (2) _**Inference Stage**_, the generated responses are\nprocessed by both the _target model_ (the model undergoing\nRL training) and a fixed _reference model_ (the initial target\nmodel, i.e., RL step=0). This processing yields logits used\nto compute a KL divergence penalty [49], which serves to\nconstrain model updates, ensuring stability and preventing\nexcessive deviation from the reference model. Concurrently\nwithin this stage, a reward is calculated with a _rule-based_\npolicy regarding response characteristics (e.g., correctness),\nrather than relying on a separate value model; (3) _**Training**_\n_**Stage**_, a loss function is constructed using the reward value\nand KL divergence. The target model\u2019s weights are then\nupdated according to this loss.\n\n**2.2** **Speculative Decoding for Efficient Reasoning RL**\n\nAs mentioned in \u00a71, the long-tail rollout stage in reasoning\nRL frequently becomes a performance bottleneck. Consequently, tailored system optimizations for decoding acceleration are essential to improve throughput and resource efficiency. To this end, we identify speculative decoding [5, 22]\nas a key approach well-suited to tackle these challenges.\n**Why Speculative Decoding** . Among numerous methods accelerating LLM decoding, speculative decoding (SD) appears\nparticularly advantageous for the reasoning RL scenario. Its\nprimary benefit lies in its _lossless_ nature, mathematically ensuring the output distribution remains identical to that of\nthe original target model. This fidelity is paramount in reasoning tasks where the correctness and logical coherence of\ngenerated steps are critical for effective reinforcement learning. It directly tackles the throughput bottleneck identified\nin the long-tail rollout stage by speeding up generation, enhancing sample efficiency without compromising the crucial\naccuracy needed for reliable reasoning RL training.\n\n3\n\n![](papers/images/Taming-the-LongTail-Efficient-Reasoning-RL-Training-with-Adaptive-Drafter.pdf-3-0.png)\n\n|Col1|Col2|\n|---|---|\n|||\n|||\n|||\n||Spot<br>Drafter<br>Training|\n|||\n|||\n|||\n\n**Figure 6.** Overview of TLT architecture and workflow.\n\n**Mechanism of Speculative Decoding** . In speculative decoding (Figure 5 (b)), a lightweight draft model rapidly generates a sequence of candidate tokens. These candidates are\nthen efficiently verified in parallel by the much larger target model in a single forward pass. The system accepts the\ndrafted sequence up to the first token that mismatches the\ntarget model\u2019s prediction, also keeping the single correct\ntoken that the target model generated at that position. As\nshown in the roofline analysis in Figure 5 (c), this process improves throughput by shifting the typically memory-bound\nautoregressive generation towards a more compute-bound\noperation, more effectively saturating the GPU resources.\n**Challenges in Reasoning RL** . Despite its effectiveness in\nalleviating the memory-bound issue and enhancing decoding\nthroughput, adapting existing speculative decoding methods\nto the context of RL training is fundamentally non-trivial.\nWe identify three key challenges:\n**C1. Evolving Target Model** . Unlike standard inference\nscenarios where models are fixed, RL training involves continuous target model updates. This constant evolution renders the draft model used in SD progressively \"stale\", diminishing the acceptance rate of speculated tokens and significantly undermining SD\u2019s effectiveness.\n**C2. Draft Model Training Costs** . Achieving high SD\nefficiency often requires dedicated draft models (e.g., singledecoder-layer models like Eagle [26], HASS [67]), rather than\narbitrary smaller models. Training a specialized drafter for\nthe target model introduces substantial additional cost.\n**C3. Fluctuating Batch Sizes** . As shown in Figure 5 (c),\nSD is typically optimized for small batch sizes. In contrast,\nRL rollouts often involve dynamically varying generation\nbatch sizes. Applying SD efficiently in this scenario is challenging, as performance can degrade with larger batches if\nnot managed properly, impacting overall throughput.\n\nTo address these challenges, TLT leverages the unique\ncharacteristics of the reasoning RL training process. Specifically, we introduce the Adaptive Drafter (\u00a74) to mitigate\nperformance degradation from dynamic model weights ( **C1** ),\nutilizing spare GPU resources for efficient draft model updates and thereby removing training overhead ( **C2** ). Furthermore, we design a specialized scheduling strategy (\u00a75)\nwithin the reasoning RL system to effectively manage the\nchallenges posed by varying batch sizes ( **C3** )."}, {"role": "assistant", "content": "**Verdict:** High utility and value. This section effectively identifies a critical, high-cost bottleneck in the training of advanced reasoning LLMs and introduces Speculative Decoding (SD) as a promising, lossless acceleration technique. It then shrewdly outlines the practical challenges of applying SD in an RL training context, laying the groundwork for the paper's novel solutions.\n\n**Summary:** Current reasoning LLMs achieve impressive accuracy on complex tasks by using extended 'chain-of-thought' and 'self-reflection' during inference, which increases computational cost at test-time. Reinforcement Learning (RL) methods like GRPO facilitate their training by reducing overhead compared to traditional PPO (e.g., no extensive critic model). However, the 'rollout stage' of RL training, where the model generates multiple responses, is a significant performance bottleneck due to the sequential nature of LLM generation. Speculative Decoding (SD) is identified as a key system optimization to address this. SD offers substantial throughput improvements and enhanced sample efficiency by shifting computation from memory-bound autoregressive generation to more compute-bound parallel verification, critically *without compromising output fidelity*. However, applying SD to RL training faces three core challenges that directly impact its cost-effectiveness: (1) the target model continuously evolves, making a fixed draft model quickly 'stale' and inefficient; (2) dedicated draft models are expensive to train; and (3) fluctuating batch sizes in RL rollouts can degrade SD's performance, as SD is typically optimized for small batches. The paper proposes an 'Adaptive Drafter' and 'specialized scheduling' to mitigate these challenges, suggesting a path to unlock efficient and cost-effective RL training for powerful reasoning LLMs. This is highly valuable for reducing the substantial operational expenses associated with developing cutting-edge LLMs."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\nIn this paper, we introduce EGGROLL, a powerful method for blackbox optimisation that scales evolutionary\nstrategies to billion-parameter models and beyond using low-rank search matrices. Our experiments demonstrate that EGGROLL is effective with rank as small as _r_ = 1, which represents substantial computational and\nmemory savings for negligible decrease in performance when compared to the full-rank ES update. Empirically,\nEGGROLL delivers large speedups over na\u00efve ES in tabula rasa and multi-agent RL, and can power end-to-end\ntraining pipelines for large language models. Our theoretical analysis reveals that the low-rank EGGROLL\nupdate quickly converges with rank _r_, however further theoretical analysis is needed to explain the success of\nour method when _r_ = 1.\n\nLooking forward, we are working on applying EGGROLL for other problems beyond the reach of modern\ngradient-based techniques. In particular, EGGROLL can enable the training of large scale end-to-end neurosymbolic systems (Sarker et al., 2021) which have nondifferentiable components. For instance, we can\ntrain neural networks that directly interface with symbolic modules for specialized functions, like memory\nor calculations. We can also optimize end-to-end systems of language models, training them to be aware of\ninference-time harnesses and interactions with other agents in complex systems.\n\n**Acknowledgements**\n\nCompute for this project is graciously provided by the Isambard-AI National AI Research Resource, under\nthe projects \u201cFLAIR 2025 Moonshot Projects\u201d and \u201cRobustness via Self-Play RL.\u201d Some experiments also\n[used compute generously given by JASMIN, the UK\u2019s collaborative data analysis environment (https:](https://www.jasmin.ac.uk)\n[//www.jasmin.ac.uk).](https://www.jasmin.ac.uk)\n\n11\n\nBidipta Sarkar is supported by the Clarendon Fund Scholarship in partnership with a Department of Engineering\nScience Studentship for his Oxford DPhil. Mattie Fellows is funded by a generous grant from the UKRI\nEngineering and Physical Sciences Research Council EP/Y028481/1. Jakob Nicolaus Foerster is partially\nfunded by the UKRI grant EP/Y028481/1 (originally selected for funding by the ERC). Jakob Nicolaus Foerster\nis also supported by the JPMC Research Award and the Amazon Research Award. Juan Agustin Duque is\nsupported by the St-Pierre-Larochelle Scholarship at the University of Montreal and by Aaron Courville\u2019s\nCIFAR AI Chair in Representations that Generalize Systematically. Lukas Seier is supported by the Intelligent\nEarth CDT with funding from the UKRI grant number EP/Y030907/1.\n\n**References**\n\nShun-ichi Amari. Natural gradient works efficiently in learning. _Neural Computation_, 10(2):251\u2013276, 1998.\ndoi: 10.1162/089976698300017746.\n\nA. B. Basset. _A Treatise on Hydrodynamics: with numerous examples_, volume 2. Deighton, Bell, and Co.,\nCambridge, UK, 1892.\n\nYoshua Bengio, R\u00e9jean Ducharme, and Pascal Vincent. A neural probabilistic language model. In T. Leen,\nT. Dietterich, and V. Tresp (eds.), _Advances in Neural Information Processing Systems_, volume 13.\n[MIT Press, 2000. URL https://proceedings.neurips.cc/paper_files/paper/2000/](https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf)\n[file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf)\n\nHans-Georg Beyer. Toward a theory of evolution strategies: Self-adaptation. _Evolutionary Computation_, 3:\n[311\u2013347, 1995. URL https://api.semanticscholar.org/CorpusID:17416734.](https://api.semanticscholar.org/CorpusID:17416734)\n\nHans-Georg Beyer and Hans-Paul Schwefel. Evolution strategies \u2013a comprehensive introduction. _Natural_\n_Computing_, 1(1):3\u201352, 2002.\n\nR. N. Bhattacharya and R. Ranga Rao. _Normal approximation and asymptotic expansions_ . Wiley series in\nprobability and mathematical statistics. Wiley, New York, 1976. ISBN 047107201X.\n\nCl\u00e9ment Bonnet, Daniel Luo, Donal Byrne, Shikha Surana, Sasha Abramowitz, Paul Duckworth, Vincent\nCoyette, Laurence I. Midgley, Elshadai Tegegn, Tristan Kalloniatis, Omayma Mahjoub, Matthew Macfarlane,\nAndries P. Smit, Nathan Grinsztajn, Raphael Boige, Cemlyn N. Waters, Mohamed A. Mimouni, Ulrich\nA. Mbou Sob, Ruan de Kock, Siddarth Singh, Daniel Furelos-Blanco, Victor Le, Arnu Pretorius, and\nAlexandre Laterre. Jumanji: a diverse suite of scalable reinforcement learning environments in jax, 2024.\n[URL https://arxiv.org/abs/2306.09884.](https://arxiv.org/abs/2306.09884)\n\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George\nNecula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable\n[transformations of Python+NumPy programs, 2018. URL http://github.com/jax-ml/jax.](http://github.com/jax-ml/jax)\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\n[are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165.](https://arxiv.org/abs/2005.14165)\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, ..., Barret Zoph, Liam Fedus, and ...\n(full list). Palm: Scaling language modeling with pathways. _J. Mach. Learn. Res._, 24(1144), 2023. URL\n[https://jmlr.org/papers/volume24/22-1144/22-1144.pdf.](https://jmlr.org/papers/volume24/22-1144/22-1144.pdf)\n\nA. P. Dawid. Some matrix-variate distribution theory: Notational considerations and a bayesian application.\n_Biometrika_, 68(1):265\u2013274, 1981. ISSN 0006-3444.\n\n12\n\nNan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\nYanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P. Bosma, Zongwei Zhou,\nTao Wang, Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju\nDuke, Lucas Dixon, Kun Zhang, Quoc Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. Glam: Efficient\nscaling of language models with mixture-of-experts. In _Proceedings of the 39th International Conference_\n_on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pp. 5547\u20135569, Jul 2022.\n[URL https://proceedings.mlr.press/v162/du22c.html.](https://proceedings.mlr.press/v162/du22c.html)\n\nRichard Durrett. _Probability: Theory and Examples_ . Cambridge University Press, Cambridge, UK, 4th edition,\n2010.\n\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: scaling to trillion parameter models\nwith simple and efficient sparsity. _J. Mach. Learn. Res._, 23(1):1\u201339, January 2022. ISSN 1532-4435. URL\n[https://jmlr.org/papers/volume23/21-0998/21-0998.pdf.](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf)\n\n[Jakob Nicolaus Foerster. Nonlinear computation in deep linear networks, sep 2017. URL https://blog.](https://blog.openai.com/nonlinear-computation-in-linear-networks/)\n[openai.com/nonlinear-computation-in-linear-networks/. Accessed: 2025-11-20.](https://blog.openai.com/nonlinear-computation-in-linear-networks/)\n\nGerald B. Folland. _Real Analysis: Modern Techniques and Their Applications_ . John Wiley & Sons, New York,\n2nd edition, 1999. See Theorem 8.22 (Riemann\u2013Lebesgue Lemma).\n\nC. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem. Brax \u2013 a\n[differentiable physics engine for large scale rigid body simulation, 2021. URL https://arxiv.org/](https://arxiv.org/abs/2106.13281)\n[abs/2106.13281.](https://arxiv.org/abs/2106.13281)\n\nAlexander D. Goldie, Chris Lu, Matthew T. Jackson, Shimon Whiteson, and Jakob N. Foerster. Can Learned\nOptimization Make Reinforcement Learning Less Difficult? In _Advances in Neural Information Processing_\n_Systems_, volume 37, pp. 5454\u20135497, 2024.\n\nAlexander David Goldie, Zilin Wang, Jaron Cohen, Jakob Nicolaus Foerster, and Shimon Whiteson. How\n[Should We Meta-Learn Reinforcement Learning Algorithms? May 2025. URL https://openreview.](https://openreview.net/forum?id=jKzQ6af2DU)\n[net/forum?id=jKzQ6af2DU.](https://openreview.net/forum?id=jKzQ6af2DU)\n\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep Learning_ [. MIT Press, 2016. http://www.](http://www.deeplearningbook.org)\n[deeplearningbook.org.](http://www.deeplearningbook.org)\n\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes,\nN. Lawrence, and K.Q. Weinberger (eds.), _Advances in Neural Information Processing Systems_, volume 27. Curran Associates, Inc., 2014. [URL https://proceedings.neurips.cc/paper_](https://proceedings.neurips.cc/paper_files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Paper.pdf)\n[files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Paper.pdf)\n\nI. S. (Izrail Solomonovich) Gradshte\u02d8\u0131n, I. M. (Iosif Moiseevich) Ryzhik, Daniel Zwillinger, Victor Moll, and\nInc Scripta Technica. _Table of integrals, series, and products_ . Academic Press, San Diego ; Tokyo, 8 edition,\n2015. ISBN 0123849330.\n\nG R Grimmett and D R Stirzaker. Probability and random processes. _Journal of the Royal Statistical Society._\n_Series A, Statistics in society_, 156(3):503\u2013503, 1993. ISSN 0964-1998.\n\nPeter Hall. _The bootstrap and Edgeworth expansion_ . Springer series in statistics. Springer-Verlag, New York,\n1992. ISBN 9780387945088.\n\n[Nikolaus Hansen. The cma evolution strategy: A tutorial, 2023. URL https://arxiv.org/abs/1604.](https://arxiv.org/abs/1604.00772)\n\n[00772.](https://arxiv.org/abs/1604.00772)\n\nNikolaus Hansen and Andreas Ostermeier. Completely Derandomized Self-Adaptation in Evolution Strategies.\n_Evolutionary Computation_, 9(2):159\u2013195, June 2001. ISSN 1063-6560. doi: 10.1162/106365601750190398.\n[URL https://ieeexplore.ieee.org/document/6790628.](https://ieeexplore.ieee.org/document/6790628)\n\n13\n\nJoel Heck and Fathi M. Salem. Simplified minimal gated unit variations for recurrent neural networks, 2017.\n[URL https://arxiv.org/abs/1701.03452.](https://arxiv.org/abs/1701.03452)\n\nSepp Hochreiter and J\u00fcrgen Schmidhuber. Lstm can solve hard long time lag problems. In M.C. Mozer,\nM. Jordan, and T. Petsche (eds.), _Advances in Neural Information Processing Systems_, volume 9.\n[MIT Press, 1996. URL https://proceedings.neurips.cc/paper_files/paper/1996/](https://proceedings.neurips.cc/paper_files/paper/1996/file/a4d2f0d23dcc84ce983ff9157f8b7f88-Paper.pdf)\n[file/a4d2f0d23dcc84ce983ff9157f8b7f88-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/1996/file/a4d2f0d23dcc84ce983ff9157f8b7f88-Paper.pdf)\n\nMark Horowitz. 1.1 computing\u2019s energy problem (and what we can do about it). In _2014 IEEE International_\n_Solid-State Circuits Conference Digest of Technical Papers (ISSCC)_, pp. 10\u201314, 2014. doi: 10.1109/ISSCC.\n2014.6757323.\n\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. In _ICLR_ . OpenReview.net, 2022.\n\nMax Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi,\nOriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, and Koray Kavukcuoglu.\n[Population Based Training of Neural Networks, November 2017. URL http://arxiv.org/abs/](http://arxiv.org/abs/1711.09846)\n[1711.09846. arXiv:1711.09846 [cs].](http://arxiv.org/abs/1711.09846)\n\nFeihu Jin, Yifan Liu, and Ying Tan. Derivative-free optimization for low-rank adaptation in large language\nmodels. _IEEE/ACM Trans. Audio, Speech and Lang. Proc._, 32:4607\u20134616, October 2024. ISSN 2329-9290.\n[doi: 10.1109/TASLP.2024.3477330. URL https://doi.org/10.1109/TASLP.2024.3477330.](https://doi.org/10.1109/TASLP.2024.3477330)\n\nJean Kaddour. The minipile challenge for data-efficient language models. _arXiv preprint arXiv:2304.08442_,\n2023.\n\nSham M Kakade. A natural policy gradient. In T. Dietterich, S. Becker, and Z. Ghahramani (eds.), _Advances in Neural Information Processing Systems_, volume 14. MIT Press,\n2001. [URL https://proceedings.neurips.cc/paper_files/paper/2001/file/](https://proceedings.neurips.cc/paper_files/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf)\n[4b86abe48d358ecf194c56c69108433e-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf)\n\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun\n(eds.), _2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April_\n_14-16, 2014, Conference Track Proceedings_ [, 2014. URL http://arxiv.org/abs/1312.6114.](http://arxiv.org/abs/1312.6114)\n\nDaria Korotyshova, Boris Shaposhnikov, Alexey Malakhov, Alexey Khokhulin, Nikita Surnachev, Kirill\nOvcharenko, George Bredis, Alexey Gorbatovski, Viacheslav Sinii, and Daniil Gavrilov. Essa: Evolutionary\n[strategies for scalable alignment, 2025. URL https://arxiv.org/abs/2507.04453.](https://arxiv.org/abs/2507.04453)\n\nJohn R. Koza. Genetic programming as a means for programming computers by natural selection. _Statistics_\n_and Computing_ [, 4(2):87\u2013112, June 1994. ISSN 1573-1375. doi: 10.1007/BF00175355. URL https:](https://doi.org/10.1007/BF00175355)\n[//doi.org/10.1007/BF00175355.](https://doi.org/10.1007/BF00175355)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep\nconvolutional neural networks. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger\n(eds.), _Advances in Neural Information Processing Systems_, volume 25. Curran Associates,\n[Inc., 2012. URL https://proceedings.neurips.cc/paper_files/paper/2012/file/](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n[c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with\npagedattention. In _Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles_,\n2023.\n\nRobert Tjarko Lange, Tom Schaul, Yutian Chen, Tom Zahavy, Valentin Dallibard, Chris Lu, Satinder Singh,\nand Sebastian Flennerhag. Discovering Evolution Strategies via Meta-Black-Box Optimization, March\n[2023. URL http://arxiv.org/abs/2211.11260. arXiv:2211.11260 [cs].](http://arxiv.org/abs/2211.11260)\n\n14\n\nPierre-Simon Laplace. M\u00e9moire sur les int\u00e9grales d\u00e9finies et leur application aux probabilit\u00e9s, et sp\u00e9cialement\n\u00e0 la recherche du milieu qu\u2019il faut choisir entre les r\u00e9sultats des observations. _M\u00e9moires de la Classe des_\n_Sciences Math\u00e9matiques et Physiques de l\u2019Institut Imp\u00e9rial de France,_ 1 [re] s\u00e9rie, 11(1 [re] partie):297\u2013347,\n1811.\n\nV. P. Leonov and A. N. Shiryaev. On a method of calculation of semi-invariants. _Theory of Probability and Its_\n_Applications_ [, 4(3):342\u2013355, 1959. URL https://www.mathnet.ru/eng/tvp4894.](https://www.mathnet.ru/eng/tvp4894)\n\nOscar Li, James Harrison, Jascha Sohl-Dickstein, Virginia Smith, and Luke Metz. Variance-reduced gradient\nestimation via noise-reuse in online evolution strategies. In _Thirty-seventh Conference on Neural Information_\n_Processing Systems_, 2023a.\n\nOscar Li, James Harrison, Jascha Sohl-Dickstein, Virginia Smith, and Luke Metz. Noise-Reuse in Online\n[Evolution Strategies, April 2023b. URL http://arxiv.org/abs/2304.12180. arXiv:2304.12180](http://arxiv.org/abs/2304.12180)\n\n[cs].\n\n[Jarek Liesen, Chris Lu, and Robert Lange. rejax, 2024. URL https://github.com/keraJLi/rejax.](https://github.com/keraJLi/rejax)\n\nRyan Lowe, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for\nmixed cooperative-competitive environments. _Advances in neural information processing systems_, 30, 2017.\n\nChris Lu, Jakub Kuba, Alistair Letcher, Luke Metz, Christian Schroeder de Witt, and Jakob Foerster. Discovered policy optimisation. _Advances in Neural Information Processing Systems_, 35:16455\u201316468, 2022.\n\nH. M. Macdonald. Zeroes of the bessel functions. _Proceedings of the London Mathematical Society_, 30:\n165\u2013179, 1899. doi: 10.1112/plms/s1-30.1.165.\n\nSadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, and Sanjeev Arora.\nFine-tuning language models with just forward passes. In _Proceedings of the 37th International Conference_\n_on Neural Information Processing Systems_, NIPS \u201923, Red Hook, NY, USA, 2023. Curran Associates Inc.\n\nMichael Matthews, Michael Beukman, Benjamin Ellis, Mikayel Samvelyan, Matthew Jackson, Samuel\nCoward, and Jakob Foerster. Craftax: A lightning-fast benchmark for open-ended reinforcement learning.\n_arXiv preprint arXiv:2402.16801_, 2024.\n\nMichael T. Matthews, Michael Beukman, Chris Lu, and Jakob Nicolaus Foerster. Kinetix: Investigating\nthe training of general agents through open-ended physics-based control tasks. In _ICLR_, 2025. URL\n[https://openreview.net/forum?id=zCxGCdzreM.](https://openreview.net/forum?id=zCxGCdzreM)\n\n[P. McCullagh. Cumulants (lecture notes). Lecture notes, University of Chicago (pdf), 2013. URL https:](https://www.stat.uchicago.edu/~pmcc/courses/stat306/2013/cumulants.pdf)\n[//www.stat.uchicago.edu/~pmcc/courses/stat306/2013/cumulants.pdf.](https://www.stat.uchicago.edu/~pmcc/courses/stat306/2013/cumulants.pdf)\n\nPeter McCullagh. _Tensor Methods in Statistics_ . Monographs on Statistics and Applied Probability. Chapman\n[& Hall / CRC, London, 1987. URL https://www.stat.uchicago.edu/~pmcc/tensorbook/](https://www.stat.uchicago.edu/~pmcc/tensorbook/DoverEdition.pdf)\n[DoverEdition.pdf. Discusses multivariate moments and cumulants (cumulant tensors).](https://www.stat.uchicago.edu/~pmcc/tensorbook/DoverEdition.pdf)\n\nWilliam Merrill, Jackson Petty, and Ashish Sabharwal. The illusion of state in state-space models. In\nRuslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and\nFelix Berkenkamp (eds.), _Proceedings of the 41st International Conference on Machine Learning_, volume\n235 of _Proceedings of Machine Learning Research_, pp. 35492\u201335506. PMLR, 21\u201327 Jul 2024. URL\n[https://proceedings.mlr.press/v235/merrill24a.html.](https://proceedings.mlr.press/v235/merrill24a.html)\n\nLuke Metz, James Harrison, C. Daniel Freeman, Amil Merchant, Lucas Beyer, James Bradbury, Naman\nAgrawal, Ben Poole, Igor Mordatch, Adam Roberts, and Jascha Sohl-Dickstein. VeLO: Training Versatile\n[Learned Optimizers by Scaling Up, November 2022. URL http://arxiv.org/abs/2211.09760.](http://arxiv.org/abs/2211.09760)\narXiv:2211.09760 [cs, math, stat].\n\nJack Parker-Holder, Vu Nguyen, and Stephen Roberts. Provably Efficient Online Hyperparameter Opti[mization with Population-Based Bandits, June 2021. URL http://arxiv.org/abs/2002.02518.](http://arxiv.org/abs/2002.02518)\narXiv:2002.02518 [cs].\n\n15\n\nBo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Xingjian Du, Haowen Hou, Jiaju Lin, Jiaxing\nLiu, Janna Lu, William Merrill, Guangyu Song, Kaifeng Tan, Saiteja Utpala, Nathan Wilce, Johan S. Wind,\nTianyi Wu, Daniel Wuttke, and Christian Zhou-Zheng. Rwkv-7 \"goose\" with expressive dynamic state\n[evolution, 2025. URL https://arxiv.org/abs/2503.14456.](https://arxiv.org/abs/2503.14456)\n\n[K. B. Petersen and M. S. Pedersen. The matrix cookbook, October 2008. URL http://www2.imm.dtu.](http://www2.imm.dtu.dk/pubdb/p.php?3274)\n[dk/pubdb/p.php?3274. Version 20081110.](http://www2.imm.dtu.dk/pubdb/p.php?3274)\n\nEduardo Pignatelli, Jarek Liesen, Robert Tjarko Lange, Chris Lu, Pablo Samuel Castro, and Laura Toni. Navix:\n[Scaling minigrid environments with jax, 2024. URL https://arxiv.org/abs/2407.19396.](https://arxiv.org/abs/2407.19396)\n\nXin Qiu, Yulu Gan, Conor F. Hayes, Qiyao Liang, Elliot Meyerson, Babak Hodjat, and Risto Miikkulainen.\n[Evolution strategies at scale: Llm fine-tuning beyond reinforcement learning, 2025. URL https://](https://arxiv.org/abs/2509.24372)\n[arxiv.org/abs/2509.24372.](https://arxiv.org/abs/2509.24372)\n\nI. Rechenberg. Evolutionsstrategien. In Berthold Schneider and Ulrich Ranft (eds.), _Simulationsmethoden_\n_in der Medizin und Biologie_, pp. 83\u2013114, Berlin, Heidelberg, 1978. Springer Berlin Heidelberg. ISBN\n978-3-642-81283-5.\n\nV. K. Rohatgi. _An introduction to probability theory and mathematical statistics_ . Wiley series in probability\nand mathematical statistics. Wiley, New York, 1976. ISBN 0471731358.\n\nFrank. Rosenblatt. _Principles of neurodynamics : perceptrons and the theory of brain mechanisms._ Spartan\nBooks, Washington, 1962.\n\nAlexander Rutherford, Benjamin Ellis, Matteo Gallici, Jonathan Cook, Andrei Lupu, Gardar Ingvarsson,\nTimon Willi, Akbir Khan, Christian Schroeder de Witt, Alexandra Souly, et al. Jaxmarl: Multi-agent rl\nenvironments in jax. _arXiv preprint arXiv:2311.10090_, 2023.\n\nTim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable\n[alternative to reinforcement learning, 2017. URL https://arxiv.org/abs/1703.03864.](https://arxiv.org/abs/1703.03864)\n\nJohn K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw. Parallel random numbers: As easy as 1, 2,\n3. In _SC \u201911: Proceedings of 2011 International Conference for High Performance Computing, Networking,_\n_Storage and Analysis_, pp. 1\u201312, 2011. doi: 10.1145/2063384.2063405.\n\nMd Kamruzzaman Sarker, Lu Zhou, Aaron Eberhart, and Pascal Hitzler. Neuro-symbolic artificial intelligence:\n[Current trends, 2021. URL https://arxiv.org/abs/2105.05330.](https://arxiv.org/abs/2105.05330)\n\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang,\nY. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open\n[language models, 2024. URL https://arxiv.org/abs/2402.03300.](https://arxiv.org/abs/2402.03300)\n\nJasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learning\n[algorithms, 2012. URL https://arxiv.org/abs/1206.2944.](https://arxiv.org/abs/1206.2944)\n\nFelipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O. Stanley, and Jeff\nClune. Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural\n[Networks for Reinforcement Learning, April 2018. URL http://arxiv.org/abs/1712.06567.](http://arxiv.org/abs/1712.06567)\narXiv:1712.06567 [cs].\n\nLaurits Tani, Diana Rand, Christian Veelken, and Mario Kadastik. Evolutionary algorithms for hyperparameter\noptimization in machine learning for application in high energy physics. _The European Physical Journal C_,\n81(2):170, February 2021. ISSN 1434-6044, 1434-6052. doi: 10.1140/epjc/s10052-021-08950-y. URL\n[http://arxiv.org/abs/2011.04434. arXiv:2011.04434 [hep-ex].](http://arxiv.org/abs/2011.04434)\n\nNico M Temme. _Bessel Functions_, chapter 9, pp. 219\u2013255. John Wiley and Sons, Ltd, 1996. ISBN\n[9781118032572. doi: https://doi.org/10.1002/9781118032572.ch9. URL https://onlinelibrary.](https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118032572.ch9)\n[wiley.com/doi/abs/10.1002/9781118032572.ch9.](https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118032572.ch9)\n\n16\n\nSebastian Towers, Aleksandra Kalisz, Philippe A. Robert, Alicia Higueruelo, Francesca Vianello, MingHan Chloe Tsai, Harrison Steel, and Jakob N. Foerster. ADIOS: Antibody Development via Opponent\n[Shaping, June 2025. URL http://arxiv.org/abs/2409.10588. arXiv:2409.10588 [q-bio].](http://arxiv.org/abs/2409.10588)\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett (eds.), _Advances in Neural Information Processing Systems_,\n[volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n[files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n\nPaul Vicol, Luke Metz, and Jascha Sohl-Dickstein. Unbiased gradient estimation in unrolled computation\ngraphs with persistent evolution strategies. In Marina Meila and Tong Zhang (eds.), _Proceedings of the_\n_38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning_\n_Research_ [, pp. 10553\u201310563. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/](https://proceedings.mlr.press/v139/vicol21a.html)\n[v139/vicol21a.html.](https://proceedings.mlr.press/v139/vicol21a.html)\n\nPaul Vicol, Zico Kolter, and Kevin Swersky. Low-Variance Gradient Estimation in Unrolled Computation Graphs with ES-Single, April 2023. [URL http://arxiv.org/abs/2304.11153.](http://arxiv.org/abs/2304.11153)\narXiv:2304.11153 [cs, stat].\n\nAmala Mary Vincent and P. Jidesh. An improved hyperparameter optimization framework for AutoML\nsystems using evolutionary algorithms. _Scientific Reports_, 13(1):4737, March 2023. ISSN 2045-2322. doi:\n[10.1038/s41598-023-32027-3. URL https://doi.org/10.1038/s41598-023-32027-3.](https://doi.org/10.1038/s41598-023-32027-3)\n\nG. N. Watson. _A Treatise on the Theory of Bessel Functions_ . Cambridge University Press, Cambridge, 2\nedition, 1944. Reprinted with corrections, various later printings.\n\nG. B. Whitham. _Linear and nonlinear waves_ . Pure and applied mathematics. Wiley-Interscience, New York,\n1999. ISBN 9786613306241.\n\nDaan Wierstra, Tom Schaul, Jan Peters, and J\u00fcrgen Schmidhuber. Natural evolution strategies. pp. 3381\u20133387,\n06 2008. doi: 10.1109/CEC.2008.4631255.\n\nDaan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, and J\u00fcrgen Schmidhuber. Natural evolution\n[strategies, 2011. URL https://arxiv.org/abs/1106.4487.](https://arxiv.org/abs/1106.4487)\n\nC.S. Withers. A simple expression for the multivariate hermite polynomials. _Statistics and Probability Letters_,\n47(2):165\u2013169, 2000. ISSN 0167-7152. doi: https://doi.org/10.1016/S0167-7152(99)00153-4. URL\n[https://www.sciencedirect.com/science/article/pii/S0167715299001534.](https://www.sciencedirect.com/science/article/pii/S0167715299001534)\n\nKe Xue, Chao Qian, Ling Xu, and Xudong Fei. Evolutionary gradient descent for non-convex optimization.\nIn Zhi-Hua Zhou (ed.), _Proceedings of the Thirtieth International Joint Conference on Artificial Intelli-_\n_gence, IJCAI-21_, pp. 3221\u20133227. International Joint Conferences on Artificial Intelligence Organization,\n[8 2021. doi: 10.24963/ijcai.2021/443. URL https://doi.org/10.24963/ijcai.2021/443.](https://doi.org/10.24963/ijcai.2021/443)\nMain Track.\n\nZiming Yu, Pan Zhou, Sike Wang, Jia Li, Mi Tian, and Hua Huang. Zeroth-order fine-tuning of llms in random\nsubspaces. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pp.\n4475\u20134485, October 2025.\n\nYihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang, Wenqing Zheng, Pin-Yu Chen, Jason D.\nLee, Wotao Yin, Mingyi Hong, Zhangyang Wang, Sijia Liu, and Tianlong Chen. Revisiting zeroth-order\noptimization for memory-efficient llm fine-tuning: A benchmark, 2024.\n\n17\n\n**Appendix**\n\n**A Proofs** **19**\n\nA.1 ES Matrix Gradient Deviations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n\nA.2 Error Rate Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n\n**B** **Mean Field Score Function Approximator** **27**\n\nB.1 Bessel Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n\nB.2 Derivation of Mean-field Approximators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n\n**C EGG Architecture** **33**\n\nC.1 Notation and Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n\nC.2 Parameter Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n\nC.3 Matrix Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n\nC.4 Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n\nC.5 Layer Normalization (LN) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n\nC.6 MLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n\nC.7 GRU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n\nC.8 Fitness Calculation in Integer Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n\n**D EGG Pretraining with Integer EGGROLL** **35**\n\nD.1 Adding EGGROLL Perturbations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\nD.2 Fitness Shaping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\nD.3 Parameter Update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\n**E** **EGG Ablations** **36**\n\nE.1 Data Efficient Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\nE.2 Controlling For Data Across Population Sizes . . . . . . . . . . . . . . . . . . . . . . . . . 37\n\n**F** **EGGROLL Speed**"}, {"role": "assistant", "content": "**Verdict:** STRONGLY POSITIVE. This method offers critical cost and utility advantages for large-scale AI development.\n\n**Summary:** EGGROLL is a highly valuable development, enabling blackbox optimization for models previously considered too large (billion-parameters and beyond) for Evolutionary Strategies. Its core innovation is the use of low-rank search matrices, which dramatically reduces computational and memory overhead. This translates directly to substantial cost savings in compute resources and enables significantly faster training times on existing hardware. Critically, it unlocks the ability to train novel AI architectures, such as neuro-symbolic systems and end-to-end models with non-differentiable components, which are currently inaccessible to gradient-based methods. The demonstrated effectiveness with rank 'r=1' implies extreme efficiency. This technology provides a competitive edge in developing next-generation, complex AI systems."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n_\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ _< \ud835\udc4a\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc61_ __\ud835\udc5a\ud835\udc4e\ud835\udc65_ **then**\n\n~~[\u221a]~~ 3\n**20** _\ud835\udc3e_ \u2190\u2212\n\n_\ud835\udc4a\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc61_ _ _\ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc36_ \u2212 _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_\n\n**21** _\ud835\udc5c\ud835\udc5f\ud835\udc56\ud835\udc54\ud835\udc56\ud835\udc5b_ _ _\ud835\udc5d\ud835\udc5c\ud835\udc56\ud835\udc5b\ud835\udc61_ \u2190\u2212 _\ud835\udc4a\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc61_ _ _\ud835\udc5a\ud835\udc4e\ud835\udc65_"}, {"role": "assistant", "content": "**Verdict:** Low-level state-tracking logic; utility depends on broader system context.\n\n**Summary:** This code implements a basic conditional update, checking if a current state variable (`cwnd`) is less than a previously recorded maximum (`Wlast_max`). If true, it calculates a numerical gap (`K = Wlast_maxC - cwnd`) and stores the historical maximum (`origin_point = Wlast_max`). This is a foundational computational step, common in resource management or control algorithms. It introduces no specific hardware costs or requirements. Its utility is in enabling adaptive behavior, such as dynamic window scaling or congestion avoidance in network protocols, by providing specific values for adjustment and a reference point for historical context. However, standing alone, it offers no direct speed improvements or cost reductions. Its value is entirely derived from its integration into a larger, performance-critical algorithm."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n**1**\n\n1.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n\n1.2 Technical Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n\n1.3 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8"}, {"role": "assistant", "content": "**Verdict:** This table of contents provides critical foundational context, but offers no direct value, costs, or utility without the actual content.\n\n**Summary:** The 'Preliminaries' section is essential for technical onboarding, ensuring rapid comprehension by establishing shared definitions. The 'Technical Overview' provides an immediate, high-level understanding of the proposed system's architecture, allowing for quick assessment of its complexity and potential integration points. 'Related Work' is vital for competitive analysis, identifying market gaps and validating the necessity of the proposed solution. While these sections don't specify hardware or direct costs, they are fundamental for a rapid initial evaluation of the paper's scope and potential impact."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n**More Application Scenarios** . In this work, we primarily\ninvestigate how speculative decoding can accelerate RL training by mitigating long-tail rollouts. We believe this technique\nalso opens promising avenues for broader applications. For\nexample: (1) _Uniformly long responses_ . When all rollouts are\nlong and no tail exists, each request demands substantial\nKV-cache capacity. This often makes the system KV-cachebound and triggers request eviction, limiting the running\nbatch size. In such cases, the workload naturally falls into\nthe \u201csweet spot\" of speculative decoding, enabling decoding\nacceleration. (2) _Multi-turn rollouts with tool-calling RL_ . In\nmulti-turn RL settings involving tool calls, partial requests\nperform GPU-free tool executions while their KV caches remain resident on the GPU. This reduces the number of active\n\ndecoding requests and again creates a favorable regime for\nspeculative decoding under our approach. (3) _Online serving_\n_and edge deployment_ . When the target model is fixed, a draft\nmodel trained via TLT can be directly deployed for inference.\nAdaptive speculative decoding remains highly effective in\nthese deployment scenarios, especially for handling variable\n\n**Figure 17. Effect of selective asynchronous checkpoint-**\n**ing and sequence packing.**\n\nload. Extending our approach to these settings is an exciting\ndirection for future work.\n\n**Can We Break RL Synchronization** ? TLT aims to improve RL training efficiency without modifying on-policy\nrequirements of the underlying algorithm. One may wonder whether it is possible to further accelerate training by\nasynchronously updating the model with partial rollouts during long decoding phases. While appealing, this approach\nrisks altering the underlying RL algorithm and potentially\nharming convergence and model performance. Asynchronous RL typically mixes multiple policy versions within a\nsingle rollout, potentially biasing gradient estimation and\nshifting the learning dynamics (e.g., drifting from on-policy\nto off-policy). Additional algorithmic modifications are required to safely accommodate such paradigm [12]. A more\nbalanced approach is to allow limited asynchrony that preserves algorithmic correctness while improving hardware\nutilization. Even under such settings, the heavy-tail rollout\nproblem remains, and our speculative decoding approach\ncan be combined with limited asynchronous RL to further\naccelerate training _without_ introducing additional loss or\npolicy staleness, leaving this as an intriguing direction for\nfuture work.\n\n**Can TLT be applied to other RL algorithms** ? GRPO represents a widely used algorithmic structure for reasoning RL.\nAlternatives such as RLOO [1], DAPO [66], REINFORCE [61],\nand REINFORCE++ [18] generally share the same overall\ntraining workflow, differing mainly in their reward formulations and use of KL-regularization. These algorithmic variations remain fully compatible with our adaptive drafter\nand spot-training design, suggesting that TLT can readily\naccelerate a broad class of RL methods."}, {"role": "assistant", "content": "**Verdict:** This section significantly expands the value proposition of TLT by demonstrating its applicability to a broader range of high-value RL training and deployment scenarios, enhancing efficiency and resource utilization in previously challenging contexts.\n\n**Summary:** This analysis highlights several key extensions for the TLT framework. It identifies TLT's utility in three critical areas: (1) accelerating systems with uniformly long responses, where KV-cache capacity is often a bottleneck, by enabling decoding acceleration. (2) Improving efficiency in multi-turn RL with tool-calling, where resident KV-caches during GPU-free operations can reduce active decoding requests. (3) Enhancing online serving and edge deployment by supporting adaptive speculative decoding for variable loads with fixed target models. Furthermore, it explicitly states TLT's compatibility with a wide array of existing RL algorithms beyond GRPO, indicating broad applicability. The main benefit is improved utilization of existing hardware (e.g., KV-cache, GPU compute), effectively reducing the cost per effective training step. No new specific hardware requirements are introduced. The discussion on breaking RL synchronization underscores a critical risk: improperly implemented asynchronous updates can harm convergence and model performance, introducing potential hidden costs in validation and re-training. TLT's approach avoids these risks by preserving algorithmic correctness while still offering acceleration."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n`UndecidedNeighbors` _is empty_ **then**\n\n**9** From this point onward, disregard the rest of the algorithm and Output 1 in\nevery round where node _i_ is still Undecided\n\n18\n\nWe prove Theorem 4.1 in three parts (Theorems 4.4, 4.5, and 4.6). Among these, Theorem 4.4\nrequires intermediate results, which we establish through Lemmas 4.2 and 4.3.\n\n**Lemma 4.2.** _Let the game be at an arbitrary history h with i \u2208_ _P_ ( _h_ ) _, where h \u2208_ _Ii \u2208Ii._\n_Suppose that one of i\u2019s_ _`UndecidedNeighbors`_ _(say, j) has its_ _`BeenCheated`_ _flag set to_ _`true`_ _._\n_Then, regardless of the history h and of i\u2019s strategy from h onward, if all the other nodes follow_\n_Algorithm 3 from h onward, the utility of node i will be at most_ 0 _._\n\n_Proof._ According to Algorithm 3, node _j_ outputs 0 only in Round 5 of an iteration, and even\nthen, only if its `BeenCheated` flag is `false` . Since the `BeenCheated` flag, once set to `true`, is\nnever reset to `false`, a node with `BeenCheated` = `true` will never output 0. Therefore, node _j_\nwill only output 1 or _\u22a5_ . In either case, the utility of _i_ is 0.\n\n**Lemma 4.3.** _Let the game be at an arbitrary history h with i \u2208_ _P_ ( _h_ ) _, where h \u2208_ _Ii \u2208Ii, and let_\n_hi be the local history of node i. Then, provided that all the undecided nodes follow Algorithm 3_\n_from this point onward, the utility of node i is at least_ 0 _._\n\n_Proof._ A node _i_ receives a utility less than zero (i.e., _\u2212\u221e_ ) only in two cases:\n\n1. _i_ outputs 0, and none of its neighbors also outputs 1, or\n\n2. _i_ outputs 1, and at least one of its neighbors also outputs 1.\n\n**Case 1** is not possible under the algorithm: a node is instructed to output 0 only in Round\n4, and only after observing at least one neighbor output 1. Hence, with any history where the\nnode _i_ is undecided in Round 3 outputs 0, it must be the case that a neighbor has already output\n1. Therefore, the first case cannot occur.\n**Case 2** is also prevented by the algorithm: a node outputs 1 only in Round 3 and only if:\n\n  - No neighbor has output 1 or _\u22a5_ in the previous rounds, and\n\n  - _i_ has the least rank compared to all its undecided Neighbors\n\nBecause all Undecided neighbors follow the algorithm, the nodes with rank less than _i_ will not\noutput 1 \u2013 they will either output 0 or _\u22a5_ in later Rounds.\nTherefore, following the algorithm ensures that node _i_ \u2019s utility is never less than 0.\n\n**Theorem 4.4.** _Algorithm 3 constitutes a Belief-Independent Sequential Equilibrium._\n\n_Proof._ Consider an arbitrary history _h_ with _i \u2208_ _P_ ( _h_ ). Let _hi_ denote the local history of node\n_i_, which implies _h \u2208_ _Ii_ ( _hi_ ) _\u2208Ii_ . We show that the continuation strategy prescribed for _i_ by\nAlgorithm 3 from information set _Ii_ ( _hi_ ) is sequentially rational irrespective of the _i_ \u2019s belief over\n_Ii_ ( _hi_ ), i.e., it gives the maximum expected payoff when all other nodes also follow Algorithm 3\nfrom _h_ . It follows that the strategy constitutes a sequential equilibrium for any belief system of\n_i_ . and it follows that the strategy constitutes a Belief-Independent Sequential Equilibrium. As\nwe mentioned in Section 2, we do not consider internal deviations; this indirectly means that\nnode _i_ knows exactly which line/round of the algorithm applies at _hi_ . We distinguish deviations\n\nas\n\n  - _Undetectable deviations:_ for example, choosing internal randomness _ri\u2192i, ri\u2192j_ with nonuniform probabilities. Since every sequence of ranks is still possible under the prescribed\nuniform strategy (albeit with small probability), neighbors can never rule out a deviation\nwith certainty, even after many observations. Similarly, voluntarily outputting 0 or _\u22a5_\nearlier than prescribed may also be indistinguishable, since it could be a valid response to\nanother neighbor having output 1, which remaining neighbors may not observe.\n\n19\n\n  - _Detectable deviations:_ e.g., failing to send a required/valid message, sending inconsistent\nmessages, or outputting 1 when not eligible. Such deviations are caught by at least one\nneighbor, which sets its `BeenCheated` flag to 1.\n\n**Case analysis.**\n\n_Case A: At h, i has a decided neighbor with output_ 1 _or \u22a5._ Then the maximum attainable\npayoff for _i_ is 0. By Lemma 4.3, following the algorithm yields a payoff of at least 0. Hence, no\nprofitable deviation exists, so the prescribed action is a best response.\n\n_Case B: At h, i do not have a decided neighbor with output_ 1 _or \u22a5_ _and there exists an_\n_undecided neighbor that i has cheated._ If the remaining undecided neighbors follow the algorithm,\nLemma 4.2 implies the most _i_ can obtain is 0; following the algorithm also yields payoff 0. Thus,\nno profitable deviation exists.\n\n_Case C: At h, i do not have a decided neighbor with output_ 1 _or \u22a5_ _and i has not cheated any_\n_undecided neighbor._\n\n  - _Detectable deviation by i:_ Some neighbor detects it and sets `BeenCheated` = 1. By\nLemma 4.2, the best payoff _i_ can then secure is 0. By Lemma 4.3, sticking to the algorithm yields payoff at most 0 and at least 0, so no profitable deviation exists.\n\n  - _Undetectable deviation by i:_ (1) Changing RPS mixing probabilities does not improve _i_ \u2019s\nwin probability against neighbors who randomize uniformly; _i_ \u2019s chance of joining the MIS\nis unchanged, so expected payoff is unchanged. (2) Voluntarily outputting 0 or _\u22a5_ can\nyield at most 0, whereas Lemma 4.3 guarantees sticking yields payoff at least 0. Hence,\nno profitable deviation exists.\n\nTherefore, we have shown that for every history _h_ with _i \u2208_ _P_ ( _h_ ), the prescribed strategy for\n_i_ based on _hi_ = proj _i_ ( _h_ ) gives maximum expected utility possible from _h_ when all other node\nare following the algorithm from _h_, which proves the theorem.\n\nWe have shown that Algorithm 3 forms a Belief-Independent Sequential Equilibrium. To\ncomplete the proof of Theorem 3.2, we now assume that all nodes follow the algorithm without\ndeviation. Under this honest execution, we analyze the simplified version of the algorithm (with\ndeviation-handling logic removed) and proceed to prove its correctness, termination, and nonzero\ninclusion probability.\n\n**4.2** **Strategy Algorithm without Deviations**\n\nSince we have established in Theorem 4.4 that Strategy Algorithm 3 constitutes a BeliefIndependent Sequential Equilibrium, no node has a unilateral incentive to deviate. When no\nnode deviates from the Strategy Algorithm 3, its execution is equivalent to running Algorithm 9,\nwhich is simply Strategy Algorithm 3 with the deviation handling logic removed.\nWe now proceed to prove the remaining claims of Theorem 4.1 on this deviation-free execution: namely, correctness (that the output forms a valid MIS), termination (with high probability), and positive inclusion probability (every node has a nonzero chance of joining the MIS).\n\n**Theorem 4.5** (Correctness and Termination) **.** _Algorithm 9 terminates in O_ (log _n_ ) _rounds with_\n_high probability, and the set of nodes that output_ 1 _at termination forms a valid Maximal Inde-_\n_pendent Set (MIS)._\n\n_Proof._ Algorithm 9 simulates the algorithm by M\u00b4etivier et al. [15] through verifiable random\npriority generation and decision-making rounds.\nEach iteration of Algorithm 9 consists of five communication rounds. This five-round process\nmirrors one round of the algorithm by M\u00b4etivier et al. [15]. Since that algorithm terminates in\n_O_ (log _n_ ) rounds with high probability and outputs a correct MIS, our algorithm also inherits\n\n20\n\n**Algorithm 9:** Rank based MIS strategy algorithm for node _i_ under no deviations\n\n**1 Initialization**\n\n**2** `UndecidedNeighbors` _\u2190_ _N_ ( _i_ ); `iteration` ~~`n`~~ `umber` _\u2190_ 0\n\n**3 if** _N_ ( _i_ ) = _\u2205_ **then**\n\n**4** Output 1\n\n**5 while** _i is_ Undecided **do**\n\n**6** `iteration` ~~`n`~~ `umber` _\u2190_ `iteration` `number` + 1;\n```\n   /* Round 1: Opponent Selection */\n```\n\n**7** Choose any _k \u2208_ `UndecidedNeighbors` as the opponent opp( _i_ ), and broadcast this\nchoice to all neighbors.\n```\n   /* Round 2: Priority Computation */\n```\n\n**8** Draw random _c_ log _n_ bit _ri\u2192i \u2208{_ 0 _,_ 1 _}_ _[c]_ [ log] _[ n]_ uniformly at random and broadcast to all\nthe neighbors;"}, {"role": "assistant", "content": "**Verdict:** CRITICAL ENABLER\n\n**Summary:** This section is crucial as it proves the game-theoretic stability of the distributed algorithm, guaranteeing that nodes will not deviate for individual gain. This ensures reliability in potentially adversarial or self-interested network environments. Its primary utility is establishing a robust and trustworthy foundation for distributed MIS formation. By proving that no node has an incentive to deviate, the system can then rely on a simplified, faster execution (Algorithm 9) which inherits provable correctness and efficient O(log n) termination from an existing algorithm. This translates to faster and more reliable distributed MIS generation. Costs include communication overhead due to multiple rounds of message passing (broadcasting random ranks and choices) and minimal computational cost for generating sufficiently complex random numbers. No specific exotic hardware requirements are indicated beyond standard network capabilities."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n_\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc61\ud835\udc52_ == _\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc52\ud835\udc45\ud835\udc47\ud835\udc47_ **then**\n\n**170** _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ = _\ud835\udc5a\ud835\udc56\ud835\udc5b_ ( _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51, \ud835\udc35\ud835\udc35\ud835\udc45\ud835\udc40\ud835\udc56\ud835\udc5b\ud835\udc43\ud835\udc56\ud835\udc5d\ud835\udc52\ud835\udc36\ud835\udc64\ud835\udc5b\ud835\udc51_ ) ;\n\n**Packet-transmission function definitions:**\n\n**171** BBR_Handle_Restart_From_Idle():"}, {"role": "assistant", "content": "**Verdict:** Critical software logic for high-performance, low-latency network data transfer.\n\n**Summary:** This code block details core BBR congestion control logic, specifically managing the network's data sending rate ('cwnd') during its crucial RTT probing phase ('ProbeRTT') and handling connection restarts after inactivity. Its utility lies in enabling BBR to accurately discover and adapt to the network's true capacity and minimum round-trip time, thereby preventing bufferbloat and improving both throughput and latency. This translates to better user experience in data-intensive applications. Costs are negligible, as it's purely a software implementation within the network stack, requiring no specialized hardware beyond standard compute and networking components. It's a fundamental element for the algorithm's effectiveness."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\nWe now analyse how fast the Gaussian score approximation from Eq. (7) converges to the true Gaussian\nES matrix gradient in Eq. (4). We introduce the following formal regularity assumption for the fitness\nfunction:\n**Assumption 2.** _Assume that f_ ( _M_ ) _is bounded, that is_ sup _M |f_ ( _M_ ) _| < \u221e._\n\nDenote the true full-rank Gaussian ES gradient as _g_ True := _\u2207\u00b5J_ ( _\u00b5_ ). Our key theoretical result characterises\nthe error rate between the Gaussian score approximator in the low-rank update \u02c6 _g_ LR _[r]_ [from Eq. (][5][) and the true]\ngradient using the matrix Frobenius norm:\n**Theorem 2.** _Let Assumptions 1 and 2 hold and set \u03c3_ 0 = 1 _, then:_\n\n\u02c6 1\n_\u2225g_ LR _[r]_ _[\u2212]_ _[g]_ [True] _[\u2225][F]_ [=] _[ O]_\n\ufffd _r_\n\n_._ (9)\n\ufffd\n\n_r_\n\nThe convergence rate in Eq. (9) is faster than the\ntypical _O_ ( [1] _/_ ~~_[\u221a]_~~ _r_ ) rate dictated by the general parametric central limit theorem. Our analysis shows that\nthis is due to the symmetry in our problem under\nAssumption 1. To obtain our results, we make an\nEdgeworth expansion (Bhattacharya & Ranga Rao,\n1976) of the density _p_ ( _Z_ _[r]_ ), which expands _p_ ( _Z_ _[r]_ )\nas the limiting Gaussian distribution plus a sum of\ndecaying terms that are controlled by the 3rd order\nand higher cumulants of _p_ ( _Z_ _[r]_ ). Each _i_ th order cumulant term is multiplied by a factor that decays at\nrate _O_ _r_ _[\u2212]_ _[i][\u2212]_ 2 [2] . For symmetric zero-mean distribu\ufffd \ufffd\n\n|r=1|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|~~r=2~~<br>r=3<br>r=5<br>r=10||||||\n|r=50<br>r=100<br>r||||||\n|||||||\n|||||||\n|||||||\n\nFigure 3: Plot of Marginal Score Multiplied by Density for\nIncreasing _r_\n\nrate _O_ _r_ _[\u2212]_ _[i][\u2212]_ 2 [2] . For symmetric zero-mean distribu\ufffd \ufffd Figure 3: Plot of Marginal Score Multiplied by Density for\n\ntions, all odd cumulants are zero (for the same reason Increasing _r_\nthat all odd moments of a symmetric distribution are\nzero). Hence, the rate of convergence to the limiting distribution is controlled by the 4th order term, which has\nrate _O_ \ufffd _r_ _[\u2212]_ [1][\ufffd] .\n\nAlthough the full distribution _p_ ( _Z_ _[r]_ ) has no general closed-form solution, the distribution over marginals\n_p_ ( _zi,j_ ) is more amenable to analysis. We derive _p_ ( _zi,j_ ) for generalised Gaussian distributed _ai,j_ and _bi,j_ in\nSection B. To illustrate the fast converge rate, we plot the negative density _\u00d7_ score function _p_ ( _zi,j_ ) _zi,j_ for the\nmarginal distribution _p_ ( _zi,j_ ) in Fig. 3 using Gaussian distributed _ai,j_ and _bi,j_ with _\u03c3_ 0 [2] [= 1][ (see Theorem][ 4][ for]\n\n2\n\na derivation). The figure shows that _p_ ( _zi,j_ ) _zi,j_ quickly converges to the limiting function ~~_\u221a_~~ _[z][i]_ 2 _[,j]_ _\u03c0_ [exp] \ufffd _\u2212_ _[z][i]_ 2 _[,j]_ \ufffd,\n\nrecovering the Gaussian form from the true natural policy gradient update. Even at _r_ = 1, the function is not a\npoor approximation. After _r_ = 10, the function has nearly converged and after _r_ = 50, the function is visually\nindistinguishable from the limit, providing evidence for the hypothesis that the low-rank approximation is\naccurate even for very low rank regimes _r \u226a_ min( _m, n_ ).\n\n2\n\n_[,j]_ _\u2212_ _[z][i][,j]_\n\n2 _\u03c0_ [exp] \ufffd 2\n\na derivation). The figure shows that _p_ ( _zi,j_ ) _zi,j_ quickly converges to the limiting function ~~_\u221a_~~ _[z][i]_ 2 _[,j]_\n\n8"}, {"role": "assistant", "content": "**Verdict:** This research provides a critical efficiency gain for Gaussian Evolution Strategies, enabling accurate gradient approximations with significantly fewer computational resources.\n\n**Summary:** This section demonstrates that the low-rank approximation of Gaussian Evolution Strategies (ES) gradients converges to the true gradient at a surprisingly fast rate of O(1/r), outperforming the typical O(1/\u221ar) expectation. This acceleration is not a statistical fluke but stems from the inherent symmetry of the problem, which causes lower-order error terms in the approximation to cancel out. The utility is substantial: achieving high accuracy requires a much smaller 'r' (rank/dimensionality), directly translating into *reduced computational cost* and *faster execution times* for ES-based optimization algorithms. This makes ES viable for scenarios with *limited hardware resources* or where *real-time performance* is critical, as accurate gradients can be obtained even in very low-rank regimes. There are no new hardware requirements; instead, the existing methodology becomes dramatically more efficient."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n**3.1** **Overview**\n\nDynaExq provides adaptive expert precision control for MoE inference by treating precision as a runtime-managed resource rather\nthan a fixed configuration. Instead of relying on offline PTQ decisions that assume static activation distributions, DynaExq continuously monitors expert usage and adjusts precision according to\nlong-term activation salience. This enables hot experts to retain accurate high-precision weights, while cold experts are aggressively\ncompressed to reduce the overall HBM footprint without affecting\nthe forward path.\nAs shown in Fig. 2, DynaExq consists of four coordinated components:\n\n - (1) **Expert Precision Controller**, which aggregates router\nstatistics, identifies persistently activated experts, and issues\npromotion or demotion commands.\n\n - (2) **Asynchronous Precision Switching Pipeline**, which\ntransfers expert weights across SSD, DRAM, and HBM with full\ncompute\u2013communication overlap.\n\n - (3) **Memory Pool Manager**, which maintains deterministic,\nfragmentation-free memory allocation for mixed-precision experts.\n\n - (4) **Resource-Aware Initialization Module**, which establishes\nper-layer precision budgets that guarantee memory stability\nunder variable workloads.\n\nTogether, these components support accuracy-aware precision allocation, non-blocking transitions, and stable throughput. By decoupling precision switching from the critical compute path, DynaExq\nsustains aggressive memory savings even when expert usage shifts\n\nover time.\n\n**3.2** **Runtime Expert Precision Controller**\n\nThe controller maintains a lightweight profile of expert behavior by\nsampling router outputs during inference. After each forward pass,\nDynaExq records the selected experts and accumulates gating probabilities, providing a workload-driven view of expert importance\nrather than relying on synthetic calibration data.\nFor each expert _\ud835\udc56_, the exponential-moving-average (EMA) hotness score within a window of length _\ud835\udc47_ is:\n\n_\ud835\udc46\ud835\udc56_ [(] _[\ud835\udc61]_ [)] = _\ud835\udefc\ud835\udc46\ud835\udc56_ [(] _[\ud835\udc61]_ [\u2212][1][)] + (1 \u2212 _\ud835\udefc_ ) _\ud835\udc54\ud835\udc56_ ( _\ud835\udc65\ud835\udc61_ ) _,_ (2)\n\n![](papers/images/Dynamic-Expert-Quantization-for-Scalable-MixtureofExperts-Inference.pdf-2-0.png)\n\n|U|Col2|\n|---|---|\n|High-precision|Low-precision|\n|High-precision|KV-Cache|\n\n**Figure 2: System architecture of DynaExq.**\n\n**Algorithm 1** Runtime Precision Scheduling for MoE Experts\n\n1: **Input:** Router probabilities _\ud835\udc54\ud835\udc56_ ( _\ud835\udc65\ud835\udc61_ ) for experts _\ud835\udc56_ = 1 _..\ud835\udc41_\n\n2: **State:** EMA hotness scores _\ud835\udc46\ud835\udc56_, precision state state[ _\ud835\udc56_ ] \u2208\n{HIGH _,_ LOW}\n\n3: **Parameter:** Update period _\ud835\udc47_, EMA factor _\ud835\udefc_, threshold _\ud835\udf0f_ (derived from memory budget)\n\n4: **procedure** UpdateHotness( _\ud835\udc61_ )\n\n5: **for** each activated expert _\ud835\udc56_ at step _\ud835\udc61_ **do**\n\n6: _\ud835\udc46\ud835\udc56_ \u2190 _\ud835\udefc\ud835\udc46\ud835\udc56_ + (1 \u2212 _\ud835\udefc_ ) _\ud835\udc54\ud835\udc56_ ( _\ud835\udc65\ud835\udc61_ )\n\n7: **end for**\n\n8: **for** each inactive expert _\ud835\udc57_ **do**\n\n9: _\ud835\udc46_ _\ud835\udc57_ \u2190 _\ud835\udefc\ud835\udc46_ _\ud835\udc57_ _\u22b2_ passive decay\n\n10: **end for**\n\n11: **end procedure**\n\n12: **procedure** PrecisionSchedule( _\ud835\udc61_ )\n\n13: **if** _\ud835\udc61_ mod _\ud835\udc47_ = 0 **then**\n\n14: Sort experts by _\ud835\udc46\ud835\udc56_ in descending order\n\n15: H \u2190 top- _\ud835\udc3e_ experts with _\ud835\udc46\ud835\udc56_ \u2265 _\ud835\udf0f_\n\n16: **for** each expert _\ud835\udc56_ **do**\n\n17: **if** _\ud835\udc56_ \u2208H and state[ _\ud835\udc56_ ] = LOW **then**\n\n18: EnqeueUpgrade( _\ud835\udc56_ )\n\n19: **else if** _\ud835\udc56_ \u2209 H and state[ _\ud835\udc56_ ] = HIGH **then**\n\n20: EnqeueDowngrade( _\ud835\udc56_ )\n\n21: **end if**\n\n22: **end for**\n\n23: **end if**\n\n24: **end procedure**\n\nwhere _\ud835\udc54\ud835\udc56_ ( _\ud835\udc65\ud835\udc61_ ) is the router probability at step _\ud835\udc61_ . This formulation\ncaptures both activation frequency and magnitude, while smoothing short-term spikes. Experts with _\ud835\udc46\ud835\udc56_ above the high-precision\nthreshold _\ud835\udf0f\u210e_ are promoted, while the remaining experts form the\nlow-precision candidate pool (as shown in Algo-1).\nTo ensure robust scheduling under fluctuating workloads, the\ncontroller operates at coarse granularity and applies smoothing to\navoid oscillations. Precision updates occur periodically rather than\nper batch, preventing thrashing when gating behavior is unstable.\n\n**MoE**\n\n**Inference**\n\n**H2D**\n\n**Transfer**\n\n**SSD**\n\n**Prefetch**\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n||**ei**|**ej**|**ek**|**em**|**en**|||\n|||||||||\n\n|Col1|Col2|Col3|downgrade|Col5|Col6|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n||**upgradeek**||||**upgrade e**|||\n||||**en**|**en**|**x**|**x**|**x**|\n\n|Col1|Col2|Col3|Col4|\n|---|---|---|---|\n||**Fetche**|||\n||**n**|**n**|**n**|\n\n**Figure 3: An example of Asynchronous Expert Swapping**\n**Pipeline.**\n\nBecause all operations execute on the CPU and do not block GPU\nkernels, the controller imposes no latency overhead on inference.\nThese mechanisms allow DynaExq to detect persistent hot experts reliably, enforce stable precision decisions, and preserve routing fidelity under strict memory constraints.\n\n**3.3** **Asynchronous Expert Swapping Pipeline**\n\nTo support continuous inference while precision changes occur\nin the background, DynaExq employs a multi-stage asynchronous\npipeline that overlaps promotion, demotion, and prefetching with\nMoE computation. High-precision and low-precision weights are\nstored on SSD and cached in DRAM, and all transfers to HBM use\n\ndedicated CUDA streams.\n\nWhen a cold expert becomes hot, the runtime triggers a nonblocking upgrade sequence: the high-precision weight is fetched\nfrom SSD (if not cached), buffered in DRAM, and streamed into\nHBM. After the transfer completes, a lightweight registration step\nbinds the new weight to the expert\u2019s compute context, and the\nlow-precision version is reclaimed immediately. Demotion mirrors\nthis process in reverse.\nDuring inference, DynaExq always uses the last stable version\nof each expert. If a promotion is in flight, the previous representation remains active, ensuring that pipeline latency never affects\nthe forward pass. This provenance-consistent strategy prevents\ncorrectness issues and avoids compute stalls even under highly\ndynamic workloads.\nTo further hide data movement latency, DynaExq proactively\nprefetches experts predicted to become hot by leveraging crosslayer activation correlations. As illustrated in Fig. 3, SSD prefetch,\nDRAM buffering, H2D transfers, and demotions proceed concurrently beneath ongoing computation, enabling smooth precision\n\ntransitions.\n\n**3.4** **Memory Pool Management**\n\nBecause precision switching changes expert storage sizes, na\u00efvely\nusing the GPU allocator can cause fragmentation and unpredictable\nmemory growth. To prevent this, DynaExq partitions HBM into\ntwo dedicated pools for high- and low-precision experts. Each pool\nis subdivided into fixed-size blocks aligned with the corresponding data format (e.g., FP16 or INT4), enabling constant-time and\nfragmentation-free allocation.\nDuring precision transitions, DynaExq immediately reclaims the\nold representation once the new weight is successfully registered,\nensuring that no expert holds dual copies longer than necessary.\nA small transient buffer absorbs transfer bursts, allowing multiple\nin-flight promotions without exceeding the memory budget.\n\nKexin Chu, Dawei Xiang, Zixu Shen, Yiwei Yang, Zecheng Lin, and Wei Zhang\n\nA compact CPU-side index records expert locations, precision\ntiers, and pool occupancy. Because this index is managed outside the\nGPU allocator, DynaExq avoids runtime cudaMalloc calls and eliminates allocator-induced jitter. This deterministic memory structure\nmaintains stable throughput even when experts frequently change\nprecision.\nBy imposing structure on the memory hierarchy and ensuring deterministic allocation rules, DynaExq provides predictable\nmemory behavior and safe operation under aggressive dynamic\nquantization.\n\n**3.5** **Resource-Aware Threshold Initialization**\n\nBefore inference begins, DynaExq computes safe precision budgets\nfor each MoE layer to ensure that dynamic switching never violates\nHBM constraints. Let _\ud835\udc40_ HBM denote available memory after reserving space for KV-cache, activations, and shared weights. Given the\naverage storage sizes of high-precision and low-precision experts,\n_\ud835\udc46_ [hot] and _\ud835\udc46_ [cold], and the number of experts _\ud835\udc41_, DynaExq computes\nfeasible values of _\ud835\udc5b_ hot and _\ud835\udc5b_ cold that satisfy:\n\n_\ud835\udc5b_ hot \u00b7 _\ud835\udc46_ [hot] + _\ud835\udc5b_ cold \u00b7 _\ud835\udc46_ [cold] \u2264 _\ud835\udc40_ HBM _,_\n\nWhere _\ud835\udc5b_ hot + _\ud835\udc5b_ cold = _\ud835\udc41_ and _\ud835\udf0f\u210e_ = _\ud835\udc61\ud835\udc5c\ud835\udc5d_ ( _\ud835\udc5b\u210e\ud835\udc5c\ud835\udc61,\ud835\udc46\ud835\udc56_ ). A brief warmup\nphase collects initial hotness scores, and the top- _\ud835\udc5b_ hot experts initialize the high-precision set. The threshold _\ud835\udf0f\u210e_ remains fixed while\nexpert identities may change at runtime based on observed activity.\nBy separating memory feasibility analysis from runtime scheduling, DynaExq guarantees that all precision transitions remain\nmemory-safe even under shifting workloads. This initialization step\nforms the foundation for predictable and stable mixed-precision\n\nexecution."}, {"role": "assistant", "content": "**Verdict:** This paper presents a highly valuable system for deploying Mixture-of-Experts (MoE) models more efficiently by dynamically managing expert precision, directly addressing a critical HBM memory bottleneck without incurring runtime latency. The utility for large-scale MoE inference is substantial.\n\n**Summary:** DynaExq offers significant memory savings for MoE inference by adaptively quantizing 'cold' experts to lower precision while retaining 'hot' experts in high precision. This allows for a reduced overall HBM footprint, enabling the deployment of larger MoE models or more models per GPU. Critically, it achieves this without introducing latency overhead to the forward pass, as precision switching operations are performed asynchronously on the CPU and via dedicated CUDA streams, fully overlapping with computation. The system ensures stable throughput through deterministic memory management (memory pools) and guaranteed memory safety via a resource-aware initialization step. The primary hardware requirement is a multi-tiered memory hierarchy (HBM, DRAM, SSD) capable of supporting efficient data transfers. While adding system-level complexity, the benefits in memory efficiency and adaptive performance make it a compelling solution for scaling MoE inference."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\nS( _\u20d7\u03b3k_ ) _\u0338_ = _\u2205_ **then** Continue\n**else** Stop\n\n**4** Compute **E** _k_ and _\u20d7\u03b1k_ from S( _\u20d7\u03b3k_ ) and _v_\n\n**5** Determine **Q** _k_ and **a** _k_ from **E** _k_ and _\u20d7\u03b1k_\n\n**6** Calculate by Formula (3.15) **x**\n\n**7** _k \u2190\u2212_ _k_ + 1\n\n**8** _\u20d7\u03b3k_ +1 _\u2190\u2212_ **x**\n\n**9** Determine S( _\u20d7\u03b3k_ +1)\n**until** S( _\u20d7\u03b3k_ +1) = S( _\u20d7\u03b3k_ )\n\n**end**\n\nMeinhardt (2013, Theorem 8.1.2) establishes that this iterative procedure converges toward a pre-kernel\npoint. In view of Meinhardt (2013, Theorem 9.1.2), we even know that at most \ufffd _n_ 2\ufffd _\u2212_ 1-iteration steps are\nsufficient to successfully terminate the search process. However, we have some empirical evidence that\ngenerically a maximum of _n_ + 1-iteration steps are needed to determine an element from the pre-kernel\n\nset.\n\n### 4 CHARACTERIZATION OF EFFICIENTLY SOLVABLE PROBLEMS\n\nResume that an algorithm is called to be of polynomial-time if its running time is upper bounded by a\npolynomial expression in the size of the input, as _O_ ( _n_ _[k]_ ), where _k_ is a constant. This means that the\nalgorithm can be considered efficient or tractable. In contrast, an algorithm is said to be of exponentialtime if the running time is upper bounded by _O_ (2 _[poly]_ [(] _[n]_ [)] ). For instance, if _poly_ ( _n_ ) = _n_, then we have\n_O_ (2 _[n]_ )\n\nTo measure the input, at least the data of a cooperative game in the total number of players must be\nsupplied to the model. Depending on the required precision and analysis of the model, more input data\nmust be provided. For instance, arbitrary precision is required for a rational number model to encode the\ninput as a finite string of bits. This implies that the number of elementary bit operations must be counted\nfor a rational number model to grasp the underlying bit complexity. This is not needed for a real Random\nAccess Machine (RAM) model, where it is assumed that the real number can be represented by infinite\nprecision so that each operation on a real number takes constant time, regardless of precision.\n\nFor estimating the computational complexity of an optimization problem, we also have to care about\nstrongly polynomial-time algorithms, for which it is stated that\n\n1. it consists of the basic operations, viz., addition, subtraction, multiplication, division, as well as\ncomparison; and\n\n2. these basic operations are carried out on rationals/integers of size polynomially bounded in the\ndimension of the input; and\n\n11\n\n_Polynomial-Time Algorithms for Computing the Nucleolus_\n\n3. the number of these operations is polynomially bounded in the dimension of the input.\n\nRoughly speaking, an algorithm is called strongly polynomial whenever its running time is a polynomial function of the number of _n_ the size of the input and independent of the magnitude of the input\nvalues.\n\nMoreover, an algorithm is called **NP** -hard if it refers to a problem that is at least as difficult to solve\nas the hardest problems in the complexity class **NP** (class of nondeterministic polynomial-time problems).\nIn particular, computing the (pre-)nucleolus or a (pre-)kernel point is **NP** -hard in general. This is due to\nthe fact that the best-known general algorithms for computing the (pre-)nucleolus involve, for instance,\nsolving a sequence of linear programming problems, which have a running time that is exponential in the\nnumber of players _n_, since there are 2 _[n]_ possible coalitions (exponential complexity).\n\n### 5 ELLIPSOID METHOD OF COMPUTING THE NUCLEOLUS\n\nFaigle et al. (2001) proposed a first polynomial-time algorithm for computing the nucleolus of a TU game\nwhile attacking an allocation in the intersection of the least-core with the pre-kernel. For TU games for\nwhich the pre-kernel contains exactly one core element, one can conclude that even the nucleolus was\ncomputed due to the following result:\n\n**Theorem 5.1** (Single Point Intersection of Core and Per-Kernel) **.** _Let \u27e8_ _N, v \u27e9_ _be a TU game. If_ C( _N, v_ ) _\u2229_\nPK( _N, v_ ) = _{_ **x** _[\u2217]_ _} is satisfied, i.e., the intersection of the core and pre-kernel just consists of a single point,_\n_then it holds \u03bd_ ( _N, v_ ) = **x** _[\u2217]_ _. Hence, the single intersection point is the nucleolus of the game \u27e8_ _N, v \u27e9._\n\nThus, the method of Faigle et al. (2001) for computing the nucleolus of a TU game can be applied to\nconvex games, where the pre-kernel is a singleton, or minimum cost spanning tree games, where the core\nconsists of a single point. But not to the class of permutation games, where the pre-kernel is inscribed in\nthe least-core of the game (for more details, we refer the reader to Solymosi (2014); Meinhardt (2014)).\n\nIn order to determine this exposed point in the intersection of the core with the pre-kernel of a TU\ngame, Faigle et al. (2001) imposed a combination of the ellipsoid method for solving sequences of linear programming problems with an approximation scheme for a (pre-)kernel point. This approximation\nscheme was first postulated by M. Maschler at the 1965 Jerusalem Conference on Game Theory, where\nthe convergence was given as an open problem. In the context of the ellipsoid method, sufficiently many\nsteps of Maschler\u2019s approximation scheme must be invoked to find suitable separating hyperplanes to apply the ellipsoid method. Stearns (1968) provided for Maschler\u2019s approximation scheme of the kernel a\nconvergence proof using topological arguments while imposing the crucial condition that there exists an\ninfinite number of transfers between a pair of players that are maximal. The required boundedness on the\ntransfer to assure convergence was inherited by the compactness of the imputation set and the definition of\nthe kernel. By Faigle et al. (2001, Theorem 4.1), this result was extended to the pre-kernel without using\nany topological argument. Here, it was sufficient to establish that the imposed transfer scheme is bounded\non the pre-imputation set, which is not a compact set. Nevertheless, their proof is logically flawed since\nthe authors confound a proof by contradiction with a proof by contraposition (for further details, we refer\nto Meinhardt (2021)). Fortunately for them, their proof works out.\n\nNotice that the main result of Faigle et al. in establishing a polynomial-time computation of the nucleolus relies on the crucial assumption that there exists a polynomial-time algorithm for computing all\nsurpluses _sij_ ( **x** _, v_ ) over all distinct pairs of players for any allocation **x** _\u2208_ R _[n]_ . This is equivalent to imposing the assumption that the minimum excess _e_ _[v]_ min [(] **[x]** [)][ for any allocation] **[ x]** _[ \u2208]_ [R] _[n]_ [ can also be efficiently]\n\n12\n\n_Polynomial-Time Algorithms for Computing the Nucleolus_\n\ncomputed by an algorithm, viz., in polynomial-time. This allows the authors to disregard the problematic\nissue of **NP** -hardness.\n\nDue to the known drawback of numerical instability in a real data model (unboundness of complexity;\nsee below for more details) of the ellipsoid method, Faigle et al. studied the ellipsoid method in the field\nof rational numbers. This imposes several requirements on the input data that must be considered for\na rational number model. Firstly, the imposed arbitrary precision of a rational number model requires\nknowledge of the encoding length of the largest number in the model that imposes an upper bound on the\ninput size. In addition, it requires that the authors need to estimate the facet complexity of the polyhedron\nspanned by the system of linear inequalities of the linear programming problem that represents the leastcore or core of the TU game. The facet complexity indicates the total number of bits required to encode\nthe system of linear constraints (polyhedron) in the model. Moreover, the computational efficiency of\nthe ellipsoid method stems from its ability to deal implicitly with an exponential number of constraints\nof a linear programming problem through a compact separation oracle. A separation oracle is a routine\nthat, given a point, either confirms that the point is in the feasible set or provides a separating hyperplane.\nFor real-valued linear programming problems, the separation oracle is just a matter of checking all the\nconstraints, which is done efficiently. The complexity of the separation oracle needs to be accounted for\nin the overall runtime. In this respect, an oracle must query to compute and output the coalitional value as\na rational number.\n\nHowever, because of numerical instability issues of the ellipsoid method with floating-point arithmetic,\nthe approach of Faigle et al. has practical limitations with floating-point arithmetic (for more details, see\nbelow).\n\n### 6 SKETCH OF THE ELLIPSOID METHOD\n\nThe algorithm for the ellipsoid method, pioneered by Khachiyan (1979) for linear programming problems,\nworks through iteratively creating smaller ellipsoids that contain a convex body like a polyhedron (feasible\nregion). The process is started with an ellipsoid that contains the feasible region. In each subsequent step,\nthe volume of the ellipsoid is reduced by a constant factor. This is done by calculating in a first step\nfor each current ellipsoid its center of gravity. In the second step, all objective values that direct in the\ndirection where all of them are becoming worse off are thrown away since the bad region cannot contain\nthe optimum. By contrast, all of those objective values that are becoming better off are put in a new\nellipsoid containing still the polyhedron, allowing the volume to be reduced by a constant factor.\n\nIn case the center of gravity of the current ellipsoid is feasible for the original linear programming\nproblem, the bad values are thrown away as before. Otherwise, the center of gravity is not feasible; there\nis still a region with bad values. The bad region of the current ellipsoid is removed as before. A new\nellipsoid is created that is smaller than the current ellipsoid. The number of iterations is governed by how\nmany such reductions are needed to guarantee that the ellipsoid is either empty or contains a point in the\nfeasible region (for more details, see Gr\u00f6tschel et al. (2012, Chapter 3)).\n\nBy the work of Gr\u00f6tschel et al. (1981, Thm. 3.1), Gr\u00f6tschel et al. (2012, Chapter 6), it has been established that the problems of separation and optimization for any given polyhedron P are computationally\nequivalent since there is a polynomial-time algorithm to solve the separation problem of a polyhedron\nP if, and only if, there is a polynomial-time algorithm to solve the linear optimization problem for the\npolyhedron P. In fact, they provide a sharpened separation result to even deal with non-full-dimensional\npolyhedrons. Thus, the problem of strictly separating any given polyhedron P by a point **x** _\u0338\u2208_ P s.t.\n**b** _[\u22a4]_ _\u00b7_ **x** _<_ **b** _[\u22a4]_ _\u00b7_ **y** for all **y** _\u2208_ P with **b** _\u2208_ Q _[n]_ holds true is computationally equivalent to solving the linear\n\n13\n\n_Polynomial-Time Algorithms for Computing the Nucleolus_\n\noptimization problem related to polyhedron P.\n\n### 7 APPROXIMATION SCHEME OF THE KERNEL\n\nThe reader may confront our Approach 3.1 based on the Fenchel-Moreau conjugation with the Standard\nMethod 7.1 from the literature for computing a kernel element while iteratively carrying out a sequence\nof side-payments between pairs of players to reduce the differences in the maximum surpluses between\nthem. Thus, the algorithm is based on the idea to iteratively carrying out bilateral maximal transfers\nbetween pairs of players to approximate a kernel element, instead of a transfer scheme that simultaneously\nadjust the payoffs of all players involved in the game.\n\n**Algorithm 7.1:** Stearns\u2019 Transfer Scheme to converge to a Kernel Element\n\n**Data:** Arbitrary TU Game _\u27e8_ _N, v \u27e9_, and a payoff vector _\u20d7\u03b3_ 0 _\u2208_ _dom h_ .\n**Result:** A payoff vector s.t. _\u20d7\u03b3k_ +1 _\u2208_ K( _v_ ).\n**begin**\n\n**0** _k \u2190\u2212_ 0\n\n**1** Select an arbitrary starting point _\u20d7\u03b3_ 0.\n\n**2** Pre-specify a tolerance value _\u03f5_ .\n**repeat**\n\n**if** _\u20d7\u03b3k \u2208_ _dom h_ **then** Continue\n**else** Stop\n\n**3** Compute the maximum surplus _sij_ ( _\u20d7\u03b3k_ ) for all _i, j \u2208_ _N, i \u0338_ = _j_ by (2.5).\n\n**4** _\u03b4_ _[\u2217]_ _\u2190\u2212_ max _ij\u2208N,i_ = _\u0338_ _j {sij_ ( _\u20d7\u03b3k_ ) _\u2212_ _sji_ ( _\u20d7\u03b3k_ ) _}_ .\n\n**5** Select a player pair ( _i_ _[\u2217]_ _, j_ _[\u2217]_ ) s.t. ( _i_ _[\u2217]_ _, j_ _[\u2217]_ ) _\u2208{i, j \u2208_ _N | \u03b4_ _[\u2217]_ = _sij_ ( _\u20d7\u03b3k_ ) _\u2212_ _sji_ ( _\u20d7\u03b3k_ ) _}_ .\n\n_\u0338_"}, {"role": "assistant", "content": "**Verdict:** Conditional utility; the paper highlights a significant theoretical advance for a subset of problems, but practical deployment is hampered by computational constraints and numerical instability for general cases.\n\n**Summary:** This section analyzes methods for computing the nucleolus and pre-kernel in cooperative games, problems that are generally **NP-hard** \u2013 implying exponential computational costs for larger `n` (number of players), which makes them intractable for many real-world scenarios. The **key value** is the identification of a **polynomial-time algorithm** by Faigle et al. (2001) for computing the nucleolus in *specific* types of TU games (e.g., convex games, minimum cost spanning tree games). This represents a substantial efficiency improvement for these constrained problem sets. However, **practical utility is significantly limited** by several factors:1.  **Numerical Instability:** The core 'Ellipsoid Method' suffers from numerical instability with standard floating-point arithmetic. This necessitates a 'rational number model', which demands arbitrary precision, increasing **software development costs** (for specialized libraries) and **runtime** due to complex encoding and operations. This also impacts **hardware requirements** by demanding more memory and processing power for arbitrary-precision arithmetic.2.  **Input Complexity:** The rational number model requires estimating 'facet complexity' and a polynomial-time separation oracle, further adding to computational overhead and implementation complexity.3.  **General NP-Hardness:** For games that don't fit the specific criteria, the problem remains NP-hard, making generalized solutions extremely **costly and slow** for practical applications.4.  **Approximation Scheme Limitations:** While an alternative, Stearns' approximation scheme is noted to have potential logical flaws in its extended convergence proofs, suggesting a lack of robust theoretical guarantees, which could affect reliability.In summary, while there's a valuable theoretical breakthrough for specific game types (improving speed), the **high computational costs, precision requirements, and numerical challenges** of the Ellipsoid Method, coupled with the inherent NP-hardness of the general problem, limit its broad market adoption without substantial engineering investment to mitigate these practical hurdles. Focus on niche applications where the problem structure aligns with the polynomial-time solution and where the precision costs can be absorbed."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n_\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61.\ud835\udc5f\ud835\udc61\ud835\udc61_ \u2265 0 **and** ( _\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61.\ud835\udc5f\ud835\udc61\ud835\udc61_ \u2264 _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc45\ud835\udc47\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc5d_ **or** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5f\ud835\udc61\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc5d_\ud835\udc52\ud835\udc65\ud835\udc5d\ud835\udc56\ud835\udc5f\ud835\udc52\ud835\udc51_ ) **then**\n\n**94** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc45\ud835\udc47\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc5d_ = _\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61.\ud835\udc5f\ud835\udc61\ud835\udc61_ ;\n\n**95** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5f\ud835\udc61\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc5d_ _ _\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5a\ud835\udc5d_ = _\ud835\udc41\ud835\udc5c\ud835\udc64_ () ;\n\n**96** BBR_Check_Probe_RTT():"}, {"role": "assistant", "content": "**Verdict:** Essential Baseline Measurement for Network Optimization.\n\n**Summary:** This code provides a fundamental mechanism for continuously identifying the minimum network Round Trip Time (RTT) or propagation delay. By actively tracking the lowest observed RTT and incorporating an expiry mechanism, it ensures that the system's understanding of baseline network performance remains current and accurate. This capability is critical for advanced congestion control algorithms to function effectively, leading to improved throughput and reduced latency. The implementation incurs negligible computational or hardware costs, primarily involving basic comparisons, assignments, and a function call. Its high utility lies in enabling smarter network resource allocation and management, directly contributing to a better user experience and potentially greater network capacity utilization."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n**6.1** **Pure Integer Pretraining of an RNN Language Model**\n\nTo demonstrate the potential of EGGROLL as a general optimization method, we study whether EGGROLL\ncould be used for language model pretraining. Since EGGROLL does not rely on gradients, we can explicitly\ndesign a language model architecture to be efficient and hardware-friendly at inference time. In particular, we\nbuild a model under the following constraints to emphasize the flexibility of EGGROLL:\n\n1. **Pure Integer Training:** On H100 systems, int8 is the fastest datatype, with int8 matrix multiplication\nwith int32 accumulation being the fastest tensor core operation. Furthermore, integer datatypes are\nmuch simpler to implement in hardware, providing massive energy savings for high-throughput\nsystems (Horowitz, 2014). Therefore, we keep all weights in int8 and all activations in integer\nformats, _never_ casting to floating point at any point during training.\n\n2. **Nonlinear RNN:** Modern language models use sequence-parallel architectures like Transformers and\nSSMs, since they enable stable gradients without backpropagation through time. However, most of\nthese sequence-parallel architectures are unable to handle simple state tracking (Merrill et al., 2024),\nwhereas classic recurrent networks like LSTMs and GRUs can handle these problems with a single\nlayer. Since EGGROLL does not require backpropagation through time, we can train on unbounded\nsequence lengths (Li et al., 2023a) with nonlinear RNNs of broader complexity classes. Specifically,\nwe develop a variant of the minGRU model (Heck & Salem, 2017) that performs all operations in\ninteger formats.\n\n3. **Removal of all Activation Functions:** Inspired by Foerster (2017), we remove all activation\nfunctions, like the rectified linear unit and hyperbolic tangent, due to the nonlinearity present in the\nint8 datatype. Specifically, the saturated addition of int8 values provides sufficient nonlinearity due to\nthe implicit clipping of values to the int8 dynamic range, which evolution strategies can exploit.\n\nWe call the resulting language model EGG, the **E** volved **G** enerative **G** RU, an EGGROLL-friendly architecture.\nIts architecture is similar to standard pre-layernorm transformer decoder models, but we (1) use a variant of L1\nnormalization instead of L2 normalization for our layernorm to avoid square roots, (2) replace self-attention\nwith our custom GRU, and (3) perform all operations in integer datatypes. See Appendix C for more details on\nthe architecture.\n\nWe train an EGG model with 6 layers and hidden dimension 256 to do character-level prediction on the\nminipile dataset (Kaddour, 2023). We update parameters after 100 tokens for each population member,\napplying truncated ES by keeping the hidden state and only resetting at document boundaries. We plot the\ntest loss in Fig. 2b over training steps across a range of population sizes, where the best test loss is 3.41\nbits/byte. We find that training is stable and loss curves are relatively smooth, especially with large population\nsizes, avoiding loss spikes, nan values, and other instabilities associated with backprop-based training at low\nprecision datatypes.\n\nNote that our largest population size is 2 [18] = 262144, which is two orders of magnitude larger than the largest\nexperiment done by Salimans et al. (2017) while only requiring a single GPU to train. We see that multiplying\nthe population size by 8 results in the loss dropping by approximately 0.4 over the range of population values\nwe have tested, though this pattern will eventually break since the loss must be strictly positive. We conduct\nmore ablations in Appendix E, determining how data efficient training can be achieved with EGGROLL and\nvalidating the importance of large batch sizes.\n\n**6.2** **Reinforcement Learning Tasks**\n\nIn these experiments, we compare the performance of EGGROLL against standard OpenES as implemented in\nSalimans et al. (2017) on reinforcement learning tasks. Given the small network sizes, we can use Open ES at\nthis scale, but we note that as network sizes increase, the use of vanilla OpenES becomes impossible. We use\nthe standard formulation of simply optimizing for the final return in the environment. For both EGGROLL\nand OpenES, we perform hyperparameter optimization (HPO) separately for each environment. For each\nalgorithm\u2013environment pair, we define plausible ranges for all key hyperparameters based on prior work and\npreliminary experiments. We then perform 20 random search trials, where each trial corresponds to a single\n\n9\n\nPendulum-v1\n\n1.0\n\n0.5\n\n0.0\n\n1.0\n\n0.5\n\n0.0\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-9-0.png)\n\n0 2 4\nSteps 1e8\n\nJumanji 2048\n\n0 2 4\nSteps 1e8\n\nCraftax Symbolic\n\n0 2 4\nSteps 1e8\n\nNavix DoorKey (8x8)\n\n0 2 4\nSteps 1e8\n\nBrax Inverted Double Pendulum\n\n1.0\n\n0.5\n\n0.0\n\n0 2 4\nSteps 1e8\n\nKinetix Thrust Control Left (m)\n\n1.5\n\n1.0\n\n0.5\n\n0.0\n\n0 2 4\nSteps 1e8\n\n1.0\n\n0.5\n\n0.0\n\n1.0\n\n0.5\n\n0.0\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-9-1.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-9-2.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-9-3.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-9-4.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-9-5.png)\n\nFigure 4: Comparison of reinforcement learning Mean returns normalized by PPO performance for 10 seeds. The returns\nare evaluated using the mean of the parameters. HPO was conducted for each algorithm/environment pair. The shaded\nregion is the standard error of the mean .\n\ntraining run with a randomly sampled hyperparameter configuration. Each configuration is evaluated based on\nthe final return achieved by the mean policy parameters at the end of training. After all trials, we select the\nconfiguration that yields the highest final return. Using this best configuration, we then run 10 independent\nseeds to evaluate performance and report the mean and standard error of the mean across these seeds.\n\nWe use policy networks with 3 layers of 256 neurons and a range of environments that demonstrate different\ncapabilities. We evaluate across the Navix (Pignatelli et al., 2024), Craftax (Matthews et al., 2024), Brax\n(Freeman et al., 2021), Kinetix (Matthews et al., 2025), and Jumanji (Bonnet et al., 2024) suites of environments.\nWe evaluate 16 environments in total. To pick environments, we choose environments that are not trivial or\nimpossible for PPO to solve, according to the original papers. We also choose environments that are part of\ndifferent categories when these are available (e.g. environment size in Kinetix or categories in Jumanji).\n\nWe show a subsample of the environments that were evaluated in Fig. 4. The remaining environment results\nare in Appendix G.1. Our findings show that EGGROLL is competitive with Open ES on 7/16 environments,\nunderperforms on 2/16, and outperforms on 7/16. This does not take into account the speed-ups when\ncompared to using OpenES (full-rank updates). We postulate that the reason for this performance increase is\nthat the large networks are difficult to optimize for Open ES and lend themselves well to low rank updates. All\nhyperparameter configuration details are available in Appendix G.1.\n\n**6.3** **LLM Fine-tuning for Reasoning Tasks**\n\nWe apply EGGROLL for LLM finetuning of RWKV-7 (Peng et al., 2025) models in two reasoning tasks:\ncountdown and GSM8K. The RWKV architecture is a recurrent model that, compared to transformers, is\nespecially suited to parallelization due to the fact that any memory otherwise spent on the KV cache can be\nused to evaluate population members. The training curves of EGGROLL and GRPO in countdown are shown\nin figure 5a. EGGROLL fine-tuning on an RWKV-7 1.5B model converges to a higher validation accuracy\nof 35% (v.s. 23%) under the same hardware and wall-clock time in the countdown task. Similarly, figure 5b\n\n10\n\nGSM8K \u2014 RWKV 7g7B\n\n0 2 4 6 8 10\n\nRelative wall-clock time (hours)\n\nGRPO (n=3) EGGROLL (n=3)\n\n(b)\n\n0.3\n\n0.2\n\n0.1\n\n0.0\n\nCountdown \u2014 RWKV 7g1.5B\n\n0 1 2 3 4 5 6 7 8\n\nRelative wall-clock time (hours)\n\nGRPO (n=3) EGGROLL (n=3)\n\n(a)\n\n0.80\n\n0.75\n\n0.70\n\n0.65\n\n0.60\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-10-0.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-10-1.png)\n\nFigure 5: Comparison of the validation score of 3 seeds of EGGROLL v.s. 3 seeds of GRPO in: (a) Countdown task with\nan RWKV 7g1.5B model on a single GPU. EGGROLL allows 1024 parallel generations per GPU whereas GRPO only 32.\n(b) GSM8K task with an RWKV 7g7B model on 8 GPUs. EGGROLL allows 8096 parallel genrations (1024 per GPU)\nwhereas GRPO only 256 (32 per GPU).\n\nshows that EGGROLL outperforms GRPO on GSM8K fine-tuning. Our scoring function draws parallels to the\ngroup relative advantage of GRPO. In particular, to score a set of noise directions, _E \u2261{E_ 1 _, . . ., En}_, we first\ncompute their accuracies, _{s_ 1 _,qi, . . ., sn,qi}_, on _|q|_ = _m_ questions, creating a matrix of scores _S \u2208_ R _[m][\u00d7][n]_ .\nWe then compute the _z_ score per question, with the main difference that we use the global variance \u00af _\u03c3_, and\naverage over all the questions to compute the final score for the noise direction _Ei_ :\n\n_m_\n\n\u00af\n_si_ = [1]\n\n_m_\n\n_m_\n\ufffd\n\n\ufffd _zi,qj_ = _m_ [1]\n\n_j_ =1\n\n_m_\n\ufffd\n\n_j_ =1\n\n_si,j \u2212_ _\u00b5qj_\n\n_\u03c3_ \u00af _._\n\nThis scoring function has the purpose of weighting all questions within the same batch the same across\npopulation members."}, {"role": "assistant", "content": "**Verdict:** Strong Buy. This research presents a highly promising, hardware-optimized approach to model training that addresses critical pain points in large-scale AI deployment.\n\n**Summary:** EGGROLL demonstrates significant utility by enabling high-performance, energy-efficient model training. The 'Pure Integer Training' is a major breakthrough, leveraging existing H100 hardware for maximal throughput and massive energy savings by exclusively using int8/int32 operations. This directly translates to lower operational costs for data centers. Its gradient-free nature avoids common backpropagation instabilities, leading to smoother, more reliable training\u2014reducing development cycles and debugging. EGGROLL's ability to support novel, hardware-friendly architectures (like nonlinear RNNs without explicit activation functions and L1 normalization) further differentiates it. The method shows competitive to superior performance across language model pretraining, reinforcement learning, and LLM fine-tuning tasks, even handling population sizes two orders of magnitude larger than previous ES methods on a single GPU. This scalability, coupled with hardware efficiency and training stability, positions EGGROLL as a compelling alternative for next-generation AI model development, particularly where cost, speed, and reliability are paramount."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\nIn this work, we study the problem of approximating Kemeny constant problem for both static and dynamic graphs. We propose two\nnovel formulas of Kemeny constant and design an biased estimator\nbased on spanning trees and 2-forests. For static graphs, we develop\na sampling-based algorithm with stronger theoretical guarantees.\nThe proposed method outperforms SOTA approaches in both efficiency and accuracy on real-world datasets. For dynamic graphs,\nwe introduce two sample maintenance strategies that efficiently\npreserve the correctness of samples instead of recomputing from\nscratch for each update. Both two methods are faster than static\nalgorithms, with only a slight loss in precision. Extensive experiments on real-world graphs demonstrate the effectiveness and\nefficiency of our solutions.\n\n**REFERENCES**\n\n[1] William Aiello, Fan Chung, and Linyuan Lu. 2001. A random graph model for\npower law graphs. _Experimental mathematics_ 10, 1 (2001), 53\u201366.\n\n[2] Diego Altafini, Dario A Bini, Valerio Cutini, Beatrice Meini, and Federico Poloni.\n2023. An edge centrality measure based on the Kemeny constant. _SIAM J. Matrix_\n_Anal. Appl._ 44, 2 (2023), 648\u2013669.\n\n[3] Reid Andersen, Fan Chung, and Kevin Lang. 2006. Local graph partitioning using\npagerank vectors. In _FOCS_ . IEEE, 475\u2013486.\n\n[4] Eugenio Angriman, Maria Predari, Alexander van der Grinten, and Henning\nMeyerhenke. 2020. Approximation of the Diagonal of a Laplacian\u2019s Pseudoinverse\nfor Complex Network Analysis. _ESA_ 173 (2020), 6:1\u20136:24.\n\n[5] Luca Avena, Fabienne Castell, Alexandre Gaudilli\u00e8re, and Clothilde M\u00e9lot. 2018.\nRandom forests and networks analysis. _Journal of Statistical Physics_ 173 (2018),\n985\u20131027.\n\n[6] Bahman Bahmani, Abdur Chowdhury, and Ashish Goel. 2010. Fast Incremental\nand Personalized PageRank. _VLDB_ 4, 3 (2010), 173\u2013184.\n\n[7] B\u00e9la Bollob\u00e1s. 2013. _Modern graph theory_ . Vol. 184. Springer Science & Business\nMedia.\n\n[8] B\u00e9la Bollob\u00e1s and B\u00e9la Bollob\u00e1s. 1998. _Random graphs_ . Springer.\n\n[9] Mo Chen, Jianzhuang Liu, and Xiaoou Tang. 2008. Clustering via Random Walk\nHitting Time on Directed Graphs. In _AAAI_, Vol. 8. 616\u2013621.\n\n[10] Fan Chung and Ji Zeng. 2023. Forest formulas of discrete Green\u2019s functions. _J._\n_Graph Theory_ 102, 3 (2023), 556\u2013577.\n\n[11] S Condamin, O B\u00e9nichou, V Tejedor, R Voituriez, and Joseph Klafter. 2007. Firstpassage times in complex scale-invariant media. _Nature_ 450, 7166 (2007), 77\u201380.\n\n[12] Emanuele Crisostomi, Stephen Kirkland, and Robert Shorten. 2011. A Google-like\nmodel of road network dynamics and its application to regulation and control.\n_Internat. J. Control_ 84, 3 (2011), 633\u2013651.\n\n[13] Peter G Doyle. 2009. The Kemeny constant of a Markov chain. _arXiv preprint_\n_arXiv:0909.2636_ (2009).\n\n[14] Peter M. Fenwick. 1994. A New Data Structure for Cumulative Frequency Tables.\n_Softw. Pract. Exp._ 24, 3 (1994), 327\u2013336.\n\n[15] Julie Fournet and Alain Barrat. 2014. Contact Patterns among High School\nStudents. _PLoS ONE_ [9, 9 (09 2014), e107878. https://doi.org/10.1371/journal.pone.](https://doi.org/10.1371/journal.pone.0107878)\n[0107878](https://doi.org/10.1371/journal.pone.0107878)\n\n[16] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for\nnetworks. In _KDD_ . 855\u2013864.\n\n[17] Takanori Hayashi, Takuya Akiba, and Yuichi Yoshida. 2016. Efficient Algorithms\nfor Spanning Tree Centrality. In _IJCAI_, Vol. 16. 3733\u20133739.\n\n[18] Wassily Hoeffding. 1994. Probability inequalities for sums of bounded random\nvariables. _The collected works of Wassily Hoeffding_ (1994), 409\u2013426.\n\n[19] Guanhao Hou, Qintian Guo, Fangyuan Zhang, Sibo Wang, and Zhewei Wei. 2023.\nPersonalized PageRank on evolving graphs with an incremental index-update\nscheme. _SIGMOD_ 1, 1 (2023), 1\u201326.\n\n[20] Jeffrey J Hunter. 2014. The role of Kemeny\u2019s constant in properties of Markov\nchains. _Communications in Statistics-Theory and Methods_ 43, 7 (2014), 1309\u20131321.\n\n[21] Michael F Hutchinson. 1989. A stochastic estimator of the trace of the influence\nmatrix for Laplacian smoothing splines. _Communications in Statistics-Simulation_\n_and Computation_ 18, 3 (1989), 1059\u20131076.\n\n[22] John G Kemeny, J Laurie Snell, et al. 1969. _Finite markov chains_ . Vol. 26. van\nNostrand Princeton, NJ.\n\n[23] Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford Large Network\n[Dataset Collection. http://snap.stanford.edu/data.](http://snap.stanford.edu/data)\n\n[24] Mark Levene and George Loizou. 2002. Kemeny\u2019s constant and the random\nsurfer. _The American mathematical monthly_ 109, 8 (2002), 741\u2013745.\n\n[25] Shiju Li, Xin Huang, and Chul-Ho Lee. 2021. An efficient and scalable algorithm\nfor estimating Kemeny\u2019s constant of a Markov chain on large graphs. In _KDD_ .\n964\u2013974.\n\n[26] Zihao Li, Dongqi Fu, and Jingrui He. 2023. Everything evolves in personalized\npagerank. In _WWW_ . 3342\u20133352.\n\n[27] Meihao Liao, Rong-Hua Li, Qiangqiang Dai, Hongyang Chen, Hongchao Qin,\nand Guoren Wang. 2023. Efficient personalized pagerank computation: The\npower of variance-reduced monte carlo approaches. _SIGMOD_ 1, 2 (2023), 1\u201326.\n\n[28] Meihao Liao, Rong-Hua Li, Qiangqiang Dai, Hongyang Chen, Hongchao Qin,\nand Guoren Wang. 2023. Efficient resistance distance computation: The power\nof landmark-based approaches. _SIGMOD_ 1, 1 (2023), 1\u201327.\n\n[29] Meihao Liao, Rong-Hua Li, Qiangqiang Dai, Hongyang Chen, and Guoren Wang.\n2023. Scalable Algorithms for Laplacian Pseudo-inverse Computation. _arXiv_\n_preprint arXiv:2311.10290_ (2023).\n\n[30] Meihao Liao, Rong-Hua Li, Qiangqiang Dai, and Guoren Wang. 2022. Efficient\npersonalized pagerank computation: A spanning forests sampling based approach. In _SIGMOD_ . 2048\u20132061.\n\n[31] Meihao Liao, Junjie Zhou, Rong-Hua Li, Qiangqiang Dai, Hongyang Chen, and\nGuoren Wang. 2024. Efficient and Provable Effective Resistance Computation on\nLarge Graphs: An Index-based Approach. _SIGMOD_ 2, 3 (2024), 1\u201327.\n\n[32] Haoyu Liu and Siqiang Luo. 2024. BIRD: Efficient Approximation of Bidirectional\nHidden Personalized PageRank. _VLDB_ 17, 9 (2024), 2255\u20132268.\n\n[33] L\u00e1szl\u00f3 Lov\u00e1sz. 1993. Random walks on graphs. _Combinatorics, Paul erdos is_\n_eighty_ 2, 1-46 (1993), 4.\n\n[34] Sam Alexander Martino, Jo\u00e3o Morado, Chenghao Li, Zhenghao Lu, and Edina\nRosta. 2024. Kemeny Constant-Based Optimization of Network Clustering Using\nGraph Neural Networks. _The Journal of Physical Chemistry B_ 128, 34 (2024),\n8103\u20138115.\n\n[35] Dingheng Mo and Siqiang Luo. 2021. Agenda: Robust personalized pageranks in\nevolving graphs. In _CIKM_ . 1315\u20131324.\n\n[36] Naoto Ohsaka, Takanori Maehara, and Ken-ichi Kawarabayashi. 2015. Efficient\npagerank tracking in evolving networks. In _KDD_ . 875\u2013884.\n\n[37] Rushabh Patel, Pushkarini Agharkar, and Francesco Bullo. 2015. Robotic surveillance and Markov chains with minimal weighted Kemeny constant. _IEEE Trans._\n_Automat. Control_ 60, 12 (2015), 3156\u20133167.\n\n[38] Bibek Paudel and Abraham Bernstein. 2021. Random walks with erasure: Diversifying personalized recommendations on social and information networks. In\n_WWW_ . 2046\u20132057.\n\n[39] Pan Peng, Daniel Lopatta, Yuichi Yoshida, and Gramoz Goranci. 2021. Local\nalgorithms for estimating effective resistance. In _KDD_ . 1329\u20131338.\n\n[40] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning\nof social representations. In _KDD_ . 701\u2013710.\n\n[41] Maria Predari, Lukas Berner, Robert Kooij, and Henning Meyerhenke. 2023.\nGreedy optimization of resistance-based graph robustness with global and local\nedge insertions. _Soc. Netw. Anal. Min._ 13, 1 (2023), 130.\n\n[42] Ryan A. Rossi and Nesreen K. Ahmed. 2015. The Network Data Repository with Interactive Graph Analytics and Visualization. In _AAAI_ . [https:](https://networkrepository.com)\n[//networkrepository.com](https://networkrepository.com)\n\n[43] Purnamrita Sarkar and Andrew W Moore. 2011. Random walks in social networks\nand their applications: a survey. _Social Network Data Analytics_ (2011), 43\u201377.\n\n[44] Christian L. Staudt, Aleksejs Sazonovs, and Henning Meyerhenke. 2015.\nNetworKit: A Tool Suite for Large-scale Complex Network Analysis.\n[arXiv:1403.3005 [cs.SI] https://arxiv.org/abs/1403.3005](https://arxiv.org/abs/1403.3005)\n\n[45] Prasad Tetali. 1991. Random walks and the effective resistance of networks.\n_Journal of Theoretical Probability_ 4, 1 (1991), 101\u2013109.\n\n[46] Hanzhi Wang, Zhewei Wei, Junhao Gan, Ye Yuan, Xiaoyong Du, and Ji-Rong\nWen. 2022. Edge-based local push for personalized PageRank. _VLDB_ 15, 7 (2022),\n1376\u20131389.\n\n[47] Scott White and Padhraic Smyth. 2003. Algorithms for estimating relative\nimportance in networks. In _KDD_ . 266\u2013275.\n\n[48] David Bruce Wilson. 1996. Generating random spanning trees more quickly\nthan the cover time. In _STOC_ . 296\u2013303.\n\n[49] Feng Xia, Haifeng Liu, Ivan Lee, and Longbing Cao. 2016. Scientific article\nrecommendation: Exploiting common author relations and historical preferences.\n_IEEE Trans. Big Data_ 2, 2 (2016), 101\u2013112.\n\n[50] Feng Xia, Jiaying Liu, Hansong Nie, Yonghao Fu, Liangtian Wan, and Xiangjie\nKong. 2019. Random walks: A review of algorithms and applications. _IEEE_\n_Transactions on Emerging Topics in Computational Intelligence_ 4, 2 (2019), 95\u2013\n107.\n\n[51] Haisong Xia and Zhongzhi Zhang. 2024. Efficient Approximation of Kemeny\u2019s\nConstant for Large Graphs. _SIGMOD_ 2, 3 (2024), 1\u201326.\n\n[52] Wanyue Xu, Yibin Sheng, Zuobai Zhang, Haibin Kan, and Zhongzhi Zhang. 2020.\nPower-law graphs have minimal scaling of Kemeny constant for random walks.\nIn _WWW_ . 46\u201356.\n\n[53] Mingji Yang, Hanzhi Wang, Zhewei Wei, Sibo Wang, and Ji-Rong Wen. 2024.\nEfficient algorithms for personalized pagerank computation: A survey. _IEEE_\n_TKDE_ (2024).\n\n[54] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, and Sourav S Bhowmick.\n2020. Homogeneous Network Embedding for Massive Graphs via Reweighted\n\nCheng Li, Meihao Liao, Rong-Hua Li, Guoren Wang\n\nPersonalized PageRank. _VLDB_ 13, 5 (2020), 670\u2013683.\n\n[55] Renchi Yang and Jing Tang. 2023. Efficient estimation of pairwise effective\nresistance. _Proceedings of the ACM on Management of Data_ 1, 1 (2023), 1\u201327.\n\n[56] Serife Yilmaz, Ekaterina Dudkina, Michelangelo Bin, Emanuele Crisostomi,\nPietro Ferraro, Roderick Murray-Smith, Thomas Parisini, Lewi Stone, and Robert\nShorten. 2020. Kemeny-based testing for COVID-19. _PLOS ONE_ 15, 11 (2020),\n1\u201319.\n\n[57] Hongzhi Yin, Bin Cui, Jing Li, Junjie Yao, and Chen Chen. 2012. Challenging the\nLong Tail Recommendation. _VLDB_ 5, 9 (2012).\n\n[58] Minji Yoon, Woojeong Jin, and U Kang. 2018. Fast and accurate random walk\nwith restart on dynamic graphs with guarantees. In _WWW_ . 409\u2013418.\n\n[59] Hongyang Zhang, Peter Lofgren, and Ashish Goel. 2016. Approximate personalized pagerank on dynamic graphs. In _KDD_ . 1315\u20131324.\n\n[60] Xiaohan Zhao, Adelbert Chang, Atish Das Sarma, Haitao Zheng, and Ben Y\nZhao. 2013. On the embeddability of random walk distances. _VLDB_ 6, 14 (2013),\n1690\u20131701.\n\n[61] Yanping Zheng, Hanzhi Wang, Zhewei Wei, Jiajun Liu, and Sibo Wang. 2022.\nInstant graph neural networks for dynamic graphs. In _KDD_ . 2605\u20132615."}, {"role": "assistant", "content": "**Verdict:** This paper presents a significant advancement in Kemeny constant approximation for both static and dynamic graphs, offering superior performance compared to existing methods.\n\n**Summary:** The research introduces new computational formulas and a sampling-based algorithm for approximating the Kemeny constant. For static graphs, the method delivers state-of-the-art efficiency and accuracy on real-world datasets, implying reduced computational resources (CPU/memory) and faster analysis for large, stable networks. For dynamic graphs, the paper proposes sample maintenance strategies that avoid costly recomputations from scratch, leading to significantly faster updates with only a minor trade-off in precision. This incremental approach is crucial for real-time network analysis or applications with frequently changing graph structures, reducing operational costs associated with continuous re-analysis. The practical utility lies in enabling faster and more scalable insights into network connectivity and random walk behavior across various industries, from social networks to logistical planning, where Kemeny constant is a valuable metric."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n_\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc60\ud835\udc4e\ud835\udc61\ud835\udc52_ == _\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc52\ud835\udc35\ud835\udc4a_ **and** _\ud835\udc35\ud835\udc35\ud835\udc45_\ud835\udc3c\ud835\udc60_\ud835\udc41\ud835\udc52\ud835\udc65\ud835\udc61_\ud835\udc36\ud835\udc66\ud835\udc50\ud835\udc59\ud835\udc52_\ud835\udc43\u210e\ud835\udc4e\ud835\udc60\ud835\udc52_ () **then**\n\n**62** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc34\ud835\udc51\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52_ _ _\ud835\udc36\ud835\udc66\ud835\udc50\ud835\udc59\ud835\udc52_ _ _\ud835\udc43\u210e\ud835\udc4e\ud835\udc60\ud835\udc52_ () ;\n\n**63** BBR_Is_Next_Cycle_Phase():\n\n**64** _\ud835\udc56\ud835\udc60_ _ _\ud835\udc53\ud835\udc62\ud835\udc59\ud835\udc59_ _ _\ud835\udc59\ud835\udc52\ud835\udc5b\ud835\udc54\ud835\udc61\u210e_ = ( _\ud835\udc41\ud835\udc5c\ud835\udc64_ () \u2212 _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc50\ud835\udc66\ud835\udc50\ud835\udc59\ud835\udc52_ _ _\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5a\ud835\udc5d_ ) _> \ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc45\ud835\udc47\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc5d_ ;"}, {"role": "assistant", "content": "**Verdict:** Essential, low-cost core logic for network protocol stability.\n\n**Summary:** This snippet defines a critical state transition logic for network congestion control (likely BBR), governing when the system progresses its internal phases during bandwidth probing. The requirement to wait for a duration exceeding the Round Trip Propagation time (`BBR.RTprop`) before advancing ensures robust network assessment and prevents premature reactions. This is a fundamental software component with no specific hardware costs, relying on basic system timing. Its utility is high, directly contributing to network efficiency, throughput, and stability, which are key for user experience and service reliability. It represents a minimal overhead, high-impact design choice."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n_\ud835\udc37\ud835\udc52\ud835\udc59_ **_** _\ud835\udc45\ud835\udc61_ **_** _\ud835\udc5c\ud835\udc53_ **_** _\ud835\udc46\ud835\udc39\ud835\udc56_ ;\n\n**8** _\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59_ _ _\ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f_ _ _\ud835\udc5c\ud835\udc53_ _ _\ud835\udc46\ud835\udc39\ud835\udc60_ + + ;\n\n**9** _\ud835\udc47\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59_ _ _\ud835\udc37\ud835\udc52\ud835\udc59_ _ _\ud835\udc45\ud835\udc61_ + = _\ud835\udc37\ud835\udc52\ud835\udc59_ _ _\ud835\udc45\ud835\udc61_ _ _\ud835\udc5c\ud835\udc53_ _ _\ud835\udc46\ud835\udc39\ud835\udc56_ ;"}, {"role": "assistant", "content": "**Verdict:** Basic aggregation; provides essential data, but no innovative value.\n\n**Summary:** This section details the calculation of a cumulative performance metric (e.g., total delay rate) by summing individual service function contributions. It is a standard data aggregation technique. Utility lies in providing a foundational system-level metric for performance monitoring or subsequent analysis, but it does not present any new cost reductions, hardware efficiencies, or speed improvements. The computational cost is negligible, requiring only basic arithmetic operations."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n**11**\n\n3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n\n3.1.1 Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n\n3.1.2 Fine-Grained Cognitive Element Annotation . . . . . . . . . . . . . . . . . . . . . . . 12\n\n3.1.3 Problem Type Classification & Response Evaluation . . . . . . . . . . . . . . . . . . . 13\n\n3.1.4 Cognitive Structure Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n\n3.2 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n\n3.2.1 Dataset Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n\n3.2.2 Analysis Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n\n3.3 Results & Analyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n\n3.3.1 Distribution of Cognitive Elements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n\n3.3.2 Behavioral Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n\n3.4 Comparison with Humans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18"}, {"role": "assistant", "content": "**Verdict:** This section details the critical methodology and experimental design, which underpins the entire research. Its primary value is establishing the credibility and reproducibility of any findings on cognitive processes.\n\n**Summary:** The core utility of this methodological framework is to systematically deconstruct complex human (or AI) problem-solving into measurable, identifiable 'cognitive elements' and 'behavioral structures.' This capability offers a foundation for developing AI that mimics human reasoning, creating targeted educational interventions, or optimizing user interfaces by understanding how users genuinely think. The main cost drivers are significant human expert time for detailed 'Fine-Grained Cognitive Element Annotation' and the computational resources required for 'Cognitive Structure Extraction,' especially for large datasets. While data collection *might* demand specific hardware depending on the modality (not specified here), the analytical steps themselves primarily require standard compute resources for pattern recognition and statistical analysis. This isn't about raw speed improvements, but about a deeper understanding that can *inform* future efficiency gains in AI or human performance."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n_\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc61\ud835\udc52_ == _\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc52\ud835\udc45\ud835\udc47\ud835\udc47_ **then**\n\n**102** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc3b\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc59\ud835\udc52_ _ _\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc52_ _ _\ud835\udc45\ud835\udc47\ud835\udc47_ () ;\n\n**103** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc56\ud835\udc51\ud835\udc59\ud835\udc52_ _ _\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61_ = _\ud835\udc53\ud835\udc4e\ud835\udc59\ud835\udc60\ud835\udc52_ ;\n\n**104** BBR_Enter_Probe_RTT():\n\n**105** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc61\ud835\udc52_ = _\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc52\ud835\udc45\ud835\udc47\ud835\udc47_ ;\n\n**106** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc56\ud835\udc5b\ud835\udc54_ _ _\ud835\udc54\ud835\udc4e\ud835\udc56\ud835\udc5b_ = 1 ;\n\n**107** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ _ _\ud835\udc54\ud835\udc4e\ud835\udc56\ud835\udc5b_ = 1 ;\n\nEvaluating the Impact of Packet Scheduling and Congestion Control Algorithms on MPTCP Performance over\nHeterogeneous Networks 55\n\n**ACK-arrival function definitions (cont. 3/4):**\n\n**108** BBR_Handle_Probe_RTT():\n\n/* Ignore low rate samples during ProbeRTT: */\n\n**109** _\ud835\udc36.\ud835\udc4e\ud835\udc5d\ud835\udc5d_ _ _\ud835\udc59\ud835\udc56\ud835\udc5a\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc51_ = ( _\ud835\udc35\ud835\udc4a.\ud835\udc51\ud835\udc52\ud835\udc59\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc51_ + _\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61\ud835\udc60_ _ _\ud835\udc56\ud835\udc5b_ _ _\ud835\udc53\ud835\udc59\ud835\udc56\ud835\udc54\u210e\ud835\udc61_ ) ? : 1 ;"}, {"role": "assistant", "content": "**Verdict:** Essential Core Functionality.\n\n**Summary:** This section outlines BBR's `ProbeRTT` phase, a critical mechanism for maintaining accurate minimum Round Trip Time (RTT) estimations. By temporarily normalizing pacing and congestion window gains, the algorithm drains network queues, thereby revealing the true underlying network latency. This is foundational to BBR's performance, as RTT accuracy directly impacts its bandwidth and latency management. There are no notable hardware or direct cost implications; this is an algorithmic refinement that enhances BBR's robustness and efficiency across heterogeneous networks. Its utility is in ensuring BBR consistently delivers optimal throughput and low latency, translating to a superior user experience and more efficient network resource utilization."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\nWe now introduce and motivate our method EGGROLL, which is presented in Algorithm 1. In Section 4.1\nwe derive a low-rank ES update that approximates a full-rank ES gradient. One practical issue with using\na low-rank matrix approximation is that its distribution and score function have no analytic solution except\nfor degenerate cases, so in Section 4.2 we derive an alternative score function from the limiting high-rank\nGaussian which we propose as an approximation.\n\n5\n\n**4.1** **Low Rank Evolution Strategies** **Algorithm 1** EGGROLL( _r, \u03b1, \u03c3, T_ max _, N_ workers)\n\nRecall the Gaussian matrix ES update from Eq. (4).\nOur goal is to introduce a tractable approximation to\ngenerating full-rank matrices, using low-rank matrices\n_AB_ _[\u22a4]_ as our search matrices instead. We denote the\ndistribution of _A_ as _p_ ( _A_ ) and _B_ as _p_ ( _B_ ) assume that the\nelements of _A_ and _B_ are drawn independently:\n**Assumption 1.** _Assume all elements ai,j \u2208_ _A and_\n_bi,j \u2208_ _B are continuous identically and independently_\n_distributed random variables according to some zero-_\n_mean, symmetric, absolutely continuous (i.e., it has a_\n_density) distribution p_ 0( _\u00b7_ ) _with finite 4th order moments_\n_and variance_ 0 _< \u03c3_ 0 [2] _[.]_\n\n**initialise** _\u00b5_ and workers with known random seeds _\u03c2_\n**for** _T_ max timesteps **do**\n\n**for** Each worker _i \u2208{_ 1 _, . . . N_ workers _}_ in parallel **do**\n\n_Ai \u223c_ _p_ ( _Ai_ ) _, Bi \u223c_ _p_ ( _Bi_ )\n_Ei \u2190_ ~~_\u221a_~~ 1 _r_ _AiBi_ _[\u22a4]_\n_fi \u2190_ _f_ ( _\u00b5_ + _\u03c3Ei_ )\n**end for**\n\nWorkers share scalar fitness _fi_ with other workers\n**for** Each worker _i \u2208{_ 1 _, . . . N_ workers _}_ in parallel **do**\n\nReconstruct _Ej_ for _j \u2208{_ 1 _, . . . N_ workers _}_ from _\u03c2_\n\n_\u00b5 \u2190_ _\u00b5_ + _\u03b1_ _N_ Workers1 \ufffd _Nj_ =1Workers _Ej_ _fj_\n**end for**\n\n**end for**\n\nThe low-rank approximation _AB_ _[\u22a4]_ must be carefully integrated into the Gaussian matrix ES objective in\nEq. (3) to ensure that the variance of the updates remains bounded with increasing number of columns _r_ as the\nelements of _AB_ _[\u22a4]_ are a sum of _r_ random variables. To counter this, we scale the outer product by ~~_\u221a_~~ 1 _r_ which\nkeeps the variance of _E_ = ~~_\u221a_~~ 1 _r_ _AB_ _[\u22a4]_ as _O_ (1).\n\nObserve that _E_ = ~~_\u221a_~~ 1 _r_ _AB_ _[\u22a4]_ maps to the manifold M _[r]_ _\u2282_ R _[m][\u00d7][n]_ of rank _r_ matrices, which is a subset of R _[m][\u00d7][n]_ .\nThis means that the density _p_ ( _E_ ) is defined with respect to a unit volume over the manifold and cannot be\ndefined with respect to the standard unit volume in Euclidean space as the volume of the manifold is zero\nusing this measure. For the corresponding score function, gradients with respect to log _p_ ( _E_ ) are defined\nover the tangent space to M _[r]_ instead of in the usual Euclidean space. An intuitive reason for this is that\nthere is dependence between elements of _E_ and so it is not possible to take partial derivatives with respect\nto each element whilst keeping all other elements free. In principle it is possible to define all distributions\nand gradients with respect to the manifold M _[r]_, however this makes an analysis of low-rank updates difficult.\nInstead, we study the variable _Z_ = ~~_\u221a_~~ 1 _r_ _AB_ _[\u22a4]_ + _\u03f5_ where _\u03f5_ is an _m \u00d7 n_ matrix with independent decaying\n1\nGaussian elements _\u03f5i,j \u223cN_ (0 _,_ _[\u03c3]_ _\u03f5_ [2] _/r_ ). The variable _Z_ = ~~_\u221a_~~ _r_ _AB_ _[\u22a4]_ + _\u03f5_ has a well-defined density _p_ ( _Z_ ) for all\n_\u03c3\u03f5 >_ 0. This conceptual tool greatly simplifies analysis and as _\u03c3\u03f5_ can be made arbitrarily small and hence _p_ ( _E_ )\ncan be arbitrarily close to _p_ ( _Z_ ), the difference between the update used in practice and the update analysed is\nnegligible. Substituting for _p_ ( _E_ ) = _p_ ( _Z_ ) in Eq. (3), we derive the low-rank ES objective:\n\n_J_ LR( _\u00b5_ ) = E _Z\u223cp_ ( _Z_ ) [ _f_ ( _M_ = _\u00b5_ + _\u03c3Z_ )] _._\n\nWe now verify that _p_ ( _z_ ) is well-defined and show how _J_ LR( _\u00b5_ ) can be optimised using the score function of\n_p_ ( _Z_ ):\n**Theorem 1.** _**Low-rank ES Gradient.**_ _Let Assumption 1 hold. Then for finite r:_\n\n_gLR_ := _\u2207\u00b5JLR_ ( _\u00b5_ ) = _\u2212_ [1]\n\n_\u03c3_ [E] _[Z][\u223c][p]_ [(] _[Z]_ [)][ [] _[\u2207][Z]_ [ log] _[ p]_ [ (] _[Z]_ [)] _[ f]_ [(] _[M]_ [ =] _[ \u00b5]_ [ +] _[ \u03c3Z]_ [)]] _[ .]_\n\n_where Z is generated by sampling A \u223c_ _p_ ( _A_ ) _, B \u223c_ _p_ ( _B_ ) _, \u03f5 \u223c_ _p_ ( _\u03f5_ ) _and making the transformation Z_ =\n1 _AB_ _[\u22a4]_ + _\u03f5._\n~~_\u221a_~~ _r_\n\n**4.2** **Score Function Approximation**\n\nAs the mapping ( _A, B, \u03f5_ ) : _\u2192_ _Z_ = ~~_\u221a_~~ 1 _r_ _AB_ _[\u22a4]_ + _\u03f5_ is non-invertible, there does not exist a simple closed form\nsolution for the density _p_ ( _Z_ ) and score function except in degenerate cases such as _m_ = _n_ = 1. Instead, we\nuse an approximation for the score function _S_ [\u02c6] ( _Z_ ) _\u2248\u2207Z_ log _p_ ( _Z_ ):\n\n\u02c6 \u02c6\n_g_ LR = _\u2212_ [1] _S_ ( _Z_ ) _f_ ( _M_ = _\u00b5_ + _\u03c3Z_ ) _._ (5)\n\n_\u03c3_ [E] _[Z][\u223c][p]_ [(] _[Z]_ [)] \ufffd \ufffd\n\nTo optimise the ES objective using the EGGROLL update, we adapt the parallelised evolutionary strategies\nalgorithm from Salimans et al. (2017). In our experiments and Algorithm 1, we use a Gaussian approximate\n\n6\n\nscore function, which is obtained from taking the limit _r \u2192\u221e_ . As _\u03f5_ is a random matrix formed of independent\nGaussian elements, it can be decomposed into an equivalent sum of _r_ independent Gaussian matrices:\n\n1\n\n_\u03f5_ =\n~~_\u221ar_~~\n\n_r_\n\ufffd _\u03f5i,_\n\n_i_ =1\n\nwhere each random matrix _\u03f5i \u223c_ _p_ ( _\u03f5i_ ) has the same distribution as _\u03f5_ . Likewise, the matrix _AB_ _[\u22a4]_ can be\ndecomposed as a sum of independent, zero-mean vector outer products:\n\n_AB_ _[\u22a4]_ =\n\n_r_\n\ufffd _aib_ _[\u22a4]_ _i_ _[,]_\n\n_i_ =1\n\nwhere _ai_ and _bi_ are the _i_ th column vectors of _A_ and _B_ . This allows us to write _Z_ as a standardised sum of _r_\nindependent random matrices:\n\n1\n_Z_ =\n~~_\u221ar_~~\n\n_r_\n\ufffd \ufffd _aib_ _[\u22a4]_ _i_ [+] _[ \u03f5][i]_ \ufffd _._ (6)\n\n_i_ =1\n\nUnder Assumption 1, the central limit theorem proves that _p_ ( _Z_ ) converges in distribution to a Gaussian\n_N_ (0 _, Im\u03c3_ 0 [4] _[, I][n][\u03c3]_ 0 [4][)][. Using this limiting distribution in place of the true distribution] _[ p]_ [(] _[Z]_ [)][, we obtain the]\nGaussian approximate score function:\n\n_S_ \u02c6( _Z_ ) = _\u2212_ [1] _Z._ (7)\n\n_\u03c3_ 0 [4]\n\nWe remark that EGGROLL is not wedded to any particular score function approximator and we derive and\nexplore a set of mean-field approximators in Appendix B as alternatives, however our experiments show that\nthe Gaussian approximator has the best overall performance on the set of tasks we evaluated on. We make\na Monte Carlo estimate of expectation in Eq. (5) with _N_ workers samples to optimise the parameters _\u00b5_ using\n(approximate) stochastic gradient ascent. This yields the Gaussian EGGROLL update:\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-6-0.png)\n\nHere we have absorbed the constants _\u03c3_ [1] [and] _\u03c3_ 10 [4] [into the tunable learning rate] _[ \u03b1][t]_ [. As each random matrix]\n\n_Ei,t_ in Eq. (8) has rank _r_ almost surely and the matrix is updated using a sum of _N_ worker such matrices,\nthe overall EGGROLL matrix parameter update has rank min( _Nr, m, n_ ) almost surely, meaning that the\noverall parameter update is not restricted to be low-rank. For all experiments in Section 6, _Nr >_ min( _, m, n_ ),\nmeaning EGGROLL parameter updates are full-rank.\n\n**4.3** **Hardware-Efficient EGGROLL Implementation**\n\nA key reason to use EGGROLL over standard ES is that large populations can be simulated in parallel on a\nGPU thanks to the low-rank perturbations. For the sake of exposition, we write equations from the perspective\nof a single worker, _i_, and explain in text how this corresponds to batched GPU operations.\n\nConsider the task of computing a batched forward pass over inputs _xi \u2208_ R _[d][in]_ for a linear layer with mean\nparameter _\u00b5 \u2208_ R _[d][out][\u00d7][d][in]_ . The standard forward pass is just a regular matrix multiplication, _x\u00b5_ _[T]_, since _\u00b5_ is\nconstant across all threads. However, naively applying ES by trying to compute _xi_ ( _\u00b5_ + _\u03c3Ei_ ) _[T]_ becomes a\nbatched matrix multiplication, which is inefficient on GPUs since every element of _\u00b5_ + _\u03c3Ei_ is only used in a\nsingle multiplication, yielding poor arithmetic intensity.\n\n7\n\nHowever, with EGGROLL we know that _xi_ ( _\u00b5_ + _\u03c3Ei_ ) = _xi\u00b5_ + ~~_\u221a_~~ _\u03c3r_ ( _xiBi_ ) _A_ _[T]_ _i_ [. In this context, the bulk of]\ncompute is spent on the efficient calculation of _xi\u00b5_ using regular matrix multiplication. Meanwhile, when\n_r_ = 1, _xiBi_ simply becomes an inexpensive batched vector-vector dot product to get a batch of scalars, which\nis then processed by a batched scalar-vector multiplication when multiplying by _A_ _[T]_ _i_ [. This decomposition]\nis key to efficient batched LoRA inference, such as those used by vLLM (Kwon et al., 2023), which why\nEGGROLL achieves the same speeds as batched LoRA inference systems.\n\nWe additionally optimize the update process by not explicitly materializing the individual _Ei_ in the computation\nof [\ufffd] _[N]_ _i_ =1 _[E][i][f][i]_ [, the key term in the Gaussian approximate score function. In particular, when the rank is 1, we]\nreconstruct _A \u2208_ R _[N]_ _[\u00d7][d][out]_ and _B \u2208_ R _[N]_ _[\u00d7][d][in]_ and calculate the expression as ( _A \u2299_ _f_ ) _B_ _[T]_, which is a simple\nmatrix multiplication."}, {"role": "assistant", "content": "**Verdict:** HIGH UTILITY, SIGNIFICANT COST REDUCTION for large-scale Evolution Strategies.\n\n**Summary:** This section introduces EGGROLL, a method for massively scaling Evolution Strategies (ES) by replacing computationally expensive full-rank matrix updates with highly efficient low-rank approximations. The primary value proposition is enabling ES to operate effectively on GPUs with large populations, which was previously a major bottleneck. The core innovation lies in the practical realization of low-rank ES through a clever conceptual tool (adding small Gaussian noise) that enables tractable mathematical analysis. This, combined with deriving a simple, robust Gaussian approximate score function, leads to straightforward and efficient updates. Hardware efficiency is a key strength; EGGROLL re-architects the ES update calculation to leverage standard, highly optimized GPU matrix multiplication routines and decomposes the low-rank perturbation into cheaper batched vector operations. This results in performance comparable to batched LoRA inference systems. Costs are minimal, primarily associated with the initial derivation and a minor theoretical approximation error that is negligible in practice. Hardware requirements are GPUs, as it's specifically designed for parallel execution. The utility is substantial: it significantly increases the scalability and speed of ES, reduces compute costs by utilizing GPUs more efficiently, and has broad applicability beyond ES due to its general approach to low-rank matrix perturbations in high-performance machine learning systems."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n_\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61.\ud835\udc51\ud835\udc52\ud835\udc59\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc51_ \u2265 _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5b\ud835\udc52\ud835\udc65\ud835\udc61_\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc51_\ud835\udc51\ud835\udc52\ud835\udc59\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc51_ **then**\n\n**55** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5b\ud835\udc52\ud835\udc65\ud835\udc61_ _ _\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc51_ _ _\ud835\udc51\ud835\udc52\ud835\udc59\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc51_ = _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc51\ud835\udc52\ud835\udc59\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc51_ ;\n\n**56** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc51_ _ _\ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc61_ + + ;\n\n**57** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc51_ _ _\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61_ = _\ud835\udc61\ud835\udc5f\ud835\udc62\ud835\udc52_ ;"}, {"role": "assistant", "content": "**Verdict:** Critical component for high-performance network congestion control, delivering significant efficiency gains.\n\n**Summary:** This code implements the fundamental round-tracking mechanism within the BBR congestion control algorithm. Its utility is in enabling precise measurement of bottleneck bandwidth and round-trip time, which is crucial for maximizing throughput and minimizing latency, especially in challenging network environments like long-haul or high-latency connections. There are no specific hardware requirements; it's a software-only implementation executed on standard computing hardware within the network stack. The operational cost is minimal, involving basic variable comparisons and assignments, while the return on investment comes from improved data transfer efficiency and application responsiveness."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n**5.1** **Quantization for Mixture-of-Experts**\n\nRecent MoE quantization techniques demonstrate that low-bit PTQ\ncan substantially reduce expert memory cost. MoQE [35], QMoE [8],\nand MxMoE [6] show that 2\u20134 bit or sub-1-bit formats can preserve\nexpert functionality, while MoEQuant [12] and EAQuant [10] refine\nsampling and calibration to reduce accuracy loss. However, these\nmethods set expert precision _offline_ and keep it fixed during inference, making them unable to react to dynamic or highly skewed\nexpert activation patterns.\nDynaExq differs by treating precision as a _runtime-managed_\n_resource_ . Expert bit-widths are adjusted online based on EMAsmoothed activation statistics and memory budgets, enabling precision schedules that track long-term workload behavior rather than\nstatic calibration decisions.\n\n**5.2** **MoE Serving Systems and Expert Offloading**\n\nMoE serving frameworks improve expert execution and data movement across heterogeneous memory tiers. DeepSpeed-MoE [23],\nTutel [14], and Megablocks [11] optimize kernel and routing efficiency, while MoE-Infinity [33], ProMoE [26], and ExpertFlow [25]\nmanage SSD/CPU offloading and expert placement.\nThese systems optimize _where_ experts reside, but assume _fixed-_\n_precision_ weights. Thus, bandwidth and memory usage scale with\nthe full-precision representation of each expert, even when their\nactivation rates differ dramatically. DynaExq complements these\napproaches by introducing precision as an additional control dimension: cold experts are aggressively compressed, reducing both HBM\nfootprint and transfer volume, while hot experts retain high-fidelity\nrepresentations.\n\n|7.2|Col2|Col3|Col4|WikiT|Text|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n|6.4<br>6.6<br>6.8<br>7.0<br>7.2<br>Perplexity|||||||||\n|6.4<br>6.6<br>6.8<br>7.0<br>7.2<br>Perplexity|||||||||\n|6.4<br>6.6<br>6.8<br>7.0<br>7.2<br>Perplexity|||||||||\n|6.4<br>6.6<br>6.8<br>7.0<br>7.2<br>Perplexity|||||||||\n|6.4<br>6.6<br>6.8<br>7.0<br>7.2<br>Perplexity|||||||||\n\n**(a) Qwen3-30B-A3B**\n\n|6.4|Col2|Col3|Col4|Wik|kiTex|xt|Col8|Col9|Col10|\n|---|---|---|---|---|---|---|---|---|---|\n|5.8<br>6.0<br>6.2<br>6.4<br>Perplexity||||||||||\n|5.8<br>6.0<br>6.2<br>6.4<br>Perplexity||||||||||\n|5.8<br>6.0<br>6.2<br>6.4<br>Perplexity||||||||||\n|5.8<br>6.0<br>6.2<br>6.4<br>Perplexity||||||||||\n|5.8<br>6.0<br>6.2<br>6.4<br>Perplexity||||||||||\n\n**(b) Qwen3-80B-A3B**\n\n**Figure 4: Perplexity Impact of Varying Low-Precision Expert**\n**Ratios Across Layers in DynaExq (Qwen3-30B-A3B: FP16 vs.**\n**Int4, Qwen3-80B-A3B: Int4 vs. Int2)**\n\nWe first examine how varying the proportion of low-precision\nexperts affects language modeling quality. Using WikiText-2, we\ntest 128 prompts (2048 input tokens, 256 output tokens) while progressively increasing the number of demoted experts per layer. As\nFigure 4 shows, perplexity increases smoothly rather than abruptly,\nindicating that the model degrades predictably under more aggressive compression.\nCrucially, DynaExq\u2019s demotion policy prioritizes rarely activated\nexperts, yielding significantly flatter perplexity curves relative to\nuniform quantization. This demonstrates that activation-aware\nprecision assignment captures expert salience effectively and avoids\nunnecessary degradation on frequently used experts.\n\n**4.3** **Accuracy Comparison**\n\nTable 1 summarizes end-task accuracy across six benchmarks. For\nQwen3-MoE-30B, FP16 achieves 65.95% accuracy. Static INT4 reduces memory from 57 GB to 17 GB with modest accuracy loss.\nDynaExq closely matches FP16 accuracy on both GPUs (65.15% on\nA6000, 64.37% on RTX 5090), and retains robustness on challenging\ntasks such as HumanEval and MMLU-ProX.\n\nFor Qwen3-MoE-80B, full-precision inference is infeasible due to\nits 152 GB footprint, so we compare against INT4 (41 GB) and INT2\n(21 GB) baselines. While static INT2 compresses aggressively, it\nsuffers substantial degradation on reasoning-heavy evaluations. In\ncontrast, DynaExq attains 76.68% accuracy on A6000 and 73.88% on\nRTX-5090, consistently outperforming static INT2 and approaching\nINT4 performance, demonstrating that runtime precision alignment\nis particularly beneficial at larger scales.\nAcross both models, DynaExq reduces accuracy loss induced by\nstatic low-bit quantization while remaining deployable under tight\nHBM constraints, validating its ability to allocate precision where\nit most impacts task quality.\n\nKexin Chu, Dawei Xiang, Zixu Shen, Yiwei Yang, Zecheng Lin, and Wei Zhang\n\n**MODEL** **METHOD** **MMLU-Pro** **GPQA** **AIME25** **GSM8K** **Human Eval** **MMLU-ProX** **AVG.**\n\nQwen3-MoE-30B FP16 73.37 54.55 23.33 90.45 84.76 69.29 65.96\n\nInt4 72.84 53.54 20.00 89.39 84.15 68.39 64.72\n\nDynaExq(A6000) 73.09 54.14 20.00 89.91 84.76 68.96 **65.15**\nDynaExq(5090) 72.58 54.54 20.00 89.54 84.48 68.39 64.37\n\nQwen3-MoE-80B Int4 75.92 72.22 70.00 87.04 85.37 75.88 77.74\n\nInt2 72.75 66.67 63.33 80.97 81.71 70.49 72.65\n\nDynaExq(A6000) 75.48 71.21 66.67 86.73 84.76 75.22 **76.68**\nDynaExq(5090) 72.89 68.18 63.33 83.47 82.68 72.73 73.88\n\n**Table 1: The comparison of accuracy across model/method.**\n\n![](papers/images/Dynamic-Expert-Quantization-for-Scalable-MixtureofExperts-Inference.pdf-5-10.png)\n\n![](papers/images/Dynamic-Expert-Quantization-for-Scalable-MixtureofExperts-Inference.pdf-5-11.png)\n\n**(a) TTFT**\n\n![](papers/images/Dynamic-Expert-Quantization-for-Scalable-MixtureofExperts-Inference.pdf-5-33.png)\n\n**(b) TPOP**\n\n![](papers/images/Dynamic-Expert-Quantization-for-Scalable-MixtureofExperts-Inference.pdf-5-31.png)\n\n**(c) Prefill Throughput**\n\n**(d) Decode Throughput**\n\n**Figure 5: The comparison of latency/throughput across model/method.**\n\n**5.3** **Dynamic Quantization and Adaptive**\n**Execution**\n\nDynamic quantization methods (e.g., HAQ [29], AdaQuant [13],\nFlexRound [17]) select bit-widths based on hardware feedback or\nlayer sensitivity, and adaptive inference techniques such as early\nexiting adjust computation depth per input. However, these approaches target dense models, operate at coarse granularity, and\ndo not address expert-level activation skew in MoE architectures.\nSimilarly, MoE offloading systems adapt expert placement but\ntreat weights as static objects with fixed precision. In contrast, DynaExq performs _expert-level precision scheduling_ integrated into the\nserving runtime. By combining online hotness profiling, precision\ntransitions, and a fragmentation-free memory pool, DynaExq enables activation-aware, memory-efficient MoE inference on a single\nGPU."}, {"role": "assistant", "content": "**Verdict:** High-value, critical advancement for large-scale MoE model deployment.\n\n**Summary:** DynaExq delivers substantial utility by enabling the deployment of massive Mixture-of-Experts (MoE) models on more constrained hardware environments, directly translating to reduced operational costs and increased accessibility. It dynamically adjusts expert precision at runtime based on activation patterns, effectively treating bit-width as a managed resource rather than a static decision. This innovation significantly lowers the HBM footprint and transfer volume for MoE weights, especially for 'cold' (rarely activated) experts, while maintaining high accuracy for 'hot' experts. For large models (e.g., Qwen3-MoE-80B, which is infeasible in FP16), DynaExq achieves comparable accuracy to INT4 quantization while dramatically reducing memory requirements and outperforming static INT2 on reasoning tasks. It allows for the use of less expensive GPUs (e.g., A6000, RTX 5090) for models that would otherwise require much larger, more expensive setups, offering a compelling return on investment through optimized resource utilization and improved cost-efficiency in MoE inference serving."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n_each selected tree \ud835\udf0f_ **do**\n\n**6** _\ud835\udf0f_ [\u2032] \u2190 link-cut ( _\ud835\udf0f,_ ( _\ud835\udc62, \ud835\udc63_ ));\n\n**7** Update _\ud835\udc53_ ( _\ud835\udf0f_ [\u2032] );\n\n**8** Calculate weight as _\ud835\udc64_ ( _\ud835\udf0f_ [\u2032] ) = _\ud835\udc51_ _[\ud835\udc51]_ ( [(] _\ud835\udf0f_ _[\ud835\udf0f]_ ~~[\u2032]~~ [)] ) _[\ud835\udc64]_ [(] _[\ud835\udf0f]_ [)][;]"}, {"role": "assistant", "content": "**Verdict:** High utility for specific, dynamic graph problems, but with significant implementation overhead and complexity risk.\n\n**Summary:** This section outlines a critical process for dynamically modifying and evaluating tree structures. The 'link-cut' operation provides efficient (logarithmic time) dynamic connectivity and path management in tree networks, making it valuable for real-time applications such as network routing, resource allocation, or maintaining minimum spanning forests in evolving systems. However, its implementation is notoriously complex, leading to high development and debugging costs. While efficient in terms of computational complexity for large datasets, the constant factors and memory overhead can be substantial. The utility is compelling for applications where dynamic graph changes are frequent and performance is paramount, but the investment in specialized expertise and maintenance must be justified by the scale and criticality of the problem."}]}
{"messages": [{"role": "user", "content": "Give me an executive summary:\n**14 end for**\n\n**15** redundant **return** NULL ; /* either one or both subflows unavailable */\n\nThe theoretical aspects of _ReMP TCP_ packet scheduling algorithm are provided in 2.2.6.\n\n**B** **Congestion Control Algorithms**\n\n**Algorithm 7:** CUBIC (v2.2)\n\nInitialization:\n\n_\ud835\udc61\ud835\udc50\ud835\udc5d_ _ _\ud835\udc53\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc60\ud835\udc60_ \u2190\u2212 1 _, \ud835\udefd_ \u2190\u2212 0 _._ 2\n\n_\ud835\udc53\ud835\udc4e\ud835\udc60\ud835\udc61_ _ _\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc54\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc52_ \u2190\u2212 1 _, \ud835\udc36_ \u2190\u2212 0 _._ 4\n\n_\ud835\udc50\ud835\udc62\ud835\udc4f\ud835\udc56\ud835\udc50_ _ _\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc52\ud835\udc61_ ()\n\nOn each ACK:\n\n**begin**"}, {"role": "assistant", "content": "**Verdict:** Standard implementation details of a widely-adopted congestion control algorithm. No novel features or significant cost/speed implications presented in this snippet.\n\n**Summary:** This section outlines the initial setup and reactive trigger for CUBIC (v2.2), a foundational TCP congestion control algorithm. It details the initialization of key parameters like `tcp_friendliness`, `beta`, and `C`, and notes that the core logic is executed 'On each ACK'. The preceding `return NULL` is standard error handling. While essential for network stability, this description offers no new intellectual property, specialized hardware requirements, or specific performance enhancements beyond the known utility of CUBIC in managing network congestion efficiently."}]}
