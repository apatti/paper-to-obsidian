{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n**2.1** **Notations and Concepts**\n\nLet _\ud835\udc3a_ = ( _\ud835\udc49, \ud835\udc38_ ) be a simple, connected, undirected graph with\n_\ud835\udc5b_ = | _\ud835\udc49_ | nodes and _\ud835\udc5a_ = | _\ud835\udc38_ | edges. The adjacency matrix _\ud835\udc34_ of _\ud835\udc3a_ is a\n_\ud835\udc5b_ \u00d7 _\ud835\udc5b_ matrix whose ( _\ud835\udc56, \ud835\udc57_ ) entry is 1 if and only if edge ( _\ud835\udc56, \ud835\udc57_ ) \u2208 _\ud835\udc38_ and\n0, otherwise. The degree matrix _\ud835\udc37_ of _\ud835\udc3a_ is a diagonal matrix where\neach entry _\ud835\udc37\ud835\udc56\ud835\udc56_ = _\ud835\udc51\ud835\udc56_ = [\ufffd] _[\ud835\udc5b]_ _\ud835\udc57_ =1 _[\ud835\udc34][\ud835\udc56\ud835\udc57]_ [represents the degree of node] _[ \ud835\udc56]_ [.]\nA random walk on graph _\ud835\udc3a_ is a stochastic process. At each step,\nthe walker moves to a randomly chosen neighbor of the current\nnode. The transition probability is described by the matrix _\ud835\udc43_ =\n( _\ud835\udc5d\ud835\udc56\ud835\udc57_ ) = _\ud835\udc37_ [\u2212][1] _\ud835\udc34_, where _\ud835\udc5d\ud835\udc56\ud835\udc57_ is the probability of moving from node _\ud835\udc56_\nto node _\ud835\udc57_ . It is well known that the stationary distribution _\ud835\udf0b_ of a\n\nScalable and Provable Kemeny Constant Computation on Static and Dynamic Graphs: A 2-Forest Sampling Approach\n\nrandom walk on undirected graphs is proportional to node degrees,\ni.e., _\ud835\udf0b\ud835\udc56_ = _\ud835\udc51\ud835\udc56_ /2 _\ud835\udc5a_ . Table 1 lists the notations that are frequently used\nin this paper.\nThe hitting time _\ud835\udc3b_ ( _\ud835\udc56, \ud835\udc57_ ) is a fundamental measure in the study\nof random walks [11, 33], defined as the expected number of steps\nrequired to visit the node _\ud835\udc57_ for the first time, starting from a node\n_\ud835\udc56_ . For an arbitrarily fixed node _\ud835\udc56_, the expected hitting time from _\ud835\udc56_\nto any other node _\ud835\udc57_, where _\ud835\udc57_ is chosen according to the stationary\ndistribution, is a constant regardless of the choice of _\ud835\udc56_ . This constant\nis known as Kemeny constant [22], denoted as _\ud835\udf05_ ( _\ud835\udc3a_ ), and is given\nby _\ud835\udf05_ ( _\ud835\udc3a_ ) = [\ufffd] _\ud835\udc57_ _[\ud835\udf0b]_ _\ud835\udc57_ _[\ud835\udc3b]_ [(] _[\ud835\udc56, \ud835\udc57]_ [)][.]\nThe Kemeny constant can also be expressed in terms of the\npseudo-inverse of the normalized Laplacian matrix. The Laplacian matrix _\ud835\udc3f_ and the normalized Laplacian matrix L is defined as\n_\ud835\udc3f_ = _\ud835\udc37_ \u2212 _\ud835\udc34_ and L = _\ud835\udc37_ [\u2212][1][/][2] _\ud835\udc3f\ud835\udc37_ [\u2212][1][/][2] = _\ud835\udc3c_ \u2212 _\ud835\udc37_ [\u2212][1][/][2] _\ud835\udc34\ud835\udc37_ [\u2212][1][/][2], respectively.\nLet 0 = _\ud835\udf0e_ 1 \u2264\u00b7 \u00b7 \u00b7 \u2264 _\ud835\udf0e\ud835\udc5b_ be the eigenvalues of L, with the corresponding eigenvectors _\ud835\udc62_ 1 _,_ - \u00b7 \u00b7 _,\ud835\udc62\ud835\udc5b_ . The pseudo-inverse of L is defined as\nL [\u2020] = [\ufffd] _[\ud835\udc5b]_ _\ud835\udc56_ =2 _\ud835\udf0e_ 1 _\ud835\udc56_ _[\ud835\udc62][\ud835\udc56][\ud835\udc62]_ _\ud835\udc56_ [\u22a4][. According to [][33][], the Kemeny constant can be]\nrepresented as the trace of the pseudo-inverse of the normalized\nLaplacian matrix, i.e.,\n\n**Truncated Random Walk Based Methods.** DynamicMC [25]\nand RefinedMC [51] utilize truncated random walks to approximate\nKC. This kind of approach takes advantage of the relationship\nbetween KC and the transition matrix _\ud835\udc43_, that is,\n\n_\ud835\udf05_ ( _\ud835\udc3a_ ) = _\ud835\udc5b_ \u2212 1 +\n\n\u221e\n\u2211\ufe01[Tr( _\ud835\udc43_ _[\ud835\udc58]_ ) \u2212 1] _._ (2)\n\n_\ud835\udc58_ =1\n\nThe _\ud835\udc56_ -th diagonal entry of _\ud835\udc43_ _[\ud835\udc58]_ represents the transition probability\nfrom _\ud835\udc56_ to itself after _\ud835\udc58_ steps, which becomes negligible when _\ud835\udc58_ is\nlarge. Therefore, by choosing a sufficiently large truncation length\n_\ud835\udc58_ and estimating the diagonal entry of _\ud835\udc43_ _[\ud835\udc58]_ using truncated random walks initiated from each node, these methods can achieve a\nsmall approximation error. To improve performance, DynamicMC\nemploys GPU acceleration to parallelize walk simulations, while\nRefinedMC improves sampling efficiency by optimizing sample\nsize, truncation length, and the number of starting nodes. However,\nbecause of the lack of strong theoretical guarantees, the improvement effect of these optimizations is limited. As a result, these\nmethods has been proven less efficient compared to LERW-based\nmethods as shown in [51].\n\n**Loop-Erased Random Walk (LERW) Based Methods.** Recently,\nthe relationship between L [\u2212][1] with ( _\ud835\udc3c_ \u2212 _\ud835\udc43\ud835\udc63_ ) [\u2212][1] has been independently explored by [51] and [29] from the perspective of resistance\ndistance and the inverse of the Laplacian submatrix _\ud835\udc3f\ud835\udc63_ [\u2212][1][, respec-]\ntively. Specifically, KC can be expressed as:\n\n_\ud835\udf05_ ( _\ud835\udc3a_ ) = Tr(L [\u2020] ) = Tr( _\ud835\udc3c_ \u2212 _\ud835\udc43\ud835\udc63_ ) [\u2212][1] + [(][L][\u2020][)] _[\ud835\udc63\ud835\udc63]_ _._ (3)\n\n_\ud835\udf0b\ud835\udc63_\n\nBoth methods, ForestMC proposed in [51] and LEwalk proposed\nin [29], employ loop-erased random walks (LERWs) to approximate Tr( _\ud835\udc3c_ \u2212 _\ud835\udc43\ud835\udc63_ ) [\u2212][1], but differ in their computation of (L [\u2020] ) _\ud835\udc63\ud835\udc63_ / _\ud835\udf0b\ud835\udc63_ :\nForestMC [51] uses truncated random walks, while LEwalk [29]\nutilizes _\ud835\udc63_ -absorbed random walks. The LERW technique itself is\nwell-established, most notably forming the basis of Wilson\u2019s algorithm for uniform random spanning trees sampling [48].\nThe Wilson algorithm constructs a spanning tree through an\niterative process: it begins with a single-node tree and sequentially\nadds LERWs trajectory starting from remaining nodes in arbitrary\norder. Each LERW terminates upon hitting the current tree, and its\nacyclic path is incorporated into the tree. Crucially, the expected\ntotal length of these LERWs is equal to Tr( _\ud835\udc3c_ \u2212 _\ud835\udc43\ud835\udc63_ ) [\u2212][1], allowing an\nefficient approximation of the first term in Eq. (3). By leveraging\nLERWs, we can obtain diagonal-related information of the matrix\ninverse in nearly linear time, significantly improving efficiency\ncompared to performing _\ud835\udc5b_ independent random walk from each\nnode. As a result, this technique has been widely adopted in recent studies on single-source problems or tasks involving matrix\ndiagonals [4, 28, 30], offering a practical alternative to traditional\nrandom walk based methods.\n\nHowever, analyzing the variance of LERW-based methods remains challenging, primarily due to the unbounded length of looperased random walks. In the worst case, the number of steps can\ngrow arbitrarily large, causing such methods to fail on certain\ngraphs. Although ForestMC provides a theoretical guarantee by\nbounding the absolute error with high probability as the time complexity of algorithm is _\ud835\udc42_ ( _\ud835\udf16_ [\u2212][2] \u0394 [2] _\ud835\udc51\ud835\udc5a\ud835\udc4e\ud835\udc65_ [2][\u0394] [log][3] _[ \ud835\udc5b]_ [\u00b7][ Tr][(] _[\ud835\udc3c]_ [\u2212] _[\ud835\udc43][\ud835\udc63]_ [)] [\u2212][1][)][, the term]\n\n_\ud835\udf05_ ( _\ud835\udc3a_ ) = Tr(L [\u2020] ) =\n\n_\ud835\udc5b_\n\u2211\ufe01\n\n_\ud835\udc56_ =2\n\n1\n_._ (1)\n_\ud835\udf0e\ud835\udc56_\n\nFrom Eq. (1), we see that the Kemeny constant can be computed\nexactly by finding the eigenvectors of the normalized Laplacian matrix, an _\ud835\udc42_ ( _\ud835\udc5b_ [3] ) operation. While theoretically sound, this approach\nbecomes prohibitively expensive for large-scale graphs. Moreover,\nreal-world networks often evolve dynamically, making the challenge of efficiently tracking the Kemeny constant over time even\nmore acute. To address these limitations, we aim to develop scalable\nalgorithms to approximate the Kemeny constant on both static and\nevolving graphs. Below, we provide a concise overview of the SOTA\nmethods for approximating KC on large graphs.\n\n**2.2** **Existing** SOTA **Methods and Their Defects**\n\nIn this section, we provide an overview of algorithms for computing\nKC, which can be broadly categorized into three classes: matrixrelated methods, truncated random walk-based methods, and looperased random walk-based methods. For each category, we briefly\ndiscuss their underlying principles and limitations.\n\n**Matrix Related Methods.** As shown in Eq. (1), KC can be expressed by eigenvalues of the normalized Laplacian matrix L. A\nnaive solution is solving eigenvalues in _\ud835\udc42_ ( _\ud835\udc5b_ [3] ) time, which is impractical for large-scale graphs. Xu et al. [52] proposed ApproxKemeny\nbased on Hutchinson\u2019s Monte Carlo [21]. It approximates the trace\n\n_\ud835\udc40_\n\nof L [\u2020], and thus KC as: _\ud835\udf05_ ( _\ud835\udc3a_ ) = Tr(L [\u2020] ) \u2248 _\ud835\udc40_ [1] \ufffd _\ud835\udc56_ =1 _[\ud835\udc65]_ _\ud835\udc56_ [\u22a4][L][\u2020] _[\ud835\udc65][\ud835\udc56][,]_ [ where] _[ \ud835\udc65][\ud835\udc56]_\n\nare Rademacher random vectors, with each entry independently\ntaking a value of 1 or \u22121 with equal probability. To compute the\nquadratic forms of L [\u2020], ApproxKemeny transforms the problem\ninto solving Laplacian linear systems: _\ud835\udc65\ud835\udc56_ [\u22a4][L][\u2020] _[\ud835\udc65][\ud835\udc56]_ [=][ \u2225] _[\ud835\udc35\ud835\udc3f]_ [\u2020] _[\ud835\udc66][\ud835\udc56]_ [\u2225][2][, where]\n_\ud835\udc66\ud835\udc56_ = _\ud835\udc37_ [1][/][2] ( _\ud835\udc3c_ \u2212 2 _\ud835\udc5a_ 1 _[\ud835\udc37]_ [1][/][2][11][\u22a4] _[\ud835\udc37]_ [1][/][2][)][. By using a Laplacian Solver to com-]\npute _\ud835\udc3f_ [\u2020] _\ud835\udc66\ud835\udc56_, KC can be approximated. However, the efficiency and\naccuracy of ApproxKemeny are limited by the performance of the\nspecific Laplacian solver used.\n\n_\ud835\udc51\ud835\udc5a\ud835\udc4e\ud835\udc65_ [2][\u0394] [makes it impractical for most real-world graphs. There is still]\nno good theoretical guarantee for this type of method. Moreover, all\naforementioned methods are designed for static graphs and cannot\nhandle dynamic updates efficiently. Even minor modifications to\nthe graph require complete recomputation, significantly limiting\ntheir applicability in evolving graph scenarios.\n\n**3** KC **ESTIMATION ALGORITHM**\n\nIn this section, we propose a novel sampling-based algorithm for\napproximating the Kemeny constant. We begin by a new formulation of KC based on 2-forests. Next, we introduce a technique\nthat can maps spanning trees to 2-forests, and we leverage this\nto design an unbiased estimator for KC. Based on this framework,\nwe develop a sampling algorithm that estimates KC via sampling\nuniform random spanning trees (UST), and further optimize it using\na Binary Index Tree and depth-first search. The correctness and\ncomplexity analysis for proposed algorithm are also discussed.\n\n**3.1** **New Forest Formula of** KC\n\nFirst, we derive a new expression for KC using 2-forests. Given two\nfixed nodes _\ud835\udc62_ and _\ud835\udc63_, we consider the set of 2-forests in which _\ud835\udc62_ and\n_\ud835\udc63_ belong to different trees, denoted as F _\ud835\udc62_ | _\ud835\udc63_ . Utilizing _\ud835\udc5b_ \u2212 1 such sets\nF _\ud835\udc62_ | _\ud835\udc63_, we present a new formula for computing KC.\n\nTheorem 3.1 (Forest Formula of KC).\n\n1\n_\ud835\udf05_ ( _\ud835\udc3a_ ) =\n2 _\ud835\udc5a_ |\u0393|\n\n\u2211\ufe01 _\ud835\udc51_ ( _\ud835\udc62_ ) \u2211\ufe01\n\n_\ud835\udc62_ \u2260 _\ud835\udc5f_ _\ud835\udc47_ \u222a _\ud835\udc47_\n\n_\ud835\udc47_ 1\u222a _\ud835\udc47_ 2 \u2208F _\ud835\udc5f_ | _\ud835\udc62_\n_\ud835\udc5f_ \u2208 _\ud835\udc47_ 1\n\n\u2211\ufe01\n\nvol( _\ud835\udc47_ 1) _,_ (4)\n\nCheng Li, Meihao Liao, Rong-Hua Li, Guoren Wang\n\n_For any given 2-forest, we denote the node set of the tree containing_\n_the root \ud835\udc5f_ _as \ud835\udc47_ 1 _, and the other as \ud835\udc47_ 2 _. For example, in_ F1 _, we have_\n_\ud835\udc47_ 1 = { _\ud835\udc63_ 1} _and \ud835\udc47_ 2 = { _\ud835\udc63_ 2 _, \ud835\udc63_ 3} _. Hence,_ vol( _\ud835\udc47_ 1) = _\ud835\udc51_ ( _\ud835\udc63_ 1) = 2 _. Note that_\n_this degree refers to \ud835\udc63_ 1 _\u2019s degree in the graph \ud835\udc3a, not in the forest._\n\nTheorem 3.1 expresses KC in terms of _\ud835\udc5b_ \u2212 1 specific subsets of\nF, denoted as F _\ud835\udc5f_ | _\ud835\udc62_ . To design an unbiased estimator based on this\nformulation, it is crucial to establish a relationship between spanning trees and F _\ud835\udc5f_ | _\ud835\udc62_, as the normalization factor in Eq. (4) involves \u0393.\nTo bridge this gap, we introduce a technique called _path mapping_,\nwhich constructs a special correspondence between spanning trees\nand 2-forests by fixing a path.\nAn interesting observation about the 2-forests in F _\ud835\udc5f_ | _\ud835\udc62_ is that\nthey can be obtained from a spanning tree by deleting any edge\nalong the path between _\ud835\udc62_ to _\ud835\udc63_, thereby ensuring that _\ud835\udc62_ and _\ud835\udc63_ lie in\ndifferent components. Consequently, a single spanning tree yields\n| _\ud835\udc43\ud835\udc62_ \u2192 _\ud835\udc63_ | valid 2-forests, where | _\ud835\udc43\ud835\udc62_ \u2192 _\ud835\udc63_ | is the number of edges on the\n_\ud835\udc62_ \u2013 _\ud835\udc63_ path.\nHowever, the sets of 2-forests generated from different spanning\ntrees overlap. For example, in Fig. 1, both _\ud835\udf0f_ 1 and _\ud835\udf0f_ 2 can produce\nF1 by deleting one edge. This redundancy prevents a direct correspondence between F _\ud835\udc5f_ | _\ud835\udc62_ and \u0393. To resolve this issue, we propose a\nnovel key technique called Path Mapping which can systematically\navoids such overlaps. The formal definition of Path Mapping is\nprovided below.\n\n_Definition 3.2 (Path Mapping)._ Let P be a simple path from node\n_\ud835\udc62_ to node _\ud835\udc5f_ in the graph. We define _path mapping_ that associates\neach spanning tree _\ud835\udf0f_ \u2208 \u0393 with a subset of F _\ud835\udc5f_ | _\ud835\udc62_, i.e., P : \u0393 \u21a6\u2192 2 [F] _[\ud835\udc5f]_ [|] _[\ud835\udc62]_,\nwhere 2 _[\ud835\udc46]_ denotes the power set of _\ud835\udc46_ . For any spanning tree _\ud835\udf0f_,\nthere exists a unique path between _\ud835\udc62_ and _\ud835\udc5f_, denoted by P _\ud835\udc62_ [(] _[\ud835\udf0f]_ \u2192 [)] _\ud835\udc5f_ [. By]\nremoving all edges shared by both P and P _\ud835\udc62_ [(] _[\ud835\udf0f]_ \u2192 [)] _\ud835\udc5f_ [, we obtain a set of]\n2-forests. Depending on the relative direction of the overlapping\nedges, we define two types of path mappings:\n\n- _Forward Path Mapping_ : if the removed edge has same direction\nin two path, the obtained 2-forest set is denoted as F _[\ud835\udc43]_ ( _\ud835\udf0f_ ), i.e.,\n\n_\ud835\udf0f_ )\nF [P] ( _\ud835\udf0f_ ) = \ufffd _\ud835\udf0f_ \\ ( _\ud835\udc56, \ud835\udc57_ ) \ufffd\ufffd ( _\ud835\udc56, \ud835\udc57_ ) \u2208P \u2227( _\ud835\udc56, \ud835\udc57_ ) \u2208P _\ud835\udc62_ (\u2192 _\ud835\udc5f_ \ufffd _,_\n\n- _Reverse Path Mapping_ : if the removed edge has opposite direction\nin two path, the obtained 2-forest set is denoted as F _[\ud835\udc43]_ [\u2032] ( _\ud835\udf0f_ ), i.e.,\n\n_\ud835\udf0f_ )\nF [P][\u2032] ( _\ud835\udf0f_ ) = \ufffd _\ud835\udf0f_ \\ ( _\ud835\udc56, \ud835\udc57_ ) \ufffd\ufffd ( _\ud835\udc56, \ud835\udc57_ )) \u2208P \u2227( _\ud835\udc57,\ud835\udc56_ ) \u2208P _\ud835\udc62_ (\u2192 _\ud835\udc5f_ \ufffd _._\n\nExample 2. _Fig. 3 illustrates an example of path mapping. To_\n_convert a spanning tree into forests in_ F _\ud835\udc63_ 2 | _\ud835\udc63_ 1 _, we first identify a simple_\n_path from \ud835\udc63_ 2 _to \ud835\udc63_ 1 _, such as_ P _\ud835\udc63_ 2 : _\ud835\udc63_ 2 \u2192 _\ud835\udc63_ 3 \u2192 _\ud835\udc63_ 1 _. Then, for a given_\n_spanning tree, e.g., \ud835\udf0f_ 1 _, we locate the path from \ud835\udc63_ 2 _to \ud835\udc63_ 1 _within \ud835\udf0f_ 1 _, that_\n_is_ P _\ud835\udc63_ [(] 2 _[\ud835\udf0f]_ \u2192 [1] [)] _\ud835\udc63_ 1 [:] _[ \ud835\udc63]_ 2 [\u2192] _[\ud835\udc63]_ 3 [\u2192] _[\ud835\udc63]_ 1 _[. We observe that two edges in]_ [ P] _\ud835\udc63_ [ (] 2 _[\ud835\udf0f]_ \u2192 [1] [)] _\ud835\udc63_ 1\n_also appear in_ P _\ud835\udc63_ 2 _and have the same direction. We can obtain two_\n_2-forests,_ F3 _and_ F1 _, by removing_ ( _\ud835\udc63_ 2 _, \ud835\udc63_ 3) _and_ ( _\ud835\udc63_ 3 _, \ud835\udc63_ 1) _respectively._\n_As for \ud835\udf0f_ 3 _, the path from \ud835\udc63_ 2 _to \ud835\udc63_ 1 _is_ P _\ud835\udc63_ [(] 2 _[\ud835\udf0f]_ \u2192 [3] [)] _\ud835\udc63_ 1 [:] _[ \ud835\udc63]_ 2 [\u2192] _[\ud835\udc63]_ 1 _[. Although the]_\n_edge_ ( _\ud835\udc63_ 1 _, \ud835\udc63_ 3) _also exists in_ P _\ud835\udc63_ 2 _and \ud835\udf0f_ 3 _, it is not part of_ P _\ud835\udc63_ [(] 2 _[\ud835\udf0f]_ \u2192 [3] [)] _\ud835\udc63_ 1 _[, and]_\n_thus cannot produce a 2-forest. Therefore, applying path mapping_\n_with_ P _\ud835\udc63_ 2 _to \ud835\udf0f_ 3 _results in an empty set._\n\nWith Path Mapping, we restrict the deletable edges of a spanning\ntree to a specific path to reduce redundancy, yet this remains insufficient. To further address this, we distinguish two cases depending\n\n_where_ \u0393 _denotes the set of all spanning trees of \ud835\udc3a, \ud835\udc5f_ _is an arbitrarily_\n_fixed node,_ vol( _\ud835\udc47_ 1) = [\ufffd] _\ud835\udc62_ \u2208 _\ud835\udc47_ 1 _[\ud835\udc51]_ [(] _[\ud835\udc62]_ [)] _[ represents the volume of the tree]_\n_containing \ud835\udc5f_ _, and_ F _\ud835\udc5f_ | _\ud835\udc62_ _is the set of 2-forests in which \ud835\udc5f_ _and \ud835\udc62_ _belong_\n_to different trees._\n\nProof.\n\n1\n_\ud835\udf05_ ( _\ud835\udc3a_ ) =\n2 _\ud835\udc5a_ |\u0393|\n\n1\n\n=\n2 _\ud835\udc5a_ |\u0393|\n\n1\n\n=\n2 _\ud835\udc5a_ |\u0393|\n\n1\n\n=\n2 _\ud835\udc5a_ |\u0393|\n\n\u2211\ufe01 _\ud835\udc51_ ( _\ud835\udc62_ ) \u2211\ufe01\n\n_\ud835\udc62_ \u2260 _\ud835\udc5f_ _\ud835\udc47_ \u222a _\ud835\udc47_\n\n_\ud835\udc47_ 1\u222a _\ud835\udc47_ 2 \u2208F _\ud835\udc5f_ | _\ud835\udc62_\n_\ud835\udc5f_ \u2208 _\ud835\udc47_ 1\n\n\u2211\ufe01 vol( _\ud835\udc47_ 1)vol( _\ud835\udc47_ 2)\n\n_\ud835\udc47_ 1\u222a _\ud835\udc47_ 2 \u2208F\n\n\u2211\ufe01\n\n_\ud835\udc47_ 1\u222a _\ud835\udc47_ 2 \u2208F\n_\ud835\udc5f_ \u2208 _\ud835\udc47_ 1\n\nvol( _\ud835\udc47_ 1) \u2211\ufe01 _\ud835\udc51_ ( _\ud835\udc62_ )\n\n_\ud835\udc62_ \u2208 _\ud835\udc47_ 2\n\n\u2211\ufe01\n\n_\ud835\udc62_ \u2260 _\ud835\udc5f_\n\n\u2211\ufe01\n\n_\ud835\udc47_ 1\u222a _\ud835\udc47_ 2 \u2208F\n_\ud835\udc5f_ \u2208 _\ud835\udc47_ 1 _,\ud835\udc62_ \u2208 _\ud835\udc47_ 2\n\nvol( _\ud835\udc47_ 1) _\ud835\udc51_ ( _\ud835\udc62_ )\n\n\u2211\ufe01\n\nvol( _\ud835\udc47_ 1) _._\n\nThe first equality follows from Corollary 1.7 in [10], which expresses\nKC in terms of all 2-forests, where F denotes the set of all 2-forests\nin _\ud835\udc3a_ . Note that for any given 2-forest _\ud835\udc47_ 1 \u222a _\ud835\udc47_ 2, the designation of\nwhich tree is _\ud835\udc47_ 1 or _\ud835\udc47_ 2 does not affect anything. Therefore, we deduce\nthe second equation by simply treating the tree containing _\ud835\udc5f_ as _\ud835\udc47_ 1,\nand utilizing the definition of vol( _\ud835\udc47_ ). \n\nExample 1. _As shown in Fig. 1, (a) illustrates a graph \ud835\udc3a, (b) dis-_\n_plays all its spanning trees, and (c) shows all its 2-forests. Assume_\n_node \ud835\udc63_ 1 _is selected as \ud835\udc5f_ _. Then,_ F _\ud835\udc63_ 2 | _\ud835\udc63_ 1 = {F1 _,_ F3} _and_ F _\ud835\udc63_ 3 | _\ud835\udc63_ 1 = { _\ud835\udc39_ 1 _, \ud835\udc39_ 2} _._\n\nScalable and Provable Kemeny Constant Computation on Static and Dynamic Graphs: A 2-Forest Sampling Approach\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-4-1.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-4-2.png)\n\n\ud835\udc6e\ud835\udc6e\n\n(a) Graph G (b) \u0393 = \ud835\udf0f\ud835\udf0f1, \ud835\udf0f\ud835\udf0f2, \ud835\udf0f\ud835\udf0f3 \u2236\ud835\udc60\ud835\udc60et of all spanning trees in G (c) \ud835\udd3d\ud835\udd3d= \u21311, \u21312, \u21313 \u2236 set of all 2-forests in G\n\n**Figure 1: An example graph, its spanning trees and 2-forests**\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-4-5.png)\n\n\ud835\udd3d\ud835\udd3d\ud835\udc63\ud835\udc631|\ud835\udc63\ud835\udc632\n\n\ud835\udcab\ud835\udcab\ud835\udc63\ud835\udc632: \ud835\udc63\ud835\udc632 \u2192\ud835\udc63\ud835\udc633 \u2192\ud835\udc63\ud835\udc631 \ud835\udf0f\ud835\udf0f1\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-4-11.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-4-6.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-4-7.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-4-8.png)\n\n**Figure 2: Proof of Lemma 3.5**\n\non whether the deleted edge aligns with the path orientation, introducing forward and reverse Path Mapping. This refinement further\neliminates redundancy and ultimately enables our goal: establishing a precise correspondence between \u0393 and F _\ud835\udc5f_ | _\ud835\udc62_ through Path\nMapping.\nThe following theorem formalizes this connection by showing\nthat, given any simple path from _\ud835\udc62_ to _\ud835\udc5f_, the set of 2-forests obtained\nfrom all spanning trees in \u0393 via forward Path Mapping, minus those\nobtained via reverse Path Mapping, is exactly F _\ud835\udc5f_ | _\ud835\udc62_ .\n\nTheorem 3.3. _Let_ P _be a simple path from \ud835\udc62_ _to \ud835\udc5f_ _, then_\n\n\ud835\udcab\ud835\udcab\ud835\udc63\ud835\udc633: \ud835\udc63\ud835\udc633 \u2192\ud835\udc63\ud835\udc631\n\nF _\ud835\udc5f_ | _\ud835\udc62_ = \u2211\ufe01\n\n_\ud835\udf0f_ \u2208\u0393\n\nF [P] ( _\ud835\udf0f_ ) \u2212 F [P][\u2032] ( _\ud835\udf0f_ ) _._\n\ufffd \ufffd\n\nProof Sketch. The forward path mapping covers all of F _\ud835\udc5f_ | _\ud835\udc62_,\nsince there must be an edge in P connecting two trees in any 2forests. Moreover, for each 2-forest, forward mapping produces\nexactly one more instance than reverse mapping, as P contains\nprecisely one additional edge from the tree of _\ud835\udc62_ to that of _\ud835\udc5f_ . \n\nLemma 3.4. _Given an arbitrary simple path_ P _from \ud835\udc62_ _to \ud835\udc5f_ _, the_\n_following equation holds:_\n\ufffd F [P] ( _\ud835\udf0f_ ) = F _\ud835\udc5f_ | _\ud835\udc62_ _._\n\n_\ud835\udf0f_ \u2208\u0393\n\nProof. The proof idea is to prove that for any 2-forest, there\nmust be a spanning tree that can be mapped to it by a feasible path.\nFor each 2-forest _\ud835\udc47_ 1 \u222a _\ud835\udc47_ 2 \u2208 F _\ud835\udc5f_ | _\ud835\udc62_, the node set _\ud835\udc49_ is divided into two\ndistinct sets, _\ud835\udc47_ 1 and _\ud835\udc47_ 2, and we assume that the tree contains _\ud835\udc5f_ is _\ud835\udc47_ 1.\nSince _\ud835\udc43_ starts from _\ud835\udc62_ and ends at _\ud835\udc5f_, there must be at least one edge\n_\ud835\udc52_ in _\ud835\udc43_ which leaves from the node in _\ud835\udc47_ 2 and enters to the node in\n_\ud835\udc47_ 1. Otherwise, _\ud835\udc62_ and _\ud835\udc5f_ are in the same one connected component,\nor path _\ud835\udc43_ fails to connect _\ud835\udc62_ and _\ud835\udc5f_ . Adding the edge accrossing two\ncomponents to the 2-forest can get a spanning tree _\ud835\udc47_ 1 \u222a _\ud835\udc47_ 2 \u222a _\ud835\udc52_, which\nmeans this 2-forest _\ud835\udc47_ 1 \u222a _\ud835\udc47_ 2 is an element of F [P] ( _\ud835\udc47_ 1 \u222a _\ud835\udc47_ 2 \u222a _\ud835\udc52_ ). \n\nLemma 3.5. _For any 2-forest \ud835\udc47_ 1 \u222a _\ud835\udc47_ 2 \u2208 F _\ud835\udc5f_ | _\ud835\udc62_ _, we have_\n\ufffd\ufffd{ _\ud835\udf0f_ | _\ud835\udc47_ 1 \u222a _\ud835\udc47_ 2 \u2208 FP ( _\ud835\udf0f_ )}\ufffd\ufffd \u2212 \ufffd\ufffd\ufffd{ _\ud835\udf0f_ | _\ud835\udc47_ 1 \u222a _\ud835\udc47_ 2 \u2208 FP\u2032 ( _\ud835\udf0f_ )}\ufffd\ufffd\ufffd = 1 _._\n\n(a) \ud835\udcab\ud835\udcab\ud835\udc63\ud835\udc63\ud835\udc56\ud835\udc56 \u2236 path from \ud835\udc63\ud835\udc63\ud835\udc56\ud835\udc56 to \ud835\udc63\ud835\udc631 (b) Mapping \u0393 to \ud835\udd3d\ud835\udd3d\ud835\udc63\ud835\udc631|\ud835\udc63\ud835\udc632(\ud835\udd3d\ud835\udd3d\ud835\udc63\ud835\udc631|\ud835\udc63\ud835\udc633) by using \ud835\udcab\ud835\udcab\ud835\udc63\ud835\udc632 (\ud835\udcab\ud835\udcab\ud835\udc63\ud835\udc633)\n\n**Figure 3: Illustration of Transforming Spanning Trees to 2-**\n**Forests Using Path Mapping**\n\nProof. for each 2-forest _\ud835\udc47_ 1 \u222a _\ud835\udc47_ 2 \u2208 F _\ud835\udc5f_ | _\ud835\udc62_, we assume _\ud835\udc5f_ is in _\ud835\udc47_ 1. As\nproved in Lemma 3.4, if a spanning tree _\ud835\udf0f_ is composed of _\ud835\udc47_ 1 \u222a _\ud835\udc47_ 2\nand an edge _\ud835\udc52_ in P, which leaves from _\ud835\udc47_ 2 and enters to _\ud835\udc47_ 1 (as the\nred edges in Fig. 2). Then _\ud835\udc47_ 1 \u222a _\ud835\udc47_ 2 appears in the set obtained by\npath mapping _\ud835\udf0f_, i.e., _\ud835\udc47_ 1 \u222a _\ud835\udc47_ 2 \u2208 F [P] ( _\ud835\udc47_ 1 \u222a _\ud835\udc47_ 2 \u222a( _\ud835\udc52_ 1 _,\ud835\udc52_ 2)), for each\n( _\ud835\udc52_ 1 _,\ud835\udc52_ 2) \u2208P, satisfying _\ud835\udc52_ 1 \u2208 _\ud835\udc47_ 2 _,\ud835\udc52_ 2 \u2208 _\ud835\udc47_ 1. Similarly F [P][\u2032] ( _\ud835\udf0f_ [\u2032] ) has _\ud835\udc47_ 1 \u222a _\ud835\udc47_ 2\nif _\ud835\udf0f_ [\u2032] is composed of _\ud835\udc47_ 1 \u222a _\ud835\udc47_ 2 and the edge that is pointed to _\ud835\udc47_ 2 from\n_\ud835\udc47_ 1 (see the blue edge in Fig. 2). Therefore\n\ufffd\ufffd{ _\ud835\udf0f_ | _\ud835\udc47_ 1 \u222a _\ud835\udc47_ 2 \u2208 FP ( _\ud835\udf0f_ )}\ufffd\ufffd \u2212 \ufffd\ufffd\ufffd{ _\ud835\udf0f_ | _\ud835\udc47_ 1 \u222a _\ud835\udc47_ 2 \u2208 FP\u2032 ( _\ud835\udf0f_ )}\ufffd\ufffd\ufffd\n\n= |( _\ud835\udc52_ 1 _,\ud835\udc52_ 2) \u2208P | _\ud835\udc52_ 1 \u2208 _\ud835\udc47_ 2 \u2227 _\ud835\udc52_ 2 \u2208 _\ud835\udc47_ 1|\n\n\u2212|( _\ud835\udc52_ 1 _,\ud835\udc52_ 2) \u2208P | _\ud835\udc52_ 1 \u2208 _\ud835\udc47_ 1 \u2227 _\ud835\udc52_ 2 \u2208 _\ud835\udc47_ 2|\n\n=1 _._\n\nThe last equality holds because P is a simple path from _\ud835\udc62_ \u2208 _\ud835\udc47_ 2 to\n_\ud835\udc5f_ \u2208 _\ud835\udc47_ 1, which makes the edges from _\ud835\udc47_ 2 to _\ud835\udc47_ 1 exactly one more than\nthe edge from _\ud835\udc47_ 1 to _\ud835\udc47_ 2 in P. \n\nProof of Theorem 3.3. According to Lemma 3.4 and Lemma 3.5,\nwe can know that [\ufffd] _\ud835\udf0f_ \u2208\u0393 [F][P][ (] _[\ud835\udf0f]_ [)][ includes all elements in][ F] _\ud835\udc5f_ | _\ud835\udc62_ [, while]\n\n\u2212 [\ufffd]\n_\ud835\udf0f_ \u2208\u0393 [F][P][\u2032][ (] _[\ud835\udf0f]_ [)][ eliminates the excess. Therefore, the right term of]\nequation precisely matches F _\ud835\udc5f_ | _\ud835\udc62_ . \n\nExample 3. _Fig. 3 illustrates the mapping from the spanning tree_\n_set_ \u0393 _to the 2-forest set_ F _\ud835\udc63_ | _\ud835\udc5f_ _. By selecting a root node and fixing a_\n_path from another node to the root, each spanning tree in_ \u0393 _can be_\n_mapped to one or more 2-forests in_ F _\ud835\udc63_ | _\ud835\udc5f_ _by removing edges along_\n_the path. For instance, in \ud835\udf0f_ 1 _, removing edges along the fixed path_\n_generates_ F1 _and_ F3 _, while \ud835\udf0f_ 2 _and \ud835\udf0f_ 3 _yield no mappings. Combining_\n_the mappings from all trees exactly recovers_ F _\ud835\udc63_ 1 | _\ud835\udc63_ 2 _, ensuring that_\n_sampling a tree uniformly from_ \u0393 _induces a uniform distribution over_\n_the corresponding 2-forests._\n\nCheng Li, Meihao Liao, Rong-Hua Li, Guoren Wang\n\n**Discussion.** In this way, we successfully establish a connection\nbetween \u0393 and F _\ud835\udc5f_ | _\ud835\udc62_ through path mapping, which lays the foundation for designing a feasible sampling algorithm. Previously, it\nwas difficult to directly sample 2-forests. Now, we can first sample uniform random spanning trees (USTs) and then apply path\nmapping to transform each spanning tree sample into multiple\n(or possibly zero) 2-forests. As a result, each sampled 2-forest is\nobtained with probability 1/|\u0393|. In the next section, we will present\nthe sampling algorithm in detail and provide the corresponding\ntheoretical analysis.\n\n**3.2** **The Tree-To-Forest Algorithm**\n\nIn this section, we introduce TTF (Tree-To-Forest), a new samplingbased algorithm for estimating the Kemeny constant. The key idea is\nto first sample USTs and then transform UST samples into 2-forests\nby path mapping, which allow us to estimate KC according to Eq. (4).\nWe begin by outlining the algorithmic intuition and establish its\ncorrectness, and then present the pseudo-code, implementation\ndetails, and complexity analysis.\n\n**High-Level Idea of** TTF **. (1) Identify the root and paths.** We\nselect an arbitrary node _\ud835\udc5f_ as the root and construct a breadth-first\nsearch (BFS) tree rooted at _\ud835\udc5f_ . This BFS tree provides a set of simple\npaths P _\ud835\udc62_ from each node _\ud835\udc62_ to _\ud835\udc5f_ . **(2) Sample USTs.** We efficiently\nsample USTs using Wilson\u2019s algorithm [48], which form the basis of\nour estimator. **(3) From tree to forest.** Each sampled UST is then\nconverted into 2-forests using forward and reverse path mapping\nwith {P _\ud835\udc62_ | _\ud835\udc62_ \u2260 _\ud835\udc5f_ }. **(4) Estimate** KC **.** Finally, we estimate KC using\nEq. (4) based on the obtained 2-forests.\nThe core of our algorithm lies in steps (3) and (4). Importantly,\nwe do not need to explicitly transform sampled USTs into 2-forests.\nInstead, it suffices to compute the contributions that these 2-forests\nwould make to KC. Therefore, we merge the two steps and directly\nestimate KC from the sampled USTs. We propose two approaches\nfor this task: a naive method and an optimized method.\n\n**Traverse Each Path (Naive Method).** For each sampled UST _\ud835\udf0f_, we\ntraverse the path from each node _\ud835\udc62_ to the root _\ud835\udc5f_ within the sampled\ntree, and compare its edges with the pre-determined path P _\ud835\udc62_ from\nstep (1). Each matching edge with same orientation indicates a valid\n2-forest in F [P] _[\ud835\udc62]_ ( _\ud835\udf0f_ ) (or F [P] _[\ud835\udc62]_ [\u2032] ( _\ud835\udf0f_ ) when edge has opposite orientation).\nFor each valid 2-forest, we accumulate the volume of the component\ncontaining _\ud835\udc5f_, i.e. vol( _\ud835\udc47_ 1), and multiply it by _\ud835\udc51_ ( _\ud835\udc62_ ) to contribute to\nthe KC estimate. Repeating this process for all nodes _\ud835\udc62_ \u2260 _\ud835\udc5f_ yields\nthe contribution of _\ud835\udf0f_, denoted by _\ud835\udc53_ ( _\ud835\udf0f_ )\n\nTheorem 3.6 (Correctness of Naive Method). _Given \ud835\udf14_ _UST_\n_samples and a fixed root \ud835\udc5f_ _with paths_ {P _\ud835\udc62_ | _\ud835\udc62_ \u2260 _\ud835\udc5f_ } _. Let_ \u02c6 _\ud835\udf0f\ud835\udc56_ _denote the_\n_\ud835\udc56-th sampled UST. Then,_ \u02dc _\ud835\udf05_ = 2 _\ud835\udc5a\ud835\udf14_ 1 \ufffd _\ud835\udc56\ud835\udf14_ =1 _[\ud835\udc53]_ [(] _[\ud835\udf0f]_ [ \u02c6] _[\ud835\udc56]_ [)] _[ is an unbiased estimator]_\n_of \ud835\udf05_ ( _\ud835\udc3a_ ) _, where_\n\nas:\n\n1\n_\ud835\udf05_ ( _\ud835\udc3a_ ) =\n2 _\ud835\udc5a_ |\u0393|\n\n1\n\n=\n2 _\ud835\udc5a_ |\u0393|\n\n1\n\n=\n2 _\ud835\udc5a_ |\u0393|\n\n1\n\n=\n2 _\ud835\udc5a_ |\u0393|\n\n\u2211\ufe01 _\ud835\udc53_ ( _\ud835\udf0f_ ) _._\n\n_\ud835\udf0f_ \u2208\u0393\n\n\u2211\ufe01\n\n\u2211\ufe01 _\ud835\udc51_ ( _\ud835\udc62_ ) \u2211\ufe01\n\n_\ud835\udc62_ \u2260 _\ud835\udc5f_ _\ud835\udc47_ \u222a _\ud835\udc47_ \u2208F\n\nvol( _\ud835\udc47_ 1)\n\n_\ud835\udc47_ 1\u222a _\ud835\udc47_ 2 \u2208F _\ud835\udc5f_ | _\ud835\udc62,\ud835\udc5f_ \u2208 _\ud835\udc47_ 1\n\n\u2211\ufe01\n\n_\ud835\udc47_ 1\u222a _\ud835\udc47_ 2 \u2208F [P ] ( _\ud835\udf0f_ )\n_\ud835\udc5f_ \u2208 _\ud835\udc47_ 1\n\n\u2211\ufe01\n\n_\ud835\udc47_ 1\u222a _\ud835\udc47_ 2 \u2208F [P ] ( _\ud835\udf0f_ )\n_\ud835\udc5f_ \u2208 _\ud835\udc47_ 1\n\n\u2211\ufe01\n\n\u2211\ufe01 _\ud835\udc51_ ( _\ud835\udc62_ ) \u2211\ufe01\n\n_\ud835\udc62_ \u2260 _\ud835\udc5f_ _\ud835\udf0f_ \u2208\u0393\n\nvol( _\ud835\udc47_ 1) \u2212 \u2211\ufe01\n\n_\ud835\udc47_ 1\u222a _\ud835\udc47_ 2 \u2208F [P\u2032 ] ( _\ud835\udf0f_ )\n_\ud835\udc5f_ \u2208 _\ud835\udc47_ 1\n\nvol( _\ud835\udc47_ 1) \u2212 \u2211\ufe01\n\n_\ud835\udc47_ 1\u222a _\ud835\udc47_ 2 \u2208F [P\u2032 ] ( _\ud835\udf0f_ )\n_\ud835\udc5f_ \u2208 _\ud835\udc47_ 1\n\nvol( _\ud835\udc47_ 1)\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\n\nvol( _\ud835\udc47_ 1)\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\n\n  \n\n_\ud835\udf0f_ \u2208\u0393\n\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\n\n\u2211\ufe01\n\n_\ud835\udf0f_ \u2208\u0393\n\n\u2211\ufe01 _\ud835\udc51_ ( _\ud835\udc62_ ) \ufffd\ufffd\ufffd\ufffd\n\n_\ud835\udc62_ \u2260 _\ud835\udc5f_ \ufffd\n\n**Traverse by Depth-First Search (Optimized Method).** We further enhance traversal efficiency by maintaining an auxiliary variable vol_sum for each node and performing a DFS on the sampled\nUST _\ud835\udf0f_ . When visiting a node _\ud835\udc63_, we update vol_sum by adding or\nsubtracting 2 _\ud835\udc5a_ \u2212vol( _Sub_ ( _\ud835\udf0f, \ud835\udc63_ )) if the edge connecting _\ud835\udc63_ to its parent\n_\ud835\udc5d_ ( _\ud835\udc63_ ) in _\ud835\udf0f_ also appears in the BFS tree obtained in step(1), where\n_Sub_ ( _\ud835\udf0f, \ud835\udc63_ ) denotes the subtree rooted at _\ud835\udc63_ in _\ud835\udf0f_ . After the update, we\nquery the vol_sum at _\ud835\udc63_, multiply it by _\ud835\udc51_ ( _\ud835\udc63_ ) to contribute to the KC\nestimate, recursively continue the DFS on its children, and finally\nrestore the affected vol_sum values by undoing the update.\n\nTheorem 3.7 (Correctness of Optimized Method). _Given an_\n_arbitrary spanning tree \ud835\udf0f_ _and a fixed spanning tree \ud835\udf0f_ 0 _, let \ud835\udc5f_ _be the_\n_root of both trees. Let_ P _\ud835\udc62_ _denote the path from node \ud835\udc62_ _to \ud835\udc5f_ _in \ud835\udf0f_ 0 _. Then,_\n_the following equality holds:_\n\u2211\ufe01 \u2211\ufe01 vol( _\ud835\udc47_ 1) = \u2211\ufe01 (2 _\ud835\udc5a_ \u2212 vol( _Sub_ ( _\ud835\udf0f, \ud835\udc63_ ))) _,_ (5)\n\n\u2211\ufe01\n\n(2 _\ud835\udc5a_ \u2212 vol( _Sub_ ( _\ud835\udf0f, \ud835\udc63_ ))) _,_ (5)\n\n_\ud835\udc62_ \u2260 _\ud835\udc5f_\n\n\u2211\ufe01\n\n_\ud835\udc62_ \u2260 _\ud835\udc5f_\n\n\u2211\ufe01 vol( _\ud835\udc47_ 1) = \u2211\ufe01\n\n_\ud835\udc47_ 1\u222a _\ud835\udc47_ 2 \u2208F [P] _[\ud835\udc62]_ ( _\ud835\udf0f_ ) ( _\ud835\udc63,\ud835\udc5d_ ( _\ud835\udc63_\n\n_\ud835\udc62_ \u2208 _\ud835\udc46_ 1\n\n\u2211\ufe01 vol( _\ud835\udc47_ 1) = \u2211\ufe01\n\n_\ud835\udc47_ 1\u222a _\ud835\udc47_ 2 \u2208F [P\u2032] _[\ud835\udc62]_ ( _\ud835\udf0f_ ) ( _\ud835\udc5d_ ( _\ud835\udc63_ ) _,\ud835\udc63_\n\n( _\ud835\udc63,\ud835\udc5d_ ( _\ud835\udc63_ )) \u2208 _\ud835\udf0f_ 0\n_\ud835\udc62_ \u2208 _\ud835\udc46_ 1\n\n( _\ud835\udc5d_ ( _\ud835\udc63_ ) _,\ud835\udc63_ ) \u2208 _\ud835\udf0f_ 0\n_\ud835\udc62_ \u2208 _\ud835\udc46_ 2\n\n\u2211\ufe01\n\n(2 _\ud835\udc5a_ \u2212 vol( _Sub_ ( _\ud835\udf0f, \ud835\udc63_ ))) _,_ (6)\n\n_where Sub_ ( _\ud835\udf0f, \ud835\udc63_ ) _is the node set in the subtree rooted at \ud835\udc63_ _within \ud835\udf0f, and_\n_\ud835\udc46_ 1 = _Sub_ ( _\ud835\udf0f, \ud835\udc63_ ) \u2229 _Sub_ ( _\ud835\udf0f_ 0 _, \ud835\udc63_ ) _,\ud835\udc46_ 2 = _Sub_ ( _\ud835\udf0f, \ud835\udc63_ ) \u2229 _Sub_ ( _\ud835\udf0f_ 0 _, \ud835\udc5d_ ( _\ud835\udc63_ )) _. \ud835\udc5d_ ( _\ud835\udc63_ ) _is_\n_the parent node of \ud835\udc63_ _in \ud835\udf0f_ _and therefore each edge_ ( _\ud835\udc63, \ud835\udc5d_ ( _\ud835\udc63_ )) _is directed_\n_towards the root \ud835\udc5f_ _in \ud835\udf0f._\n\nProof. For each node _\ud835\udc62_, we apply the forward path mapping\nP _\ud835\udc62_, which gives\n\u2211\ufe01 vol( _\ud835\udc47_ 1) = \u2211\ufe01 \ufffd2 _\ud835\udc5a_ \u2212 vol( _\ud835\udc46\ud835\udc62\ud835\udc4f_ ( _\ud835\udf0f, \ud835\udc63_ ))\ufffd _._\n\n\u2211\ufe01 vol( _\ud835\udc47_ 1) = \u2211\ufe01\n\n_\ud835\udc47_ 1\u222a _\ud835\udc47_ 2 \u2208F [P] _[\ud835\udc62]_ ( _\ud835\udf0f_ ) ( _\ud835\udc63,\ud835\udc5d_ ( _\ud835\udc63_ )) \u2208\n\n\ufffd2 _\ud835\udc5a_ \u2212 vol( _\ud835\udc46\ud835\udc62\ud835\udc4f_ ( _\ud835\udf0f, \ud835\udc63_ ))\ufffd _._\n\n( _\ud835\udc63,\ud835\udc5d_ ( _\ud835\udc63_ )) \u2208 _\ud835\udf0f_ 0\u2229 _\ud835\udf0f_\n_\ud835\udc62_ \u2208 _\ud835\udc46\ud835\udc62\ud835\udc4f_ ( _\ud835\udf0f,\ud835\udc63_ )\u2229 _\ud835\udc46\ud835\udc62\ud835\udc4f_ ( _\ud835\udf0f_ 0 _,\ud835\udc63_ )\n\n\u2211\ufe01 _\ud835\udc51_ ( _\ud835\udc62_ ) \ufffd\ufffd\ufffd\n\n_\ud835\udc62_ \u2260 _\ud835\udc5f_\n\ufffd\n\n_\ud835\udc53_ ( _\ud835\udf0f_ ) = \u2211\ufe01\n\n\u2211\ufe01 vol( _\ud835\udc47_ 1) \u2212 \u2211\ufe01\n\n_\ud835\udc47_ 1\u222a _\ud835\udc47_ 2 \u2208F [P] _[\ud835\udc62]_ ( _\ud835\udf0f_ ) _\ud835\udc47_ \u222a _\ud835\udc47_ \u2208F\n\n\u2211\ufe01\n\n\u2211\ufe01 vol( _\ud835\udc47_ 1)\ufffd\ufffd\ufffd\n\n_\ud835\udc47_ 1\u222a _\ud835\udc47_ 2 \u2208F [P\u2032] _[\ud835\udc62]_ ( _\ud835\udf0f_ ) \ufffd\n\n_._\n\nProof. Based on Theorem 3.1 and Theorem 3.3, we can represent the set F _\ud835\udc5f_ | _\ud835\udc62_ by applying the path mapping technique to the\nspanning tree set \u0393. With this representation, KC can be expressed\n\nBy the definition of path mapping, to obtain a valid 2-forest in\nF [P] _[\ud835\udc62]_ ( _\ud835\udf0f_ ), the deleted edge ( _\ud835\udc63, \ud835\udc5d_ ( _\ud835\udc63_ )) in _\ud835\udf0f_ must appear on both paths\nfrom _\ud835\udc62_ to _\ud835\udc5f_ in the two spanning trees _\ud835\udf0f_ and _\ud835\udf0f_ 0. This implies that\n_\ud835\udc62_ must belong to the intersection of the subtrees of _\ud835\udc63_ in _\ud835\udf0f_ and _\ud835\udf0f_ 0,\ndenoted as _\ud835\udc46_ 1 = _\ud835\udc46\ud835\udc62\ud835\udc4f_ ( _\ud835\udf0f,\ud835\udc63_ ) \u2229 _\ud835\udc46\ud835\udc62\ud835\udc4f_ ( _\ud835\udf0f_ 0 _, \ud835\udc63_ ). Therefore, summing over all\n_\ud835\udc62_ \u2260 _\ud835\udc5f_, we obtain\n\u2211\ufe01 \u2211\ufe01 vol( _\ud835\udc47_ 1) = \u2211\ufe01 (2 _\ud835\udc5a_ \u2212 vol( _Sub_ ( _\ud835\udf0f, \ud835\udc63_ ))) _._\n\n\u2211\ufe01\n\n(2 _\ud835\udc5a_ \u2212 vol( _Sub_ ( _\ud835\udf0f, \ud835\udc63_ ))) _._\n\n_\ud835\udc62_ \u2260 _\ud835\udc5f_\n\n\u2211\ufe01 vol( _\ud835\udc47_ 1) = \u2211\ufe01\n\n_\ud835\udc47_ 1\u222a _\ud835\udc47_ 2 \u2208F [P] _[\ud835\udc62]_ ( _\ud835\udf0f_ ) ( _\ud835\udc63,\ud835\udc5d_ ( _\ud835\udc63_ )) \u2208\n\n( _\ud835\udc63,\ud835\udc5d_ ( _\ud835\udc63_ )) \u2208 _\ud835\udf0f_ \u2229 _\ud835\udf0f_ 0\n_\ud835\udc62_ \u2208 _\ud835\udc46_ 1\n\nScalable and Provable Kemeny Constant Computation on Static and Dynamic Graphs: A 2-Forest Sampling Approach\n\n**Algorithm 1:** TTF\n\n**Input:** A graph _\ud835\udc3a_ = ( _\ud835\udc49, \ud835\udc38_ ), the sample size _\ud835\udf14_, the root node _\ud835\udc5f_\n**Output:** \u02dc _\ud835\udf05_ as the estimation of Kemeny Constant\n\n**1** \u02dc _\ud835\udf05_ \u2190 0;\n\n**2** Generate _\ud835\udf0f_ 0 using BFS with root _\ud835\udc5f_ ;\n\n**3** DFSin, DFSout \u2190 _\ud835\udf0f_ 0;\n\n**4** vol_sum \u2190 BIT(| _\ud835\udc49_ |);\n\n**5 for** _\ud835\udc56_ \u2190 1 **to** _\ud835\udf14_ **do**\n\n**6** _\ud835\udf0f\ud835\udc56_ \u2190 wilson ( _\ud835\udc3a,\ud835\udc5f_ );"}, {"role": "assistant", "content": "**Analogy:** Imagine a big playground with lots of kids and ropes connecting them. We want to find a 'friendship score' for this playground, which tells us how quickly, on average, a kid starting anywhere can visit all their friends by skipping along the ropes.\n\n**Explanation:** Figuring out this 'friendship score' for a really big playground is super tricky and takes too long. So, these clever people found a new way to guess it! They play a special game where they first make a 'tree map' of the playground \u2013 that's a way to connect all the kids with just enough ropes so everyone can reach everyone else, but without any loops. Then, they use a clever trick called 'path mapping' to cut just one rope on that tree map, which splits the kids into two connected groups, like two tiny forests. By doing this cutting game many, many times on different tree maps, and adding up some numbers for each 'two-forest' they make, they can get a very good guess for the 'friendship score' much faster. It's like having a special recipe to count things in a clever way instead of counting every single thing one by one."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n_\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc53\ud835\udc56\ud835\udc59\ud835\udc59\ud835\udc52\ud835\udc51_\ud835\udc5d\ud835\udc56\ud835\udc5d\ud835\udc52_ **then**\n\n**150** _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ = _\ud835\udc5a\ud835\udc56\ud835\udc5b_ ( _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ + _\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61\ud835\udc60_ _ _\ud835\udc51\ud835\udc52\ud835\udc59\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc51, \ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\ud835\udc61_ _ _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ ) ;"}, {"role": "assistant", "content": "**Analogy:** Imagine a super fun slide at the park where kids want to go down. Our 'cwnd' is like how many kids we let go down the slide at the same time.\n\n**Explanation:** If the slide is already full of kids (that's `BBR.filled_pipe` being true), we want to let even more kids go down! So, we try to add the new kids who just arrived (`packets_delivered`) to the ones already on the slide. But, we have a special rule: we can never let more kids on the slide than the biggest number the slide can handle (`BBR.target_cwnd`). So, we pick the smaller number: either the new bigger group, or the slide's biggest limit. This way, the slide never gets too crowded, and everyone has a good time!"}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\nIn this work, we study the problem of approximating Kemeny constant problem for both static and dynamic graphs. We propose two\nnovel formulas of Kemeny constant and design an biased estimator\nbased on spanning trees and 2-forests. For static graphs, we develop\na sampling-based algorithm with stronger theoretical guarantees.\nThe proposed method outperforms SOTA approaches in both efficiency and accuracy on real-world datasets. For dynamic graphs,\nwe introduce two sample maintenance strategies that efficiently\npreserve the correctness of samples instead of recomputing from\nscratch for each update. Both two methods are faster than static\nalgorithms, with only a slight loss in precision. Extensive experiments on real-world graphs demonstrate the effectiveness and\nefficiency of our solutions.\n\n**REFERENCES**\n\n[1] William Aiello, Fan Chung, and Linyuan Lu. 2001. A random graph model for\npower law graphs. _Experimental mathematics_ 10, 1 (2001), 53\u201366.\n\n[2] Diego Altafini, Dario A Bini, Valerio Cutini, Beatrice Meini, and Federico Poloni.\n2023. An edge centrality measure based on the Kemeny constant. _SIAM J. Matrix_\n_Anal. Appl._ 44, 2 (2023), 648\u2013669.\n\n[3] Reid Andersen, Fan Chung, and Kevin Lang. 2006. Local graph partitioning using\npagerank vectors. In _FOCS_ . IEEE, 475\u2013486.\n\n[4] Eugenio Angriman, Maria Predari, Alexander van der Grinten, and Henning\nMeyerhenke. 2020. Approximation of the Diagonal of a Laplacian\u2019s Pseudoinverse\nfor Complex Network Analysis. _ESA_ 173 (2020), 6:1\u20136:24.\n\n[5] Luca Avena, Fabienne Castell, Alexandre Gaudilli\u00e8re, and Clothilde M\u00e9lot. 2018.\nRandom forests and networks analysis. _Journal of Statistical Physics_ 173 (2018),\n985\u20131027.\n\n[6] Bahman Bahmani, Abdur Chowdhury, and Ashish Goel. 2010. Fast Incremental\nand Personalized PageRank. _VLDB_ 4, 3 (2010), 173\u2013184.\n\n[7] B\u00e9la Bollob\u00e1s. 2013. _Modern graph theory_ . Vol. 184. Springer Science & Business\nMedia.\n\n[8] B\u00e9la Bollob\u00e1s and B\u00e9la Bollob\u00e1s. 1998. _Random graphs_ . Springer.\n\n[9] Mo Chen, Jianzhuang Liu, and Xiaoou Tang. 2008. Clustering via Random Walk\nHitting Time on Directed Graphs. In _AAAI_, Vol. 8. 616\u2013621.\n\n[10] Fan Chung and Ji Zeng. 2023. Forest formulas of discrete Green\u2019s functions. _J._\n_Graph Theory_ 102, 3 (2023), 556\u2013577.\n\n[11] S Condamin, O B\u00e9nichou, V Tejedor, R Voituriez, and Joseph Klafter. 2007. Firstpassage times in complex scale-invariant media. _Nature_ 450, 7166 (2007), 77\u201380.\n\n[12] Emanuele Crisostomi, Stephen Kirkland, and Robert Shorten. 2011. A Google-like\nmodel of road network dynamics and its application to regulation and control.\n_Internat. J. Control_ 84, 3 (2011), 633\u2013651.\n\n[13] Peter G Doyle. 2009. The Kemeny constant of a Markov chain. _arXiv preprint_\n_arXiv:0909.2636_ (2009).\n\n[14] Peter M. Fenwick. 1994. A New Data Structure for Cumulative Frequency Tables.\n_Softw. Pract. Exp._ 24, 3 (1994), 327\u2013336.\n\n[15] Julie Fournet and Alain Barrat. 2014. Contact Patterns among High School\nStudents. _PLoS ONE_ [9, 9 (09 2014), e107878. https://doi.org/10.1371/journal.pone.](https://doi.org/10.1371/journal.pone.0107878)\n[0107878](https://doi.org/10.1371/journal.pone.0107878)\n\n[16] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for\nnetworks. In _KDD_ . 855\u2013864.\n\n[17] Takanori Hayashi, Takuya Akiba, and Yuichi Yoshida. 2016. Efficient Algorithms\nfor Spanning Tree Centrality. In _IJCAI_, Vol. 16. 3733\u20133739.\n\n[18] Wassily Hoeffding. 1994. Probability inequalities for sums of bounded random\nvariables. _The collected works of Wassily Hoeffding_ (1994), 409\u2013426.\n\n[19] Guanhao Hou, Qintian Guo, Fangyuan Zhang, Sibo Wang, and Zhewei Wei. 2023.\nPersonalized PageRank on evolving graphs with an incremental index-update\nscheme. _SIGMOD_ 1, 1 (2023), 1\u201326.\n\n[20] Jeffrey J Hunter. 2014. The role of Kemeny\u2019s constant in properties of Markov\nchains. _Communications in Statistics-Theory and Methods_ 43, 7 (2014), 1309\u20131321.\n\n[21] Michael F Hutchinson. 1989. A stochastic estimator of the trace of the influence\nmatrix for Laplacian smoothing splines. _Communications in Statistics-Simulation_\n_and Computation_ 18, 3 (1989), 1059\u20131076.\n\n[22] John G Kemeny, J Laurie Snell, et al. 1969. _Finite markov chains_ . Vol. 26. van\nNostrand Princeton, NJ.\n\n[23] Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford Large Network\n[Dataset Collection. http://snap.stanford.edu/data.](http://snap.stanford.edu/data)\n\n[24] Mark Levene and George Loizou. 2002. Kemeny\u2019s constant and the random\nsurfer. _The American mathematical monthly_ 109, 8 (2002), 741\u2013745.\n\n[25] Shiju Li, Xin Huang, and Chul-Ho Lee. 2021. An efficient and scalable algorithm\nfor estimating Kemeny\u2019s constant of a Markov chain on large graphs. In _KDD_ .\n964\u2013974.\n\n[26] Zihao Li, Dongqi Fu, and Jingrui He. 2023. Everything evolves in personalized\npagerank. In _WWW_ . 3342\u20133352.\n\n[27] Meihao Liao, Rong-Hua Li, Qiangqiang Dai, Hongyang Chen, Hongchao Qin,\nand Guoren Wang. 2023. Efficient personalized pagerank computation: The\npower of variance-reduced monte carlo approaches. _SIGMOD_ 1, 2 (2023), 1\u201326.\n\n[28] Meihao Liao, Rong-Hua Li, Qiangqiang Dai, Hongyang Chen, Hongchao Qin,\nand Guoren Wang. 2023. Efficient resistance distance computation: The power\nof landmark-based approaches. _SIGMOD_ 1, 1 (2023), 1\u201327.\n\n[29] Meihao Liao, Rong-Hua Li, Qiangqiang Dai, Hongyang Chen, and Guoren Wang.\n2023. Scalable Algorithms for Laplacian Pseudo-inverse Computation. _arXiv_\n_preprint arXiv:2311.10290_ (2023).\n\n[30] Meihao Liao, Rong-Hua Li, Qiangqiang Dai, and Guoren Wang. 2022. Efficient\npersonalized pagerank computation: A spanning forests sampling based approach. In _SIGMOD_ . 2048\u20132061.\n\n[31] Meihao Liao, Junjie Zhou, Rong-Hua Li, Qiangqiang Dai, Hongyang Chen, and\nGuoren Wang. 2024. Efficient and Provable Effective Resistance Computation on\nLarge Graphs: An Index-based Approach. _SIGMOD_ 2, 3 (2024), 1\u201327.\n\n[32] Haoyu Liu and Siqiang Luo. 2024. BIRD: Efficient Approximation of Bidirectional\nHidden Personalized PageRank. _VLDB_ 17, 9 (2024), 2255\u20132268.\n\n[33] L\u00e1szl\u00f3 Lov\u00e1sz. 1993. Random walks on graphs. _Combinatorics, Paul erdos is_\n_eighty_ 2, 1-46 (1993), 4.\n\n[34] Sam Alexander Martino, Jo\u00e3o Morado, Chenghao Li, Zhenghao Lu, and Edina\nRosta. 2024. Kemeny Constant-Based Optimization of Network Clustering Using\nGraph Neural Networks. _The Journal of Physical Chemistry B_ 128, 34 (2024),\n8103\u20138115.\n\n[35] Dingheng Mo and Siqiang Luo. 2021. Agenda: Robust personalized pageranks in\nevolving graphs. In _CIKM_ . 1315\u20131324.\n\n[36] Naoto Ohsaka, Takanori Maehara, and Ken-ichi Kawarabayashi. 2015. Efficient\npagerank tracking in evolving networks. In _KDD_ . 875\u2013884.\n\n[37] Rushabh Patel, Pushkarini Agharkar, and Francesco Bullo. 2015. Robotic surveillance and Markov chains with minimal weighted Kemeny constant. _IEEE Trans._\n_Automat. Control_ 60, 12 (2015), 3156\u20133167.\n\n[38] Bibek Paudel and Abraham Bernstein. 2021. Random walks with erasure: Diversifying personalized recommendations on social and information networks. In\n_WWW_ . 2046\u20132057.\n\n[39] Pan Peng, Daniel Lopatta, Yuichi Yoshida, and Gramoz Goranci. 2021. Local\nalgorithms for estimating effective resistance. In _KDD_ . 1329\u20131338.\n\n[40] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning\nof social representations. In _KDD_ . 701\u2013710.\n\n[41] Maria Predari, Lukas Berner, Robert Kooij, and Henning Meyerhenke. 2023.\nGreedy optimization of resistance-based graph robustness with global and local\nedge insertions. _Soc. Netw. Anal. Min._ 13, 1 (2023), 130.\n\n[42] Ryan A. Rossi and Nesreen K. Ahmed. 2015. The Network Data Repository with Interactive Graph Analytics and Visualization. In _AAAI_ . [https:](https://networkrepository.com)\n[//networkrepository.com](https://networkrepository.com)\n\n[43] Purnamrita Sarkar and Andrew W Moore. 2011. Random walks in social networks\nand their applications: a survey. _Social Network Data Analytics_ (2011), 43\u201377.\n\n[44] Christian L. Staudt, Aleksejs Sazonovs, and Henning Meyerhenke. 2015.\nNetworKit: A Tool Suite for Large-scale Complex Network Analysis.\n[arXiv:1403.3005 [cs.SI] https://arxiv.org/abs/1403.3005](https://arxiv.org/abs/1403.3005)\n\n[45] Prasad Tetali. 1991. Random walks and the effective resistance of networks.\n_Journal of Theoretical Probability_ 4, 1 (1991), 101\u2013109.\n\n[46] Hanzhi Wang, Zhewei Wei, Junhao Gan, Ye Yuan, Xiaoyong Du, and Ji-Rong\nWen. 2022. Edge-based local push for personalized PageRank. _VLDB_ 15, 7 (2022),\n1376\u20131389.\n\n[47] Scott White and Padhraic Smyth. 2003. Algorithms for estimating relative\nimportance in networks. In _KDD_ . 266\u2013275.\n\n[48] David Bruce Wilson. 1996. Generating random spanning trees more quickly\nthan the cover time. In _STOC_ . 296\u2013303.\n\n[49] Feng Xia, Haifeng Liu, Ivan Lee, and Longbing Cao. 2016. Scientific article\nrecommendation: Exploiting common author relations and historical preferences.\n_IEEE Trans. Big Data_ 2, 2 (2016), 101\u2013112.\n\n[50] Feng Xia, Jiaying Liu, Hansong Nie, Yonghao Fu, Liangtian Wan, and Xiangjie\nKong. 2019. Random walks: A review of algorithms and applications. _IEEE_\n_Transactions on Emerging Topics in Computational Intelligence_ 4, 2 (2019), 95\u2013\n107.\n\n[51] Haisong Xia and Zhongzhi Zhang. 2024. Efficient Approximation of Kemeny\u2019s\nConstant for Large Graphs. _SIGMOD_ 2, 3 (2024), 1\u201326.\n\n[52] Wanyue Xu, Yibin Sheng, Zuobai Zhang, Haibin Kan, and Zhongzhi Zhang. 2020.\nPower-law graphs have minimal scaling of Kemeny constant for random walks.\nIn _WWW_ . 46\u201356.\n\n[53] Mingji Yang, Hanzhi Wang, Zhewei Wei, Sibo Wang, and Ji-Rong Wen. 2024.\nEfficient algorithms for personalized pagerank computation: A survey. _IEEE_\n_TKDE_ (2024).\n\n[54] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, and Sourav S Bhowmick.\n2020. Homogeneous Network Embedding for Massive Graphs via Reweighted\n\nCheng Li, Meihao Liao, Rong-Hua Li, Guoren Wang\n\nPersonalized PageRank. _VLDB_ 13, 5 (2020), 670\u2013683.\n\n[55] Renchi Yang and Jing Tang. 2023. Efficient estimation of pairwise effective\nresistance. _Proceedings of the ACM on Management of Data_ 1, 1 (2023), 1\u201327.\n\n[56] Serife Yilmaz, Ekaterina Dudkina, Michelangelo Bin, Emanuele Crisostomi,\nPietro Ferraro, Roderick Murray-Smith, Thomas Parisini, Lewi Stone, and Robert\nShorten. 2020. Kemeny-based testing for COVID-19. _PLOS ONE_ 15, 11 (2020),\n1\u201319.\n\n[57] Hongzhi Yin, Bin Cui, Jing Li, Junjie Yao, and Chen Chen. 2012. Challenging the\nLong Tail Recommendation. _VLDB_ 5, 9 (2012).\n\n[58] Minji Yoon, Woojeong Jin, and U Kang. 2018. Fast and accurate random walk\nwith restart on dynamic graphs with guarantees. In _WWW_ . 409\u2013418.\n\n[59] Hongyang Zhang, Peter Lofgren, and Ashish Goel. 2016. Approximate personalized pagerank on dynamic graphs. In _KDD_ . 1315\u20131324.\n\n[60] Xiaohan Zhao, Adelbert Chang, Atish Das Sarma, Haitao Zheng, and Ben Y\nZhao. 2013. On the embeddability of random walk distances. _VLDB_ 6, 14 (2013),\n1690\u20131701.\n\n[61] Yanping Zheng, Hanzhi Wang, Zhewei Wei, Jiajun Liu, and Sibo Wang. 2022.\nInstant graph neural networks for dynamic graphs. In _KDD_ . 2605\u20132615."}, {"role": "assistant", "content": "**Analogy:** Imagine you have a big neighborhood map, and you want to know how easy it is for a little postman to deliver mail to everyone, no matter where he starts. The 'Kemeny constant' is like figuring out the average time it takes for him to visit every house at least once. Sometimes the neighborhood is 'static' like a drawing that never changes. Other times, it's 'dynamic' like a drawing where new houses pop up or old ones disappear all the time!\n\n**Explanation:** We found two new, clever ways (like new rules for drawing maps) to figure out this 'postman's travel time'. We also made a special guesser (like a helpful bird) that looks at just the main roads and a few side streets (called 'spanning trees' and '2-forests') instead of every single tiny path. For the unchanging map, we have a game where we pick some houses randomly to check, and we're super sure our guess is right. For the changing map, instead of drawing a brand new map every time a house moves, we found two smart ways to just erase the old house and draw the new one quickly on the same map. This makes our postman's travel time guess much faster, even if it's just a tiny bit less perfect. It's like only updating the small part of the map that changed instead of starting from scratch!"}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n**4.1** **Setup**\n\nWe implement DynaExq using PyTorch [22] and HuggingFace\nTransformers [31], with AutoRound [16] providing low-bit expert quantization. All experiments are conducted on **RTX-5090**\nand **A6000** GPUs (32-48 GB HBM), representing realistic memoryconstrained single-device settings targeted by this work.\n**Models and Datasets.** We evaluate DynaExq on **Qwen3-30B-**\n**A3B** [1] and **Qwen3-80B-A3B** [2]. The 30B model contains 48\nlayers and 128 experts per layer (8 active), while the 80B variant\nincreases to 512 experts (10 active) with a comparable active parameter count of \u223c3B. Both models exhibit substantial expert-level\nredundancy, making them strong candidates for runtime precision\nadaptation.\nWe benchmark across diverse tasks: WikiText-2 [21], MMLUPro [30], GPQA [24], AIME25 [3], GSM8K [5], HumanEval [4], and\nMMLU-ProX [32]. These datasets jointly stress different aspects of\nmodel behavior, including linguistic and mathematical reasoning,\nmultilingual understanding, and code generation.\nTo isolate the effect of expert-level quantization, all non-expert\nand routing components remain in full precision. We evaluate DynaExq against uniform FP16, INT4, and INT2 baselines, ensuring\nconsistent routing dynamics and enabling a controlled study of\nexpert precision strategies.\n\nDynamic Expert Quantization for Scalable Mixture-of-Experts Inference\n\n**4.2** **Perplexity under Expert Quantization**\n\n**4.4** **Performance Metrics**\n\nWe further assess latency and throughput: time-to-first-token (TTFT),\ntime-per-output-token (TPOP), prefill throughput, and decode throughput (Figures 5). DynaExq achieves performance between highprecision (FP16/INT4) and low-precision (INT2/INT4) baselines,\ntypically retaining more than 85% of INT2 throughput.\nDespite selectively assigning high precision only to hot-path\nexperts, DynaExq maintains stable performance across both GPUs\nand both model sizes. This indicates that its asynchronous precision\nswitching pipeline effectively decouples background transitions\nfrom the forward path, avoiding runtime stalls.\nTaken together, these results show that DynaExq achieves a\nfavorable balance: it recovers much of the accuracy benefit of\nhigh-precision experts while incurring only moderate overhead\nrelative to uniformly low-precision inference. The resulting hybridprecision execution enables large MoE models to run on single-GPU\nhardware otherwise unable to host full-precision variants. As model\nscales continue to grow, this precision-aware approach provides a\npractical path toward sustaining deployability under increasingly\nstringent memory limits."}, {"role": "assistant", "content": "**Analogy:** Imagine you have a giant toy factory that makes lots of different toys. Some toys, like a fancy dancing robot, need super detailed, perfect instructions. Other toys, like simple building blocks, can use easier, less detailed instructions. If you give super detailed instructions to *every single worker* for *every single toy*, the factory needs huge instruction books and gets really slow. But what if you only give the super detailed instructions to the workers making the fancy dancing robots (the 'hot-path experts') and simpler instructions to everyone else? Your factory runs much faster, still makes great dancing robots, and can even fit into a smaller building!\n\n**Explanation:** This smart computer trick is like making sure the most important workers in our computer's 'brain' get super clear instructions, while the less busy workers get simpler ones. This way, the giant brainy computer can still do amazing things, like answering questions or drawing pictures, even if it's running on a smaller computer box that doesn't have enough space for super clear instructions for absolutely everything. It helps the big computer work well without needing a super-duper huge instruction book for every tiny piece of information."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n**6.1** **Experimental Setup**\n\n**Implementation** . We implement TLT on top of VeRL [53]\nand utilizes Ray [37] for distributed execution. The Worker\nCoordinator is realized with a centralized controller that\n\nemploys ZeroMQ [17] messaging. The Adaptive Drafter\u2019s\nimplementation is based on training pipelines from EAGLE\nand trained with FSDP2 [44]. For asynchronous checkpointing, we leverage PyTorch DCP [45]. To further improve generation efficiency, we employ SGLang [70] as the rollout\nbackend, with implementing adaptive enabling of SD, drafter\nweight update and the BEG-MAB tuner following [55]. We\nadditionally use [68] as our model-free drafter.\n**Testbed** . We conduct experiments on a cluster of 8 NVIDIA\nDGX H100 [39] servers, totaling 64 GPUs. Each server is\nequipped with 8 NVIDIA H100 GPUs, 2 Intel Xeon Platinum\n8480C (112 cores) CPUs and 2TB memory. GPUs are interconnected to each other by 900GB/s NVLink, and inter-node\ncommunication is achieved via NVIDIA Mellanox 400Gb/s\nInfiniBand. To evaluate system\u2019s performance across heterogeneous hardware, we also conduct experiments on a\ndifferent cluster equipped with NVIDIA DGX A100 servers.\nUnless otherwise specified, experiments are conducted on\nH100 GPUs by default.\n**Models** . Following the training procedure introduced in\nDeepSeek-R1 [9], the reasoning RL process can begin either from a fine-tuned model or directly from a base model\nwithout additional fine-tuning. To demonstrate that TLT robustly accommodates different model architectures, scales,\nand training paradigms, we evaluate the following models:\n\n(1) _Qwen2.5-7B_ (Qwen2.5-7B): a popular base model;\n(2) _DeepSeek-R1-Distill-Qwen-7B_ (DeepSeek-7B): a distilled\nmodel exhibiting good initial reasoning capabilities;\n(3) _Qwen2.5-32B_ (Qwen-32B): a larger base model without\nany additional fine-tuning, as in [66];\n(4) _Llama-3.3-70B-Instruct_ (Llama-70B): a larger instructiontuned model to assess scalability.\n**Datasets** . For the primary RL training dataset, we use a\nsubset from Eurus-2-RL [8], a comprehensive dataset curated\nfor reasoning RL. This dataset contains mathematics and\ncoding problems equipped with verifiers (LaTeX answers and\n\ntest cases, respectively). The coding problems originate from\ncompetitive programming sources (e.g., CodeContests [23],\nCodeforces [7]), while the math problems range from high\nschool to olympiad-level difficulty [24]. For drafter training,\nwe use a subset from OpenThoughts2-1M [41] as the warmup dataset.\n**RL Settings** . We primarily follow the reasoning RL algorithm GRPO proposed by DeepSeek [9, 51]. Specifically, for\neach prompt, we set the temperature=0.9, with a maximum\ngeneration length of 32K tokens, consistent with common RL\ntraining settings. We apply TP= 2, 4, or 8 depending on model\nscale for the rollout engine. Additionally, all experiments utilize mixed-precision training with the Adam optimizer [19],\nwhere BF16 precision is employed during both model rollout\nand inference stages.\n**Baselines** . We consider the following baselines.\n\n(1) _Open-R1_ [10]: a popular RL training framework built\nupon TRL[59], integrating vLLM [21] for rollouts and\nDeepSpeed [47] for training. Currently, it only supports\nseparate model placement, requiring serving and training to be performed on distinct nodes.\n(2) _VeRL_ [53]: the state-of-the-art open-source RL training\nframework by ByteDance. It enables colocation of models on shared devices by utilizing GPU time-sharing.\n(3) _TLT-Base_ : Our base implementation with vanilla speculative decoding support. Specifically, we disable adaptive drafter and use the model-free drafter instead.\n\n**Metrics** . Following [53], we use token throughput (tokens/sec)\nfor end-to-end evaluation. This metric is calculated by dividing the total number of tokens in both prompts and responses\nwithin a global batch by the duration of a single RL step. We\naverage results over three training steps following an initial\nwarm-up step and use a trained draft model for evaluation.\n\n**6.2** **End-to-End Evaluation**\n\nWe evaluate TLT against popular RL training frameworks,\nincluding Open-R1 [10] and VeRL [53], across multiple GPU\nplatforms (NVIDIA H100 and A100) and various LLM scales.\nFigure 11 demonstrates that TLT consistently outperforms\nVeRL. Open-R1 consistently underperforms, primarily because it struggles to fully utilize all available GPUs concurrently and suffers from a tight coupling between its rollout\nand training batch size. Similar substantial gains are observed\non the A100 platform, highlighting its effectiveness across\ndifferent hardware generations. We also present results for\nTLT-Base, which utilizes a model-free drafter and yields good\nbenefits, demonstrating that the TLT framework provides\ncontinuous efficiency gains throughout the entire training\nprocess. These end-to-end performance gains stem directly\nfrom TLT\u2019s ability to mitigate the long-tail rollout bottleneck\nof reasoning RL training.\nMoreover, Figure 12 illustrates the average reward curves\nduring RL training for both VeRL and TLT using Qwen2.57B and Qwen2.5-32B models. It is evident that two curves\n\n9\n\n![](papers/images/Taming-the-LongTail-Efficient-Reasoning-RL-Training-with-Adaptive-Drafter.pdf-9-0.png)\n\n**Figure 11. End-to-end Training Speed Evaluation.** The y-axis indicates the relative training throughput of each system\nrunning GRPO [51] RL algorithm. TLT achieves 1.7-2.1\u00d7 speedup over the state-of-the-art RL training system VeRL [53].\n\n0.6\n\n0.5\n\n![](papers/images/Taming-the-LongTail-Efficient-Reasoning-RL-Training-with-Adaptive-Drafter.pdf-9-1.png)\n\n0.4\n\n0.3\n\n0.2\n\n20 40 60 80 100\nRL Step\n\n20 40 60 80 100\nRL Step\n\n**Figure 12. End-to-end Training Curves.** Average reward curves for VeRL and TLT using the Qwen2.5 7B and 32B models.\n\n**Table 1. Effect of topK for speculative decoding.** Results\nare benchmarked using Qwen-32B on H100 GPUs (TP=4),\nwith Draft_Depth=12 and Tokens_to_Verify=64.\n\nTopK 4 6 8 10 12 16\n\nAccept Length 8.29 8.66 8.67 8.67 8.60 8.42\n\nSpeedup 3.51\u00d7 **3.65** \u00d7 3.64\u00d7 3.64\u00d7 3.56\u00d7 3.47\u00d7\n\n![](papers/images/Taming-the-LongTail-Efficient-Reasoning-RL-Training-with-Adaptive-Drafter.pdf-9-2.png)\n\n![](papers/images/Taming-the-LongTail-Efficient-Reasoning-RL-Training-with-Adaptive-Drafter.pdf-9-4.png)\n\n![](papers/images/Taming-the-LongTail-Efficient-Reasoning-RL-Training-with-Adaptive-Drafter.pdf-9-3.png)\n\n**Figure 13. Effect of hyperparameters for speculative**\n**decoding.** Measurements performed with Qwen-32B (TP=4).\n\noverlap closely, indicating that TLT preserves model quality.\nThis confirms that TLT\u2019s adaptive drafter and rollout engine\neffectively accelerate training without compromising the\nlearning dynamics of the underlying RL algorithm.\n\n**6.3** **Evaluation of Adaptive SD**\n\nIn this section, we provide in-depth analysis on the effectiveness of our design for adaptive speculative decoding.\n**Effect of Adaptive SD Strategy** . Tuning the SD configuration is crucial for achieving optimal speedups. Figure 13\nillustrates how Draft_Depth and Tokens_to_Verify influence speculative decoding performance with batch size = 1,\ntopK=8 and CUDA Graph enabled. For more stable accept\nlength in the grid search, we set temperature=0. Increasing\nthe draft depth generally raises the average accept length\n(a), though this benefit tapers off once the draft depth becomes sufficiently large. (b) reports the resulting speedups\nover the non-speculative baseline. This demonstrates that\n\nmaximizing accept length alone is insufficient; drafting parameters must be tuned to optimize real performance rather\nthan intermediate metrics.\n\nRegarding topK, we find that efficiency is relatively insensitive to the value used during drafting (Table 1). To balance\noverall efficiency with framework simplicity, we fix this hyperparameter for our MAB auto-tuner.\nMoreover, the optimal configurations for speculative decoding also vary with input batch size (Table 2). Although\nthe speedup decreases as batch size grows, the model still\nbenefits substantially from SD even at batch size 32. Another\nkey observation is that Tokens_to_Verify should be adjusted based on the batch size: larger batches achieve optimal\nperformance when fewer tokens are verified. These findings\nfurther demonstrate the effectiveness of our BEG MAB Selec\ntor in identifying appropriate configurations across dynamic\nrunning request numbers.\n**Case Study for Adaptive SD** . Figure 14 presents a case\nstudy of the rollout profile during an RL step with 128 requests using Qwen-32B on H100 GPUs. Instead of consistently applying SD throughout the entire rollout process,\nleading to potential slowdown during the early phase when\n\n10\n\n**Table 2. Effect of batch sizes on speculative decoding.**\nSpeedup numbers are benchmarked using Qwen-32B on\nH100 GPUs (TP=4), with Draft_Depth=10 and topK=8.\n\n**# Tokens to Verify**\n**Speedup**\n\n16 32 48 64\n\n![](papers/images/Taming-the-LongTail-Efficient-Reasoning-RL-Training-with-Adaptive-Drafter.pdf-10-1.png)\n\nBatch Size = 1 3.22\u00d7 3.46\u00d7 3.56\u00d7 **3.62** \u00d7\n\n|Col1|Col2|Col3|\n|---|---|---|\n||Baseline<br>||\n||Rollout Ends<br>~~Adaptive SD (Ours)~~||\n||||\n|138<br>337<br>~~**SD starts**~~|138<br>337<br>~~**SD starts**~~|138<br>337<br>~~**SD starts**~~|\n||||\n\n**Figure 14. Rollout running-request profiling.** Example\nof processing 128 requests with and without adaptive decoding using Qwen-32B (TP=4).\n\nthe number of running requests is high, TLT switches to SD\nonly when the number of remaining requests drops below a\nthreshold (default 32), where SD becomes beneficial. Within\nthese SD-enabled regions, TLT further dynamically adapts\nSD configurations based on the current request count to\nmaximize throughput. Overall, TLT delivers a 2.44\u00d7 speedup\nover the baseline system without speculative decoding.\n**Bucketed CUDAGraph Capturing** . Preparing CUDAGraph\nfor multiple SD strategies can be memory-intensive. As\nshown in Table 3, naively capturing a separate graph for\neach of the four candidate strategies inflates memory usage from 7.81 GB to 30.39 GB. Our Bucketed CUDAGraph\ndesign addresses this problem by reducing graph memory\nconsumption to just 10.69 GB, a 2.8\u00d7 reduction compared\nto the naive method and only a marginal increase over the\nsingle static strategy, allowing the adaptive tuner to flexibly\nswitch between SD configurations for maximized efficiency.\n\n**6.4** **Evaluation of Spot Trainer**\n\n**Training Adaptive Drafter** . Figure 15 presents an example\ntraining curve illustrating the adaptive drafter\u2019s training\nprocess. After initial warm-up, the drafter\u2019s top-3 next-token\nprediction accuracy increases significantly. When the target\nmodel is updated after an RL step, the drafter\u2019s accuracy\ntemporarily dips due to the distribution shift caused by the\nupdate. However, this accuracy is rapidly recovered within\na few subsequent drafter training iterations, demonstrating\nthe effectiveness and robustness of our adaptive method.\n\n**Table 3. Bucketed CUDAGraph reduces memory foot-**\n**print.** Numbers measured using Llama-3-8B on H100 GPUs\n(TP=4). Search space includes 4 different strategies.\n\n**Method** **Memory Footprint**\n\nSingle Strategy 7.81 GB\n\n100\n\n# Drafter Training Steps\n\n|Col1|Col2|Col3|Col4|\n|---|---|---|---|\n|Draft Model Accuracy|Draft Model Accuracy|Draft Model Accuracy||\n|Draft Model Accuracy|Draft Model Accuracy|Draft Model Accuracy|Draft Model Accuracy|\n||Target Model Training Step<br>Target Model Update|||\n|2K<br>4K<br>6K<br>8K<br>10K<br>12K<br>14K<br>|2K<br>4K<br>6K<br>8K<br>10K<br>12K<br>14K<br>|2K<br>4K<br>6K<br>8K<br>10K<br>12K<br>14K<br>|2K<br>4K<br>6K<br>8K<br>10K<br>12K<br>14K<br>|\n\n**Figure 15. Accuracy of the draft model during adaptive**\n**training.** The adaptive drafter\u2019s top-3 accuracy shows a\nconsistent upward trend. While target model updates cause\nminor dips, the drafter quickly adapts and recovers.\n\n**Effectiveness of Adaptive Drafter** . Table 4 highlights the\neffectiveness of adaptively training the draft model. With\nthis adaptive training mechanism, TLT consistently achieves\nhigher average accept lengths for the Target-R model across\nboth RL- ~~trainin~~ g tasks and downstream evaluations, demonstrating improved alignment and performance.\nFigure 16 further corroborates this finding, which shows\nthe acceptance probability at different token positions within\nthe drafted sequence. Due to the output distribution shift of\nthe target model and error accumulation, the vanilla draft\nmodel struggles to accurately predict tokens several steps\nahead, severely limiting the effective accept length. The adaptive drafter, however, sustains higher acceptance probabilities across these positions, showcasing its ability to mitigate\nevolving target model and maintain longer accept lengths.\n**Efficient Spot Trainer** . Spot Trainer achieves preemptible,\nhigh-throughput training with negligible runtime overheads.\nAmong its optimizations, Selective Asynchronous Checkpointing (Figure 17 (a)) facilitates the trainer\u2019s preemptible\ndesign without significant work loss. Compared to the vanilla\nsynchronous design, our approach reduces the checkpointing latency by 9.2 \u00d7 through both offloading the save process\nto a background thread and selectively dumping only the\ntrainable parameters of the draft model. Figure 17 (b) demonstrates the effectiveness of sequence packing. This technique\neliminates wasteful computation on padding tokens by concatenating variable-length sequences, improving training\nthroughput by 2.2\u00d7 over vanilla batching.\n**System Overhead Analysis** . We identify three primary\nsources of overhead in TLT: (1) Stage-transition overhead,\nintroduced by additional the drafter-weight update and optimizer offloading compared to VeRL [53]. This accounts for\nless than 1% of the RL-step duration due to the much smaller\n\n11\n\n**Table 4. Effectiveness of Adaptive Drafter.** Adaptive\ndrafter maintains alignment with the target model. TargetBase denotes Qwen2.5-7B; and Target-R denotes the model\nafter RL. Downstream indicates the collection of math, coding and reasoning domains in MTBench [69].\n\n**RL Training** **Downstream**\n\nTarget-Base Target-R Target-Base Target-R\n\nAccept Length 4.59 6.53 3.76 5.15\n\n![](papers/images/Taming-the-LongTail-Efficient-Reasoning-RL-Training-with-Adaptive-Drafter.pdf-11-0.png)\n\n|280|ms|893|ms<br>32|Col5|Col6|\n|---|---|---|---|---|---|\n|**280**|**ms**||**92**<br>**.x**|||\n||||**.x**|||\n\n|893|ms|22x|Col4|\n|---|---|---|---|\n|** sam**|** ples/sec**|**.**||\n\n![](papers/images/Taming-the-LongTail-Efficient-Reasoning-RL-Training-with-Adaptive-Drafter.pdf-11-2.png)\n\n![](papers/images/Taming-the-LongTail-Efficient-Reasoning-RL-Training-with-Adaptive-Drafter.pdf-11-1.png)\n\n**Figure 16. Token accept rate of different draft models** .\nOur adaptive draft model achieves prominently higher accept\nrate when predicting distant tokens in rollout scenario.\n\nmodel size. (2) SD switch overhead, incurred when switching from normal decoding to SD. Because the draft model is\ninitially inactive for large batch sizes, we perform a reprefill step to activate SD, which completes within 3 seconds.\n(3) Drafter-training coordination overhead, caused by occasional contention during spot-trainer execution. However,\nwe find that it is unnecessary to train the drafter every RL\nstep, performing once every 10 steps is sufficient to maintain accuracy. Overall, the performance gains of TLT far\noutweigh this overhead, yielding end-to-end speedups."}, {"role": "assistant", "content": "**Analogy:** Imagine you're trying to build a really tall LEGO tower, but you only have a picture of the finished tower, not instructions for each step. Usually, you'd pick one block, put it down, and then think really hard about the next one. This paper is like having a super-fast little helper friend who can guess the next few LEGO blocks in a row and quickly snap them together for you. You just need to check if they're right. If they are, great! You save a lot of time. If they're wrong, you take them off and put the right one on yourself. The clever part is, your helper friend learns over time to guess better and better by watching what blocks you actually use, and you only ask your helper to guess when it's most useful, like when you're almost done with a section.\n\n**Explanation:** This paper talks about making a special computer brain, called an AI, learn much faster without making it less smart. The AI is learning to solve tricky problems, like math puzzles. The usual way is slow because the AI has to think very hard for each tiny step. So, they made a 'helper brain' (the drafter) that can quickly guess many steps ahead. The main AI then just quickly checks if the helper's guesses are good. If they are, it saves a lot of time. The helper brain also watches the main AI to get better at guessing, so it keeps learning. They also found smart ways to make the computer hardware work together like a super-organized team, making everything happen much quicker, especially for the parts that used to take a long time."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\nThe rapid progress of artificial intelligence (AI) has placed unprecedented demands on efficient GEMM computations. In GEMM,\nwe perform matrix multiplication _\ud835\udc36_ = _\ud835\udc34_ \u00d7 _\ud835\udc35_, where _\ud835\udc34_ \u2208 R _[\ud835\udc40]_ [\u00d7] _[\ud835\udc3e]_,\n_\ud835\udc35_ \u2208 R _[\ud835\udc3e]_ [\u00d7] _[\ud835\udc41]_, and _\ud835\udc36_ \u2208 R _[\ud835\udc40]_ [\u00d7] _[\ud835\udc41]_, with _\ud835\udc40_, _\ud835\udc41_, and _\ud835\udc3e_ denoting the number\nof rows of _\ud835\udc34_, the number of columns of _\ud835\udc35_, and the reduction dimension, respectively. GEMM efficiency relies critically on effective\ntiling strategies, which increase arithmetic intensity by reusing\ndata to reduce total data movement to and from memory. As a\nresult, tiling has become a standard practice across CPUs, GPUs,\nand domain-specific accelerators, enabling high performance while\nrespecting tight memory-bandwidth constraints.\nConventional practice is to employ _symmetric tile buffering_, in\nwhich the buffered tile size of input _\ud835\udc34_ along the _\ud835\udc40_ dimension and\ninput _\ud835\udc35_ along the _\ud835\udc41_ dimension are the same as the output tile\nsize of _\ud835\udc36_ . This uniform mapping, defined by the three parameters ( _\ud835\udc47\ud835\udc40,\ud835\udc47\ud835\udc3e,\ud835\udc47\ud835\udc41_ ), simplifies scheduling and implementation and is\nwidely adopted in CPU, GPU, and NPU software libraries, as well\nas hardware accelerator designs [19\u201321, 23]. Despite its ubiquity,\nsymmetric buffering increases buffer requirements and leaves arithmetic intensity below the theoretical maximum (Fig. 1, right), a\nlimitation that is particularly restrictive when targeting narrow\nmemories such as L1 caches or scratchpads.\nHowever, we show that this symmetry is unnecessary for tiling.\nIn principle, the buffer lifetime of input _\ud835\udc34_ is only as long as it takes\nto accumulate a single row of _\ud835\udc36_ ; once that row is complete, the\ncorresponding row of _\ud835\udc34_ can be safely evicted. This observation\nimplies that the input and output tile buffers do not need to share\nthe same _\ud835\udc40_ dimension, opening the opportunity to reduce the\nbuffering requirements for _\ud835\udc34_ and allocate more capacity to _\ud835\udc35_ and\n\n![](papers/images/Can-Asymmetric-Tile-Buffering-Be-Beneficial.pdf-0-0.png)\n\n**Figure 1: Architecture and Roofline: AMD AIE XDNA2\u2122spa-**\n**tial architecture (left) and the roofline view illustrating the**\n**performance impact of kernel and tiling strategies (right).**\n\n_\ud835\udc36_ . This insight raises the central question of this work: _When and_\n_how can asymmetric tile buffering be beneficial?_ To answer it:\n\n  - We introduce ATB, a general tiling strategy that decouples\nthe buffered _\ud835\udc40_ dimensions of _\ud835\udc34_ and _\ud835\udc36_, reducing input buffer\npressure and increasing arithmetic intensity.\n\n  - We develop an analytical performance model that captures\nboth the benefits of ATB (higher reuse and arithmetic intensity) and its costs (more kernel switching overheads),\nproviding principled guidance for selecting tiling factors.\n\n  - We present the first case study applying ATB to AMD NPU,\nachieving a speedup of up to 4.54\u00d7 over the state of the art.\n\n  - We analyze ATB\u2019s performance on AIE and show that it\nimproves GEMM throughput by up to 40% even over highly\noptimized symmetric kernels."}, {"role": "assistant", "content": "**Analogy:** Imagine you're building a giant Lego castle (that's your 'C' tower) using two different kinds of special Lego bricks: 'A' bricks and 'B' bricks. You usually keep big piles of both 'A' and 'B' bricks in your play area so you can quickly grab what you need. But sometimes, you only need a few 'A' bricks for a very short time to make one part of the castle wall, while the 'B' bricks are needed for much longer to build many parts. If you keep the 'A' pile big just because the 'C' castle part is big, you're wasting space.\n\n**Explanation:** This paper says, 'What if we only kept a tiny pile of 'A' bricks, just enough for the small part we're building right now, and then quickly swap them out for new 'A' bricks? This frees up a lot of space for the 'B' bricks, which we need to hold onto for longer!' By doing this, we can fit more of the important 'B' bricks in our play area at once, so we don't have to keep running to the big storage box as often. This makes building the castle much faster!"}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n_\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61_\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5f\ud835\udc63\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b_ **then**\n\n**167** _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ = _\ud835\udc5a\ud835\udc4e\ud835\udc65_ ( _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51, \ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61\ud835\udc60_ _ _\ud835\udc56\ud835\udc5b_ _ _\ud835\udc53\ud835\udc59\ud835\udc56\ud835\udc54\u210e\ud835\udc61_ + _\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61\ud835\udc60_ _ _\ud835\udc51\ud835\udc52\ud835\udc59\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc51_ ) ;\n\n**168** BBR_Modulate_Cwnd_For_ProbeRTT():"}, {"role": "assistant", "content": "**Analogy:** Imagine you are sending toy cars to your friend across a long road. You have a special traffic cop who decides how many cars can be on the road at one time.\n\n**Explanation:** This part tells the traffic cop: 'Hey, look at all the cars that are already driving to your friend's house, and also count the cars that just safely arrived. Make sure you let at least that many cars onto the road! Don't make the road empty if it can hold more!' This helps keep the road busy so your friend gets cars quickly. Then, the next part is like sometimes sending just one tiny car very slowly to see how long it *really* takes to get to your friend, just to check if the road is clear and fast or if there are secret delays."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n_\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61.\ud835\udc5f\ud835\udc61\ud835\udc61_ \u2265 0 **and** ( _\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61.\ud835\udc5f\ud835\udc61\ud835\udc61_ \u2264 _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc45\ud835\udc47\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc5d_ **or** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5f\ud835\udc61\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc5d_\ud835\udc52\ud835\udc65\ud835\udc5d\ud835\udc56\ud835\udc5f\ud835\udc52\ud835\udc51_ ) **then**\n\n**94** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc45\ud835\udc47\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc5d_ = _\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61.\ud835\udc5f\ud835\udc61\ud835\udc61_ ;\n\n**95** _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc5f\ud835\udc61\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc5d_ _ _\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5a\ud835\udc5d_ = _\ud835\udc41\ud835\udc5c\ud835\udc64_ () ;\n\n**96** BBR_Check_Probe_RTT():"}, {"role": "assistant", "content": "**Analogy:** Imagine your computer is like a little detective sending tiny paper airplanes to a friend and waiting for them to throw it back. The time it takes is how long it takes to go there and back. Your computer wants to remember the *fastest* time ever, so it always knows the quickest way.\n\n**Explanation:** This computer is like a little detective trying to find the speediest way for its messages to travel. It remembers the fastest time a message ever came back, like remembering the quickest way to run to the swings and back. But because things can change, it also peeks every now and then to see if there's a new even faster way or if the old fastest way is still good. If it finds a new fastest way, it writes it down in its special notebook!"}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\nEvolution Strategies (ES) (Rechenberg, 1978; Beyer, 1995; Beyer & Schwefel, 2002) are an attractive\nalternative to first-order methods based on gradient backpropagation, for several reasons. First, ES does not\nrequire differentiability, so it can optimise a broader class of models, like models with discrete parametrisation\nspaces (cellular automata), and can optimize objectives where gradients are unavailable or noisy, like outcomeonly rewards in LLM fine-tuning (Qiu et al., 2025). Second, ES is more robust to noisy and ill-conditioned\noptimisation landscapes (Wierstra et al., 2011; Xue et al., 2021). Unlike gradients, population-based exploration\nsmooths irregularities (Salimans et al., 2017), tolerates discontinuities, and mitigates issues like ill-conditioned\ncurvature or vanishing and exploding gradients in long-range or recurrent settings (Hansen, 2023). Third,\nES is highly amenable to scaling through parallelisation, since fitness evaluations are independent across\npopulation members and require only the communication of scalar fitnesses, which maps cleanly onto modern\ninference infrastructure and yields near-linear speedups on large clusters (Salimans et al., 2017). By contrast,\nbackpropagation requires communicating and aggregating gradients across devices, yielding updates with high\nmemory and computation costs. Additionally, backpropagation requires special care when training models\nwith low-precision datatypes, whereas ES can directly optimize any model with the same datatypes used\nat inference time. Together, these properties position ES as a potentially powerful foundation for training\nlarge, discrete, or hybrid architectures, and end-to-end systems with non-differentiable components, including\nlarge language models (LLMs) (Brown et al., 2020; Chowdhery et al., 2023; Du et al., 2022; Fedus et al.,\n2022).\n\nDespite this potential, there are practical obstacles to employing ES at scale. In deep learning architectures\n(Goodfellow et al., 2016), the majority of trainable parameters form linear mappings represented by matrices\n(Rosenblatt, 1962; Hochreiter & Schmidhuber, 1996; Bengio et al., 2000; Krizhevsky et al., 2012; Goodfellow\net al., 2014; Kingma & Welling, 2014; Vaswani et al., 2017); na\u00efvely adapting ES therefore requires generating\nfull-rank matrix perturbations that replicate the entire parameter set for every population member. This inflates\nmemory costs and forces frequent movement of large weight tensors. Evaluating these perturbations then\nrequires a separate sequence of matrix multiplications per member, so the total compute and wall-clock\ntime scale roughly with the population size and sequence length. In billion-parameter regimes, these two\ncosts dominate, making it difficult to scale ES beyond small models or small populations (Qiu et al., 2025;\nKorotyshova et al., 2025).\n\nTo mitigate both memory and computational bottlenecks, we introduce Evolution Guided General Optimization\nvia Low-rank Learning (EGGROLL), an ES algorithm that allows for the efficient training of neural network\narchitectures with billions of parameters. Analogous to LoRA\u2019s low-rank adapters in gradient-based training\n(Hu et al., 2022), EGGROLL generates _low-rank_ parameter-space perturbations for ES: instead of sampling\na full-rank matrix _E \u2208_ R _[m][\u00d7][n]_, we sample _A \u2208_ R _[m][\u00d7][r]_ and _B \u2208_ R _[n][\u00d7][r]_ with _r \u226a_ min( _m, n_ ) and form\n_E_ = ~~_\u221a_~~ 1 _r_ _AB_ _[\u22a4]_ . This reduces auxiliary perturbation matrix storage from _mn_ to ( _m_ + _n_ ) _r_ per layer, and\nproportionally reduces tensor movement. Moreover, we use a counter-based deterministic random number\ngenerator (RNG) (Salmon et al., 2011; Bradbury et al., 2018) to reconstruct noise on demand, so matrix\n\n2\n\nperturbations need not persist in memory. When evaluating the fitness of members of multiple perturbations\nin parallel, EGGROLL batches a population of low-rank adapters and shares the base activations, enabling\na single forward pass that applies all _AB_ _[\u22a4]_ updates via specialized batched matrix multiplications. The\ncompute required beyond standard batched inference scales with _O_ ( _r_ ( _n_ + _m_ ) _N_ ), where _N_ is the population\nsize, instead of _O_ ( _nmN_ ) for standard ES, yielding substantial memory and inference savings and making\nEGGROLL efficient for extremely large models. We emphasise that our method does not restrict the updates\nto be low-rank: the overall EGGROLL update is an average of rank _r_ matrices across the population, making\nthe matrix parameter update rank min( _Nr, m, n_ ) .\n\nEGGROLL is described in detail in Section 4. We provide a rigorous theoretical analysis of the low-rank\napproximation accuracy in Section 5, proving that EGGROLL updates converge to the full rank Gaussian\nES updates at an _O_ ( [1] _/r_ ) rate. This fast convergence rate suggests that low-rank updates are sufficient to\ntrain large-scale architectures, even with _r \u226a_ min( _m, n_ ). In our extensive empirical evaluation, we test this\nhypothesis across a wide range of domains in Section 6. In tabula rasa and multi-agent RL (MARL) settings,\nwe show that EGGROLL does not compromise performance compared to na\u00efve ES despite being faster.\nTo demonstrate the scalability of our method for LLM fine-tuning, we conduct experiments on pretrained\nRWKV7 (Peng et al., 2025) models, modern recurrent language models that enable large batch inference\ndue to their constant state size. Finally, we develop a nonlinear RNN language model that operates purely in\ninteger datatypes, and demonstrate that EGGROLL can stably pretrain this language model, a feat which is\nonly feasible due to the large population sizes enabled by EGGROLL."}, {"role": "assistant", "content": "**Analogy:** Imagine you want to build the best LEGO castle ever, and you try to make it better by changing little things. Most ways of trying would mean building a whole new giant castle every time you want to change just one block, which takes up all the floor space and takes forever! EGGROLL is like a magic way to change only tiny parts of your super-big LEGO castle. Instead of rebuilding a huge wall, you just swap out a small special window or a different flag on top. This is super quick!\n\n**Explanation:** We want to find the best way to make something work really well, like finding the best way for a smart computer program to talk. Sometimes, we have to try many slightly different versions to see which one is best. This is called Evolution Strategies (ES). It's like having many friends build slightly different toy robots, and you pick the coolest one. But if the robots are super-duper big, making even a tiny change means you have to make a whole new giant robot, which takes up too much room and takes too long to make and test. EGGROLL is a clever trick! Instead of making big new robot copies, it only changes tiny pieces of the robot at a time. So, if you want to make an arm different, you just swap the arm, not the whole robot. And because the changes are so small, EGGROLL also has a secret way to remember them without keeping giant copies everywhere. Plus, it can get lots and lots of friends to try out many tiny robot part changes all at the same time, making it super fast to find the very best robot!"}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n_All proofs for theoretical results in this paper are found in the Appendix._\n\n**2.1** **Low Rank Matrix Approximations**\n\nWhen adapting high dimensional foundation models for specific tasks, updating the parameters using gradient\nbased methods has high memory requirements. LoRA uses low-rank approximations to the matrix multiplications to reduce these costs. For each matrix _Mi \u2208_ R _[m][\u00d7][n]_ in the model, a low-rank approximation can be made\nby decomposing each matrix:\n\n_Mi \u2248_ _Mi_ [0] [+] _[ A][i][B]_ _i_ _[\u22a4][,]_\n\nwhere _Mi_ [0] [:=][ StopGrad][(] _[M][i]_ [)][ is the imported matrix from the foundation model with frozen parameters and]\n_Ai \u2208_ R _[m][\u00d7][r]_ and _Bi \u2208_ R _[n][\u00d7][r]_ are low-width column matrices (i.e., _r \u226a_ min( _m, n_ )) whose parameters are\nupdated through gradient-based optimisation during task-specific adaptation. This reduces the number of\noptimisation parameters for each matrix from _mn_ to _r_ ( _m_ + _n_ ). In this paper, we use a similar low-rank matrix\napproximation for evolutionary strategies.\n\n**2.2** **Gaussian Matrix Distribution and Matrix Norms**\n\nIn this paper, we focus on evolution strategies that target _matrix parameters_ . Many variables we study are\nGaussian distributed. When working in matrix space, it is convenient to use the matrix Gaussian distribution\n(Dawid, 1981), which is defined directly over matrices _X \u2208_ R _[m][\u00d7][n]_ :\n\n1\n_N_ ( _M, U, V_ ) = ~~_mn_~~\n(2 _\u03c0_ ) 2 det( _U_\n\n_\u2212_ [1]\n2 [exp] \ufffd 2\n\n~~_n_~~ ~~_m_~~\n2 det( _U_ ) 2 det( _V_ ) 2\n\n[1] \ufffd _V_ _[\u2212]_ [1] ( _X \u2212_ _M_ ) _[\u22a4]_ _U_ _[\u2212]_ [1] ( _X \u2212_ _M_ )\ufffd [\ufffd] _,_\n\n2 [tr]\n\nwhere _M \u2208_ R _[m][\u00d7][n]_ is the mean matrix, _U \u2208_ R _[m][\u00d7][m]_ is the row covariance matrix and _V \u2208_ R _[n][\u00d7][n]_ is the\ncolumn covariance matrix. The matrix Gaussian distribution is a generalisation of the multivariate Gaussian\ndistribution _N_ ( _\u00b5,_ \u03a3) defined over vector space. Sampling a matrix _X \u223cN_ ( _M, U, V_ ) from a Gaussian matrix\ndistribution is equivalent to sampling a vector vec( _X_ ) _\u223cN_ ( _\u00b5,_ \u03a3) from a multivariate Gaussian distribution\nwith mean _\u00b5_ = vec( _M_ ) and covariance matrix \u03a3 = _V \u2297_ _U_ where _\u2297_ denotes the Kronecker product. For\nisotropic matrix Gaussian distributions with covariance matrices _U_ = _\u03c3_ [2] _Im_ and _V_ = _\u03c3_ [2] _In_, the equivalent\nmultivariate Gaussian distribution is also isotropic with \u03a3 = _\u03c3_ [2] _Imn_ . To measure distance between matrices,\nwe use the Frobenius norm:\n\n_\u2225M_ _\u2225F_ := _mi,j_ [2] _,_\n~~\ufffd\ufffd~~ _i,j_\n\n3\n\nwhich provides an upper bound on the matrix 2-norm (Petersen & Pedersen, 2008). We denote the _\u2113_ [2] vector\nnorm as _\u2225\u00b7\u2225_ .\n\n**2.3** **Evolution strategies**\n\nEvolution strategies (ES) (Rechenberg, 1978; Beyer, 1995; Beyer & Schwefel, 2002) is a set of blackbox\nmethods for optimising general systems. ES has emerged as a useful alternative to gradient-based methods,\nparticularly when a system is noisy or non-differentiable. Our problem setting focuses on fitness functions\nwhose parameters are matrices: our goal is to find a matrix _M_ _[\u22c6]_ _\u2208_ R _[m][\u00d7][n]_ that maximises the fitness function\n_M_ _[\u22c6]_ _\u2208_ arg max _M_ _\u2208_ R _m\u00d7n f_ ( _M_ ). In comparison to gradient-based methods which use derivatives of the\nfunction _f_ ( _M_ ) to update the parameters _M_ directly, evolutionary methods update a population distribution\nover the parameter space. This is achieved by learning a set of parameters _\u03b8_ to maximise the expected fitness\n_f_ ( _z_ ) under a population distribution _\u03c0_ ( _M_ _|\u03b8_ ):\n\n_J_ ( _\u03b8_ ) = E _M_ _\u223c\u03c0_ ( _M_ _|\u03b8_ ) [ _f_ ( _M_ )] _._ (1)\n\nTaking derivatives of _J_ ( _\u03b8_ ) yields the gradient:\n\n_\u2207\u03b8J_ ( _\u03b8_ ) = E _M_ _\u223c\u03c0_ ( _M_ _|\u03b8_ ) [ _\u2207M_ log _\u03c0_ ( _M_ _|\u03b8_ ) _f_ ( _M_ )] _,_\n\nwhich is used to update the population distribution\u2019s parameters using (stochastic) gradient ascent with a\nsuitable stepsize _\u03b1t_ :\n\n_\u03b8t_ +1 _\u2190_ _\u03b8t_ + _\u03b1t\u2207\u03b8J_ ( _\u03b8t_ ) _._ (2)\n\n_\u2207M_ log _\u03c0_ ( _M_ _|\u03b8_ ) is known as the score function, which avoids taking gradients directly through the fitness\nfunction in Eq. (1).\n\n**2.4** **Gaussian Matrix ES**\n\nIn this paper, we study ES using Gaussian policies: _\u03c0_ ( _M_ _|\u03b8_ ) = _N_ ( _\u00b5, Im\u03c3_ [2] _, In\u03c3_ [2] ). In addition to its\nmathematical convenience, the central limit theorem means that the Gaussian distribution emerges naturally\nfrom a low-rank approximation as rank increases, even if the matrices _A_ and _B_ are themselves non-Gaussian.\nWe assume that _\u03c3_ [2] is fixed and the ES algorithm optimises over the mean matrix _\u03b8_ = _\u00b5_ which acts as a proxy\nfor the true maximum of the fitness function. By adapting well-known derivations (Wierstra et al., 2011) to the\nGaussian matrix case, we derive the ES gradient for our problem setting:\n**Proposition 1.** _Using the Gaussian matrix policy \u03c0_ ( _M_ _|\u00b5_ ) = _N_ ( _\u00b5, Im\u03c3_ [2] _, In\u03c3_ [2] ) _the ES objective can be_\n_written as:_\n\n_J_ ( _\u00b5_ ) = E _E\u223cP_ ( _E_ ) [ _f_ ( _M_ = _\u00b5_ + _\u03c3E_ )] _._ (3)\n\n_and the matrix gradient of the ES objective is:_\n\n_\u2207\u00b5J_ ( _\u00b5_ ) = _\u2212_ [1] (4)\n\n_\u03c3_ [E] _[E][\u223c][P]_ [ (] _[E]_ [)][ [] _[E][ \u00b7][ f]_ [(] _[M]_ [ =] _[ \u00b5]_ [ +] _[ \u03c3E]_ [)]] _[ .]_\n\n_where P_ ( _E_ ) _is a zero-mean standard normal p_ ( _E_ ) = _N_ (0 _, Im, In_ ) _. Moreover, the natural matrix gradient is_\n_equal to \u03c3_ [2] _\u2207\u00b5J_ ( _\u00b5_ ) _, i.e. equivalent to Eq._ (4) _up to a factor of \u03c3_ [2] _._\n\nWe see from Eq. (4) that Gaussian matrix ES methods optimise the objective _J_ ( _\u00b5_ ) by generating search\nmatrices _E \u223c_ _P_ ( _E_ ) from a standard matrix normal distribution _N_ (0 _, Im, In_ ) around the parameter matrix\n_\u03b8_ = _\u00b5_ .\n\nWe remark that for Gaussian population distributions, the ES update in Eq. (4) is equal to the natural evolution\nstategy (NES) update up to a factor of _\u03c3_ [2] . NES (Wierstra et al., 2008; 2011) updates follow the _natural gradient_\n(Amari, 1998; Kakade, 2001) of the objective in Eq. (1). This means that for our problem setting where _\u03c3_ is\nassumed fixed and is absorbed into the stepsize in Eq. (2), Gaussian matrix ES and NES are equivalent. A key\nbenefit of the natural gradient is that it takes into account the local geometry of the underlying parameter space\nwhen updating the search distribution, making the updates invariant to the choice of parametrisation.\n\n4"}, {"role": "assistant", "content": "**Analogy:** Imagine you have a giant LEGO castle (a big computer model) that you want to make a little bit better for a specific game. Instead of changing every single LEGO brick (which is super hard and takes forever), you only change a small, special section of new, easy-to-change LEGO bricks you add to the castle. These special bricks are the only ones we touch.\n\n**Explanation:** To find the best way to change these special new LEGO bricks, we don't just guess. We have a team of friends. Each friend tries a slightly different way to build their small LEGO part around the main castle. We see which friend's LEGO part makes the castle work best for the game. Then, we tell everyone to build their next small LEGO part more like the friend who did best. We keep doing this until the castle is super good. The way we tell our friends where to try next is like how sand falls from a bucket - most friends try near the best spot, and fewer try far away, making sure we don't miss a good idea."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n**1**\n\n1.1 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n\n1.2 Previous work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n\n1.3 Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5"}, {"role": "assistant", "content": "**Analogy:** Imagine you have a big treasure map for a storybook. This part is like a tiny map at the very beginning that tells you where to find the 'cool toys' (results), where to find out what other kids built before (previous work), and how the whole treasure hunt is put together (organization).\n\n**Explanation:** This part of the paper is like a little list at the front of a book that tells you which page to turn to for different parts of the story. It shows you where the 'good news' about what they found is, where they talk about what other smart people did, and how they put the whole paper together."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\nWe evaluate our method on various datasets, including ImageNet-1k [12] and ImageNet-100 [42] for\nour primary results, Spawrious [26] and Waterbirds [38] for a study on adversarial datasets, Stanford\nDogs [21] and CUB-200-2011 [45] for fine-grained visual classification, and ArtBench [24] to test\nthe method\u2019s out-of-distribution capabilities. The majority of our experiments use four pre-trained\nself-supervised feature extractors as backbones: CLIP [34], DINO-v2 [31], EVA-02 [14], and MoCov3 [10]. In all our experiments, we distill the given dataset for 5000 iterations before training linear\nprobes to convergence on the resulting synthetic images. All experiments are conducted at 224 _\u00d7_ 224\nresolution and use the \u201cViT-B\u201d version of the given model. All distilled datasets by default use 10\nsets of augmentations per batch except for ImageNet-1k, for which only 3 sets of augmentations are\nused due to compute constraints. For further implementation details, please see the Appendix.\n\n**Evaluation and Baselines.** To measure our method\u2019s performance on a given feature extractor,\nwe randomly initialize a linear classifier and optimize it to convergence using the distilled images\nbefore evaluating on the test set. The same procedure is used to evaluate real-image baselines. In\nprior works on self-supervised learning [31], the evaluation consists of a grid search across a number\nof hyper-parameters, including from which layer(s) the features used to train the linear probe should\nbe taken from. For simplicity, we instead use only the features from the backbone\u2019s _last layer_ and\nkeep the training hyper-parameters consistent across the training of all linear probes.\n\nWe compare our method against three real-image baselines. For the **Neighbors** baseline, we choose\nthe real image for each class whose _distill-model_ embedding is closest to that of the corresponding\nsynthetic image produced by our method. Similarly, for **Centroids**, we take the real training image\nwith embedding closest to the _mean_ embedding for each class. Lastly, for **Random**, we simply select\na random image for each class and average the performance over 10 different seeds.\n\n5\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-4-0.png)\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-5-1.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-5-0.png)\n\nFigure 3: Performing more rounds of differentiable\naugmentation on the synthetic data during each distillation step improves both the single-model and crossmodel performance of the distilled images.\n\n|Distil<br>Centr|Col2|Col3|Distil<br>Centr|\n|---|---|---|---|\n|30<br>20<br>10|0|10<br>20|10<br>20|\n\nFigure 4: We distill ImageNet-Fruits and observe the\nPCA of the training image embeddings. Each color\nrepresents a class. Note that the distilled images typically lie on the edge or outside of their class\u2019s cluster.\n\n**4.1** **Linear Gradient Matching Out-Performs Real-Image Baselines**\n\nFirst, we evaluate our method on the original dataset distillation task: using a backbone model\nto distill a dataset and then training a model _of the same architecture_ on the resulting synthetic\nimages. Note that this setting differs from previous dataset distillation works in that we use a\n_pre-trained backbone_ distillation and reuse that backbone during evaluation by training a new\nrandomly-initialized linear classifier on top of it.\n\nIn Table 1, we see results for the single-model setting when distilling ImageNet-100 and ImageNet-1k.\nAcross both datasets and all four models, **our method convincingly out-performs all real-image**\n**baselines** . In particular, our method enables a linear probe trained on top of DINO-v2 [31] to reach\n**75% test accuracy** on ImageNet-1k while only ever having seen a **single labeled image per class** .\nIn comparison, training on the full dataset of 1.3 million real images reaches just 7 points higher at\n83%. Similar results are seen for the other models and datasets as well.\n\nIn Figure 4 we visualize the _embeddings_ of the distilled images relative to the real training data\nby plotting the 2D principal component analysis (PCA). For the sake of visual clarity, we use\nImageNet-Fruits [6, 7], a toy dataset of 10 fruit classes from ImageNet-1k. The embeddings in this\nfigure are from the same model used to distill the dataset (DINO-v2). We observe that the embeddings\nof the distilled images tend to lie far away from their respective class\u2019s centroid, often falling on\nthe outside edge of the class\u2019s distribution. We hypothesize that this is due in-part to the distillation\nembedding _highly-discriminative_ features within the synthetic images.\n\n**4.2** **Distilled Images Generalize Across Models**\n\nNext, we investigate the cross-model generalization capabilities of our distilled datasets. That is, we\nfirst distill a dataset using one backbone model (e.g., CLIP) and then evaluate the synthetic images\u2019 performance on the _other_ models (DINO-v2, EVA-02, MoCo-v3) that were not seen during distillation.\n\nWe visualize these results in Table 2. The diagonal elements are equivalent to the first row of Table 1\nand represent single-model performance while the off-diagonals show cross-model capabilities. We\nsee that the distilled datasets tend to generalize well to unseen models save for an outlier between\nCLIP and Moco, possibly due to poor model _alignment_ (Section 4.6). We also observe that the highestaccuracy model on the full dataset (DINO) also has the distilled dataset with the best cross-model performance, suggesting that a model\u2019s quality affects the generalization capabilities of its distilled data.\n\n**4.3** **Regularization and Augmentation Enable High Performance**\n\nIn this section, we quantitatively (Figure 3) and qualitatively (Table 5) analyze the effects of the\nvarious regularization and augmentation techniques included in our distillation process. Table 3 shows\nthe effects of ablating various components when evaluating using the same model used to distill ( **top** )\n\n6\n\nTrain Set ImageNet-100\n(1 Img/Cls)\n\nCLIP DINO-v2 EVA-02 MoCo-v3 Average\n\nFull (Ours) **84.9** _\u00b1_ **0.1 91.5** _\u00b1_ **0.1 89.0** _\u00b1_ **0.0 83.4** _\u00b1_ **0.1 87.2** _\u00b1_ **0.1**\n-Decorrelate 82.6 _\u00b1_ 0.1 **91.3** _\u00b1_ **0.2 89.0** _\u00b1_ **0.1** 83.2 _\u00b1_ 0.1 86.5 _\u00b1_ 0.1\n\n-Pyramid 83.5 _\u00b1_ 0.2 91.0 _\u00b1_ 0.3 87.9 _\u00b1_ 0.1 80.5 _\u00b1_ 0.1 85.7 _\u00b1_ 0.1\n-Augment 58.4 _\u00b1_ 0.2 82.6 _\u00b1_ 0.4 74.0 _\u00b1_ 0.3 59.6 _\u00b1_ 0.5 68.6 _\u00b1_ 0.4\n\nFull (Ours) **75.4** _\u00b1_ **0.3 80.9** _\u00b1_ **0.1** 76.6 _\u00b1_ 0.1 78.2 _\u00b1_ 0.1 **77.8** _\u00b1_ **0.2**\n-Decorrelate 69.4 _\u00b1_ 0.4 79.5 _\u00b1_ 0.1 **76.8** _\u00b1_ **0.1 79.9** _\u00b1_ **0.2** 76.4 _\u00b1_ 0.2\n\n-Pyramid 57.3 _\u00b1_ 0.2 74.5 _\u00b1_ 0.2 68.5 _\u00b1_ 0.2 68.1 _\u00b1_ 0.2 67.1 _\u00b1_ 0.2\n-Augment 35.3 _\u00b1_ 0.4 31.6 _\u00b1_ 0.2 34.3 _\u00b1_ 0.5 32.2 _\u00b1_ 0.5 33.3 _\u00b1_ 0.4\n\nFull Dataset 92.5 _\u00b1_ 0.0 95.2 _\u00b1_ 0.1 94.1 _\u00b1_ 0.1 89.4 _\u00b1_ 0.3 92.8 _\u00b1_ 0.1\n\nTable 3: **Evaluating Ablations:** While all three components\nprovide improvements, the Augmentation has the most dramatic effect, especially in the cross-model setting. Likewise,\nthe Pyramid optimization seems to matter more in the crossmodel setting than the same-model setting by mitigating overfitting to the model used during distillation.\n\nFigure 5: **Visualizing Ablations:** Removing\nvarious components of our pipeline causes\nvisual degradation in the distilled images.\n\nand averaging performances across the other three models ( **bottom** ). Figure 5 visualizes the distilled\nPineapple class from ImageNet-100 under the same ablations.\n\n**Color Decorrelation** Inspired by prior work in feature visualization [30] and motivated by a desire\nto spare our synthetic images from any color-related biases imposed by the model used to distill\nthem, we apply a color decorrelation technique to our distillation process. In Figure 5, we see that the\nimages distilled without the color decorrelation (column 2) look less realistic than those using our\nfull method (column 1); they are over saturated and contain high levels of incorrect colors (blue in\nthis example). Quantitatively, however, this component has the lowest effect of the three analyzed in\nthis section. While generally slightly helpful, it only offers a large improvement in the cross-model\nsetting when distilling using CLIP.\n\n**Pyramid Representation** It has been shown in prior work [7] that pixel-based optimization for\ndataset distillation does not scale well to higher resolutions; the synthetic images tend to become\nriddled with high-frequency patterns that inhibit their usefulness as training data. As discussed in\nSection 3.2, we instead adopt a pyramid representation for the distillation process. In Figure 5, we\nsee samples distilled from ImageNet-100 optimized using either the pyramid (column 1) or na\u00efve\npixel representation (column 3). We encourage the reader to zoom in and observe the high-frequency\npatterns and overall lack of coherence in the pixel-based images. In Table 3, we see that the images\ndistilled without the pyramid representation do indeed make far worse training data in the cross-model\nsetting, causing the model to overfit to the high-frequency patterns.\n\n**Differentiable Augmentations** Since its first proposal [53], the incorporation of differentiable\naugmentations during the distillation process has proven critical to the efficacy of the synthesized\nimages as training data. As such, we also perform differentiable augmentations in this work, as\ndescribed in Section 3.3. In Figure 5, we see samples from ImageNet-100 distilled both with (column 1) and without (column 4) augmentations. Visually, we observe that the images distilled _without_\naugmentations lack much meaningful structure and appear to simply be blobs of color with just a hint\nof geometry. In Table 3, we see that the lack of augmentations _severely_ limits the synthetic images\u2019\nusefulness as training data, even when evaluation on the same model used to distill.\n\nAs earlier discussed, we also apply _multiple rounds_ of augmentations at each distillation step and\nconcatenate the multiple augmented versions together to form the synthetic batch. Figure 3 illustrates\nhow the number of _augmentations per batch_ affects both the single-model and cross-model performance of the distilled data. Specifically, we see that when distilling ImageNet-100 with DINO-v2,\nraising the number of augmentations per batch from 1 to 5 and eventually 10 steadily increases the\ndistilled images\u2019 effectiveness as training data both on the backbone model used to distill (DINO-v2)\nand the other unseen models (CLIP, EVA-02, MoCo-v3).\n\n7\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-3.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-4.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-5.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-6.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-7.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-8.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-9.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-10.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-14.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-15.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-16.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-17.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-18.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-19.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-20.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-7-21.png)\n\nFigure 6: **Distilling Datasets with Spurious Correlations:** The 4-class \u201cSpawrious\u201d dataset contains spurious\nbackground correlations in the training set that are then subverted in the test set ( **left** ). A DINO linear\nprobe trained on the full training set performs well, reaching 78% test accuracy, while a MoCo probe fails\ncatastrophically, only reaching 36%. The distilled images ( **right** ) hint as to why: those distilled with DINO-v2\nstill contain mostly decipherable dog breeds while the MoCo-v3 counterparts focus almost entirely on the\nbackground environments. These images likely reflect the same biases held by the models used to distill them.\n\nTrain Set Spawrious WaterBirds\n(1 Img/Cls)\n\nCLIP DINO-v2 EVA-02 MoCo-v3 Average CLIP DINO-v2 EVA-02 MoCo-v3 Average\n\nDistilled (Ours) **43.1** _\u00b1_ **6.4 80.8** _\u00b1_ **3.1 36.5** _\u00b1_ **3.1 32.7** _\u00b1_ **2.3 48.3** _\u00b1_ **3.7 77.9** _\u00b1_ **0.2 82.1** _\u00b1_ **2.9 78.0** _\u00b1_ **0.8 77.8** _\u00b1_ **0.0 79.0** _\u00b1_ **1.0**\nNeighbors **41.7** _\u00b1_ **4.6 76.9** _\u00b1_ **1.2 40.7** _\u00b1_ **2.7 33.0** _\u00b1_ **3.4 48.1** _\u00b1_ **3.0 74.6** _\u00b1_ **6.5** 67.3 _\u00b1_ 5.8 **74.3** _\u00b1_ **3.3** 45.2 _\u00b1_ 11.4 65.3 _\u00b1_ 6.8\nCentroids **44.8** _\u00b1_ **5.0 80.2** _\u00b1_ **2.7 38.1** _\u00b1_ **3.3 30.5** _\u00b1_ **1.6 48.4** _\u00b1_ **3.2 69.8** _\u00b1_ **8.6** 65.7 _\u00b1_ 3.4 64.6 _\u00b1_ 7.9 62.0 _\u00b1_ 5.2 65.5 _\u00b1_ 6.3\n\nRandom **46.2** _\u00b1_ **4.4** 68.1 _\u00b1_ 8.1 **33.3** _\u00b1_ **5.3 31.8** _\u00b1_ **2.5 44.9** _\u00b1_ **5.1** 71.9 _\u00b1_ 4.5 57.1 _\u00b1_ 8.6 59.8 _\u00b1_ 13.3 67.4 _\u00b1_ 5.7 64.0 _\u00b1_ 8.0\n\nFull Dataset 50.4 _\u00b1_ 0.3 78.1 _\u00b1_ 0.1 50.1 _\u00b1_ 0.4 35.8 _\u00b1_ 0.0 53.6 _\u00b1_ 0.2 86.0 _\u00b1_ 0.1 95.5 _\u00b1_ 0.1 90.4 _\u00b1_ 0.2 74.3 _\u00b1_ 0.1 86.5 _\u00b1_ 0.1\n\nTable 4: **Performance on Datasets with Spurious Background Correlations:** The Spawrious ( **left** ) and Waterbirds ( **right** ) training sets contain intentionally adversarial background correlations that are then subverted in the\ntest sets. On Spawrious, our method no longer out-performs the real-image baselines as with the standard datasets\n(Table 1). This is perhaps due to the synthetic data adopting the models\u2019 biases and overfitting to the backgrounds\non the training sets. We also see interesting interpretability results in the images themselves (Figure 6).\n\n**4.4** **Distilling Adversarial Datasets Reveals Interpretable Model Weaknesses**\n\nThe \u201cSpawrious\u201d dataset [26] consists of four classes of dog breeds and is designed to evaluate a\nmodel\u2019s ability to focus on the relevant content of an image rather than spurious correlations. In the\ntraining data, each breed is paired with its own unique environment: (Bulldog, Forest), (Corgi, Desert),\n(Dachshund, Mountain), and (Labrador, Snow). However, in the test set, the environments are intentionally different: (Bulldog, Mountain), (Corgi, Forest), (Dachshund, Snow), and (Labrador, Desert).\nWe also experiment with a similarly constructed data, \u201cWaterbirds\u201d [38] consisting of just two classes\n(land birds and water birds) with similarly spurious background correlations.\n\nQuantitative results on these datasets are shown in Table 4. Our method generally outperforms the\nreal-image baselines, but to a significantly smaller extent than on the standard datasets (Table 1). We\nalso observe that when training on the full Spawrious dataset, DINO-v2 [31] _far_ out-performs the\nrest of the models (81% test accuracy) with MoCo-v3 [10] doing dramatically worse (37%), both to\ndegrees far greater than seen when training on standard datasets (as in Table 1).\n\nBy _distilling_ Spawrious with our method, we gain potential insights as to why we might see such\ndramatic results. As seen in Figure 6, the images distilled with DINO-v2 contain clear (albeit\nabstract) depictions of the correct dog breeds. On the other hand, those yielded by MoCo-v3 are\nalmost entirely indecipherable, with the only clear portions being the spurious backgrounds from the\ntraining set. This interpretability result gives a clue towards explaining MoCo-v3\u2019s poor performance\non this dataset; the model clearly seems to focus on the spurious background correlations present\nin the training set rather than the relevant subjects, causing catastrophic overfitting.\n\n**4.5** **Distillation Excels in Fine-Grained Visual Classification**\n\nWhile computer vision methods are typically benchmarked against datasets containing broad categories of objects, such as ImageNet [12], much research also focuses on the task of Fine-Grained\nVisual Classification (FGVC) wherein datasets consist of many closely related classes that can be\n\n8\n\nFigure 7: **Distilling Fine-Grained Datasets:** Our distillation method captures the details necessary to teach a\nclassifier to distinguish between highly similar classes. Pictured above are just 10 of the 120 classes distilled\nfrom the Stanford Dogs [21] dataset ( **top** ) and 10 of the 200 classes distilled from Caltech-UCSD Birds [45]\n[using DINO-v2. For the full distilled datasets, please see the Appendix or our project page.](http://georgecazenavette.github.io/linear-gm)\n\nTrain Set Stanford Dogs CUB-2011\n(1 Img/Cls)\n\nCLIP DINO-v2 EVA-02 MoCo-v3 Average CLIP DINO-v2 EVA-02 MoCo-v3 Average\n\nDistilled (Ours) **52.1** _\u00b1_ **0.2 83.0** _\u00b1_ **0.1 74.8** _\u00b1_ **0.1 69.6** _\u00b1_ **0.2 69.9** _\u00b1_ **0.2 62.2** _\u00b1_ **0.2 86.0** _\u00b1_ **0.1 74.1** _\u00b1_ **0.2 42.5** _\u00b1_ **0.2 66.2** _\u00b1_ **0.2**\nNeighbors 33.4 _\u00b1_ 0.1 71.3 _\u00b1_ 0.2 58.5 _\u00b1_ 0.2 56.3 _\u00b1_ 0.1 54.9 _\u00b1_ 0.2 39.4 _\u00b1_ 0.1 76.9 _\u00b1_ 0.0 52.6 _\u00b1_ 0.3 28.1 _\u00b1_ 0.0 49.2 _\u00b1_ 0.1\nCentroids 43.3 _\u00b1_ 0.1 73.0 _\u00b1_ 0.2 60.9 _\u00b1_ 0.2 55.2 _\u00b1_ 0.2 58.1 _\u00b1_ 0.2 54.3 _\u00b1_ 0.2 78.5 _\u00b1_ 0.2 59.9 _\u00b1_ 0.3 30.2 _\u00b1_ 0.1 55.7 _\u00b1_ 0.2\n\nRandom 23.3 _\u00b1_ 1.5 51.9 _\u00b1_ 1.8 38.3 _\u00b1_ 1.8 36.6 _\u00b1_ 1.4 37.5 _\u00b1_ 1.6 37.5 _\u00b1_ 1.6 64.4 _\u00b1_ 1.5 44.3 _\u00b1_ 1.5 19.1 _\u00b1_ 0.5 41.3 _\u00b1_ 1.3\n\nFull Dataset 76.9 _\u00b1_ 0.1 88.6 _\u00b1_ 0.1 82.6 _\u00b1_ 0.1 72.3 _\u00b1_ 0.5 80.1 _\u00b1_ 0.2 77.5 _\u00b1_ 0.7 90.2 _\u00b1_ 0.2 84.0 _\u00b1_ 0.3 43.7 _\u00b1_ 0.8 73.8 _\u00b1_ 0.5\n\nTable 5: **Performance on Fine-Grained Datasets:** Our distillation method captures the most discriminative\naspects of each class, thereby enabling a down-stream classifier to correctly identify samples from datasets\nwhere all classes are closely related. In particular, the performance gap between our method and the real-image\nbaselines is even higher on these fine-grained datasets (Stanford Dogs [21] and CUB-200-2011 [45]) than on the\nstandard ImageNet benchmarks.\n\nchallenging for even human experts to distinguish. Such datasets require models to learn fine-grained\nfeatures in order to identify the subtle differences between each class.\n\nWe apply our linear method to two common FGVC datasets: Stanford Dogs [21] and Caltech-UCSD\nBirds-200-2011 [45]. Samples of these datasets distilled using DINO-v2 can be seen in Figure 7. As\nseen in Table 5, in this more challenging fine-grained setting, our method outperforms the real-image\nbaselines to a _significantly higher_ degree than on the standard ImageNet benchmarks (Table 1). It\nseems as though the distillation process\u2019s ability to store didactic information in a single image\nper class is even more relevant in this setting since any one real image is less likely to contain the\ninformation necessary to distinguish its entire respective class from the other highly similar classes.\n\n**4.6** **Cross-Model Performance Predicts Model Alignment**\n\nRecent research has suggested an \u201calignment\u201d of various self-supervised models despite large\ndifferences in their training methods or even data modalities. The notion that all these models are\nindeed converging to a unified embedding space has been dubbed the \u201cPlatonic Representation\nHypothesis\u201d [20]. While the representations of today\u2019s models are not yet perfectly synchronized, it\nis still of interest to measure the degree to which they are aligned.\n\nIn the hypothesis\u2019s titular work, the authors introduce a method of measuring the alignment of two\nmodels called \u201cmutual _k_ nearest neighbors.\u201d In short, this method finds each sample\u2019s _k_ nearest\nneighbors in the embedding spaces of each model and then computes the fraction of neighbors that\nare shared between the two.\n\nWhen we distill images with a given model A and measure the 1 nearest neighbor performance\n(since there is just one image per class) on a different model B, we find that the normalized test\naccuracy is strongly correlated with the mutual _k_ -nn alignment between models A and B, as seen\nin Table 6. This is an interesting interpretability result showing that the cross-model performance of\na distilled dataset highly depends on the alignment between the two models. Given this dependency,\nour distillation method then acts as a method of _visualizing_ how well these models align by backing\nthe discrepancies in the embedding space out to image space. For example, given the two least\naligned models (CLIP and MoCo), we can see in Figure 1 that even for different sets of classes, the\ntwo models clearly distill very different _styles_ of images, alluding towards their misalignment.\n\n9\n\nDistill Normalized _k_ -NN Accuracy\nModel\n\nCLIP DINO-v2 EVA-02 MoCo-v3\n\nSource Model Alignment\nModel\n\nCLIP DINO-v2 EVA-02 MoCo-v3\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-9-0.png)\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-9-1.png)\n\nTable 6: **Distilled datasets predict model alignment.** We distill ImageNet-1k and find the synthetic data\u2019s\n_cross-model_ performance ( **left** ) by evaluating on a model other than the one used during distillation. We find that\nthis cross-model performance correlates well with the _alignment_ between models\u2019 embedding spaces ( **right** ).\nNote the similarity of the per-row trends between the two tables. Rows are colored from highest to lowest.\n\nFigure 8: **Distilling Out-of-Distribution Datasets:** Despite only ever being trained on the real-world images\nin ImageNet-1k [12], our method can still use DINO-v1 [5] to distill out-of-distribution datasets such as\nArtBench [24] that have no overlapping content. We see that the distillation is not simply \u201ccopying\u201d samples\nfrom the dataset by comparing to the nearest real neighbor in the model\u2019s embedding space and observing the\nstark differences. This interpretability insight granted by our method highlights DINO\u2019s remarkable ability to\ngeneralize beyond its training distribution.\n\n**4.7** **Self-Supervised Backbones can Distill Out-of-Distribution Datasets**\n\nNote that while we have thus far been discussing DINO-v2, this section in particular will reference\nDINO-v1 since this version\u2019s training set is open-sourced. Since DINO-v1 [5] was only trained on\nImageNet [12], a dataset of nearly all \u201creal-world\u201d images, we can easily test its ability to distill an\n**out-of-distribution** dataset such as ArtBench [24] that consists of 10 classes of art styles. In Figure 8,\nwe visualize the distilled images ( **top** ) as well as the nearest neighbor in DINO-v1\u2019s embedding\nspace. Surprisingly, despite having previously only ever seen \u201creal-world\u201d images from ImageNet,\nDINO-v1 is still able to effectively distill ArtBench into a single image per class. Furthermore, we\nconfirm that the distilling is not simply \u201ccopying\u201d real images from the training set by observing\nthe stark differences between the synthetic images and their nearest neighbors. This is especially\napparent in the cases of Realism, Renaissance, and Romanticism where the nearest neighbors are\nblack and white while the distilled images are colorful."}, {"role": "assistant", "content": "**Analogy:** Imagine you have a giant box of LEGOs, and you want to teach your friend how to build all sorts of cool things, like a dog, a cat, or a flower. Instead of giving them the whole, giant box for each thing, you make a tiny, super-special instruction card for each type of LEGO model. This instruction card is so good that your friend can build the perfect model with just one look, even if they've never seen that specific LEGO design before! Our special picture cards are like those super-instruction sheets.\n\n**Explanation:** We made special tiny pictures that are like super-teachers for computers. We used many different kinds of picture sets, like ones with dogs, birds, or even art. We also used different super-smart computer 'eyes' (like CLIP and DINO) to help make these special pictures. These tiny pictures are so good that they can teach a computer almost as well as giving it all the thousands of real pictures. Sometimes, these tiny pictures even help us see if a computer is getting confused, like when it only sees the grass instead of the dog! We also found out that if we make these tiny pictures extra special with colors and different shapes, they teach even better. And guess what? If one computer eye makes a special picture, another computer eye can often learn from it too, which is super cool!"}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n_\ud835\udc46\ud835\udc39\ud835\udc61\u210e\ud835\udc56\ud835\udc60_ /* Close _\ud835\udc46\ud835\udc39\ud835\udc61\u210e\ud835\udc56\ud835\udc60_ whose _\ud835\udc47\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59_ _ _\ud835\udc37\ud835\udc52\ud835\udc59_ _ _\ud835\udc45\ud835\udc61_ _< \ud835\udefd_ \u2217 _\u210e\ud835\udc56\ud835\udc54\u210e\ud835\udc52\ud835\udc60\ud835\udc61_ _ _\ud835\udc4f\ud835\udc64_ _ _\ud835\udc4e\ud835\udc5a\ud835\udc5c\ud835\udc5b\ud835\udc54_ _ _\ud835\udc4e\ud835\udc59\ud835\udc59_ _ _\ud835\udc46\ud835\udc39\ud835\udc60_\nfor five successive ProbeBw states */\n\n**Fulfilling Goal 2 (MPTCP fairness):**\n\n**22** _\ud835\udefc_ = 20, _\ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f_ _ _\ud835\udc5c\ud835\udc53_ _ _\ud835\udc46\ud835\udc39\ud835\udc60_ _ _\ud835\udc56\ud835\udc5b_ _ _\ud835\udc4f\ud835\udc61\ud835\udc59\ud835\udc5b\ud835\udc52\ud835\udc50\ud835\udc58_ = 0 ;"}, {"role": "assistant", "content": "**Analogy:** Imagine a big group of kids trying to get through a door. Some kids are super fast, like a superhero zooming past! Others are a bit slow, taking their time. If one kid is super, super slow for a long, long time, and much slower than the fastest kid, we might gently ask them to wait on the side for a bit so everyone else can get through more smoothly.\n\n**Explanation:** This helps make sure that everyone gets a fair turn and the path doesn't get stuck because of one slow-poke. It's like making sure all the toys are being played with fairly and efficiently."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n_\u2212_ 22 _\u2212_ 49\n\nLunarLander DDPG _\u2212_ 117 _._ 0 _\u00b1_ 45 _._ 3 _\u2212_ 29 _._ 9 _\u00b1_ 35 _._ 8 26 _._ 3 _\u00b1_ 98 _._ 4 **37** _._ **1** _\u00b1_ **78** _._ **4** \u2013 **1** _._ **0** 6 _._ 7 5 _._ 7 12 23 **27**\n\nPPO _\u2212_ **6** _._ **9** _\u00b1_ **132** _._ **3** _\u2212_ 19 _._ 0 _\u00b1_ 113 _._ 4 _\u2212_ 71 _._ 9 _\u00b1_ 87 _._ 3 _\u2212_ 40 _._ 6 _\u00b1_ 42 _._ 5 7 _._ 3 **6** _._ **7** 9 _._ 0 6 _._ 8 5 4 **28**\n\nTRPO _\u2212_ 15 _._ 0 _\u00b1_ 89 _._ 0 21 _._ 9 _\u00b1_ 80 _._ 5 79 _._ 0 _\u00b1_ 93 _._ 2 **94** _._ **1** _\u00b1_ **112** _._ **8 5** _._ **5** 7 _._ 0 8 _._ 7 6 _._ 3 41 **48** 27\n\nAnt DDPG 140 _._ 6 _\u00b1_ 113 _._ 9 278 _._ 9 _\u00b1_ 136 _._ 8 **412** _._ **4** _\u00b1_ **150** _._ **0** 377 _._ 7 _\u00b1_ 115 _._ 4 5 _._ 0 **4** _._ **0** 4 _._ 2 4 _._ 8 3 4 **11**\n\nPPO 822 _._ 6 _\u00b1_ 35 _._ 8 842 _._ 4 _\u00b1_ 42 _._ 4 768 _._ 3 _\u00b1_ 97 _._ 3 **863** _._ **7** _\u00b1_ **67** _._ **4 5** _._ **0** 7 _._ 2 9 _._ 0 6 _._ 8 **39** _\u2212_ 31 19\n\nTRPO **897** _._ **6** _\u00b1_ **41** _._ **2** 808 _._ 8 _\u00b1_ 62 _._ 0 771 _._ 2 _\u00b1_ 69 _._ 1 837 _._ 6 _\u00b1_ 47 _._ 1 8 _._ 7 8 _._ 0 **5** _._ **0** 7 _._ 7 20 _\u2212_ 67 **24**\n\nHalfCheetah DDPG 576 _._ 3 _\u00b1_ 749 _._ 9 909 _._ 0 _\u00b1_ 331 _._ 4 **1137** _._ **8** _\u00b1_ **185** _._ **6** 931 _._ 0 _\u00b1_ 345 _._ 4 10 _._ 0 **7** _._ **0** 8 _._ 6 8 _._ 0 43 **52** 26\n\nPPO **97** _._ **1** _\u00b1_ **377** _._ **9** 34 _._ 2 _\u00b1_ 596 _._ 5 _\u2212_ 1004 _._ 3 _\u00b1_ 409 _._ 5 95 _._ 5 _\u00b1_ 561 _._ 8 **5** _._ **7** 7 _._ 4 10 _._ 0 6 _._ 8 _\u2212_ 16 **48** _\u2212_ 14\n\nTRPO 228 _._ 4 _\u00b1_ 416 _._ 1 81 _._ 2 _\u00b1_ 524 _._ 3 40 _._ 9 _\u00b1_ 708 _._ 6 **391** _._ **9** _\u00b1_ **314** _._ **9** 7 _._ 3 6 _._ 0 7 _._ 0 **5** _._ **3** 6 _\u2212_ 4 **17**\n\nHopper DDPG 1189 _._ 6 _\u00b1_ 600 _._ 4 1048 _._ 7 _\u00b1_ 555 _._ 5 1394 _._ 5 _\u00b1_ 727 _._ 7 **1492** _._ **2** _\u00b1_ **562** _._ **9** 7 _._ 8 **4** _._ **5** 5 _._ 8 6 _._ 3 _\u2212_ **3** _\u2212_ 16 _\u2212_ 8\nPPO **825** _._ **5** _\u00b1_ **71** _._ **4** 802 _._ 8 _\u00b1_ 49 _._ 6 338 _._ 9 _\u00b1_ 330 _._ 7 634 _._ 3 _\u00b1_ 281 _._ 6 **8** _._ **4** 9 _._ 0 9 _._ 0 10 _._ 0 _\u2212_ 42 _\u2212_ **25** _\u2212_ 88\n\nTRPO **1196** _._ **3** _\u00b1_ **464** _._ **7** 856 _._ 5 _\u00b1_ 526 _._ 0 646 _._ 5 _\u00b1_ 283 _._ 6 724 _._ 4 _\u00b1_ 219 _._ 2 **8** _._ **2** 9 _._ 0 \u2013 \u2013 _\u2212_ 20 **19** _\u2212_ 8\n\nWalker2D DDPG 113 _._ 6 _\u00b1_ 90 _._ 7 **299** _._ **4** _\u00b1_ **74** _._ **8** 248 _._ 7 _\u00b1_ 145 _._ 6 267 _._ 6 _\u00b1_ 88 _._ 5 8 _._ 0 5 _._ 0 **4** _._ **9** 5 _._ 4 _\u2212_ **41** _\u2212_ 80 _\u2212_ 44\n\nPPO 185 _._ 6 _\u00b1_ 107 _._ 2 110 _._ 8 _\u00b1_ 40 _._ 2 **199** _._ **3** _\u00b1_ **108** _._ **2** 126 _._ 4 _\u00b1_ 25 _._ 1 4 _._ 4 1 _._ 8 1 _._ 8 **1** _._ **2** _\u2212_ 89 _\u2212_ 86 _\u2212_ **63**\n\nTRPO **260** _._ **2** _\u00b1_ **207** _._ **4** 192 _._ 6 _\u00b1_ 133 _._ 2 141 _._ 6 _\u00b1_ 54 _._ 4 231 _._ 4 _\u00b1_ 182 _._ 4 3 _._ 0 **2** _._ **2** 2 _._ 4 2 _._ 4 _\u2212_ 45 _\u2212_ **19** _\u2212_ 56\n\nHumanoid DDPG _\u2212_ 120 _._ 1 _\u00b1_ 26 _._ 7 42 _._ 6 _\u00b1_ 12 _._ 5 51 _._ 6 _\u00b1_ 14 _._ 3 **57** _._ **1** _\u00b1_ **14** _._ **3** \u2013 3 _._ 7 **2** _._ **8** 3 _._ 1 **50** 45 48\n\nPPO **58** _._ **0** _\u00b1_ **7** _._ **1** 48 _._ 2 _\u00b1_ 6 _._ 9 47 _._ 9 _\u00b1_ 9 _._ 1 48 _._ 3 _\u00b1_ 7 _._ 5 8 _._ 1 1 _._ 9 2 _._ 2 **1** _._ **1** 3 **7** 4\n\nTRPO **68** _._ **8** _\u00b1_ **6** _._ **4** 44 _._ 8 _\u00b1_ 6 _._ 2 47 _._ 8 _\u00b1_ 6 _._ 9 47 _._ 0 _\u00b1_ 7 _._ 2 8 _._ 0 **2** _._ **0** 3 _._ 0 3 _._ 0 _\u2212_ 21 _\u2212_ **14** _\u2212_ 28\n\nTable 1: Comparison across environments and algorithms. _Avg. Return_ reports final mean _\u00b1_ std over seeds; _Rounds to 0.95\u00d7Best_\ncounts iterations to reach 95% of the best average return; _Variability \u2193_ _(%)_ measures relative reduction in return variance. **Bold**\nindicates best performance; dash (\u2013) indicates not reached within budget. Abbreviations: Base (vanilla baseline), LB (Lookback),\nMU (MixUp), TP (Three-Points).\n\n**Theorem 2.** _We set \u03b7_ = _O_ (1 _/\u221aT_ ) _and run 1(Lookback) for_\n\n_T steps. If the bias \u03f5\u03b8 satisfies \u03f5\u03b8 \u2264O_ (1 _/T_ ) _, then we have_\n\nmin _t\u2264T_ min _{_ E _\u2225\u2207V_ _[\u03c0][\u03b8t]_ _\u2225_ 2 _,_ E _\u2225\u2207V_ _[\u03c0][\u03b8t]_ _\u2225_ 2 [2] _[} \u2264O]_ \ufffd ~~_\u221a_~~ 1 _T_\n\n_._\n\ufffd\n\nThe result shows that our algorithm converges to a stationary point, as long as the value estimation is accurate enough.\nMoreover, due to the gradient dominance property of the\nvalue function (Agarwal et al. 2021), our result also implies\nthe global optimality of the learned policy."}, {"role": "assistant", "content": "**Analogy:** Imagine you're teaching a robot how to play different games, like driving a car or helping a little person walk. The robot gets a score in each game. Some robots are good at remembering what worked before, which helps them learn faster and play better.\n\n**Explanation:** This paper talks about a special way to teach the robot, like giving it a super memory for its best moves. If the robot remembers its good choices (like how much to turn the steering wheel) and also makes sure its 'guess' about what's good isn't too wobbly, then it will get really, really good at the game. Not just a little bit good, but the absolute best it can be! The big chart shows how different robots played in different games \u2013 some got super high scores, some learned super fast, and some were very steady players."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\nIn this work, we introduce a new task in the area of _Dataset Distillation_ : learning a tiny set of synthetic\nimages designed to train **linear probes** on top of **pre-trained self-supervised vision models** . Our\nproposed solution, _Linear Gradient Matching_, optimizes a meta gradient objective to ensure that our\nsynthetic images induce training updates similar to those obtained from the real data. Quantitatively,\nour method outperforms all baselines, enabling, for example, a DINO-v2 linear probe to reach 75%\ntest accuracy on ImageNet-1k while only having ever seen **one labeled image** per class.\n\nWe also showcase the importance of secondary aspects of our Linear Gradient Matching method, such\nas the pyramid representation and differentiable augmentations, and highlight their importance in\nlearning highly efficacious distilled datasets. Furthermore, we evaluate our method on challenging finegrained datasets and show that it out-performs the real-image baselines by an even larger margin than\non the standard ImageNet benchmarks. Lastly, our method yields several interesting interpretability\nresults, such as giving insights into how these pre-trained models \u201csee,\u201d predicting how well different\nmodels align, or elucidating a model\u2019s ability to generalize beyond its training distribution.\n\nWe hope our work brings to light the interesting set of problems posed by this new task and inspires\n[others to continue in this area. Code and distilled datasets can be found on our project page.](http://georgecazenavette.github.io/linear-gm)\n\n10\n\n**Acknowledgments**\n\nThis work was supported by the National Science Foundation under Grant No. 2211259, by the\nSingapore DSTA under DST00OECI20300823 (New Representations for Vision, 3D Self-Supervised\nLearning for Label-Efficient Vision), by the Intelligence Advanced Research Projects Activity\n(IARPA) via Department of Interior/Interior Business Center (DOI/IBC) under 140D0423C0075, by\nONR MURI grant #033697-00007, by the Amazon Science Hub, by the MIT-Google Program for\nComputing Innovation, by Sony Interactive Entertainment, and by a 2025 MIT ORCD Seed Fund\nCompute Grant.\n\n11\n\n**References**\n\n[[1] Adam McLean. The surrealism website. https://surrealism.website/. 16](https://surrealism.website/)\n\n[2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski,\nG. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong,\nM. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang,\nY. Lu, C. K. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, S. Zhang,\nM. Suo, P. Tillet, X. Zhao, E. Wang, K. Zhou, R. Zou, X. Wang, A. Mathews, W. Wen, G. Chanan, P. Wu,\nand S. Chintala. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and\ngraph compilation. In _Proceedings of the 29th ACM International Conference on Architectural Support for_\n_Programming Languages and Operating Systems, Volume 2_, ASPLOS \u201924, New York, NY, USA, 2024.\nAssociation for Computing Machinery. 15, 16\n\n[3] L. Bossard, M. Guillaumin, and L. Van Gool. Food-101 \u2013 mining discriminative components with random\nforests. In _ECCV_, 2014. 16, 17, 36\n\n[4] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke,\nJ. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy\nprograms, 2018. 15\n\n[5] M. Caron, H. Touvron, I. Misra, H. J\u00e9gou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in\nself-supervised vision transformers. In _ICCV_, 2021. 3, 10, 16\n\n[6] G. Cazenavette, T. Wang, A. Torralba, A. A. Efros, and J.-Y. Zhu. Dataset distillation by matching training\ntrajectories. In _CVPR_, 2022. 1, 2, 3, 4, 6\n\n[7] G. Cazenavette, T. Wang, A. Torralba, A. A. Efros, and J.-Y. Zhu. Generalizing dataset distillation via\ndeep generative prior. In _CVPR_, 2023. 1, 3, 4, 6, 7\n\n[8] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual\nrepresentations. In _ICML_, 2020. 3\n\n[9] X. Chen, H. Fan, R. Girshick, and K. He. Improved baselines with momentum contrastive learning. _arXiv_\n_preprint arXiv:2003.04297_, 2020.\n\n[10] X. Chen, S. Xie, and K. He. An empirical study of training self-supervised vision transformers. In _ICCV_,\n2021. 3, 5, 8, 17, 21\n\n[11] J. Cui, R. Wang, S. Si, and C.-J. Hsieh. Scaling up dataset distillation to imagenet-1k with constant memory.\nIn _ICML_, 2023. 1, 3\n\n[12] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In _CVPR_, 2009. 2, 5, 8, 10, 16, 17, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n\n[13] Y. Fang, Q. Sun, X. Wang, T. Huang, X. Wang, and Y. Cao. Eva-02: A visual representation for neon\ngenesis. _Image and Vision Computing_, 2024. 3, 16, 20\n\n[14] Y. Fang, W. Wang, B. Xie, Q. Sun, L. Wu, X. Wang, T. Huang, X. Wang, and Y. Cao. Eva: Exploring the\nlimits of masked visual representation learning at scale. In _CVPR_, 2023. 3, 5, 17\n\n[15] S. Fort and J. Whitaker. Direct ascent synthesis: Revealing hidden generative capabilities in discriminative\nmodels. _arXiv preprint arXiv:2502.07753_, 2025. 4\n\n[16] Z. Guo, K. Wang, G. Cazenavette, H. LI, K. Zhang, and Y. You. Towards lossless dataset distillation via\ndifficulty-aligned trajectory matching. In _ICLR_, 2024. 1, 3\n\n[17] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick. Masked autoencoders are scalable vision learners.\nIn _CVPR_, 2022. 3\n\n[18] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation\nlearning. In _CVPR_, 2020. 3\n\n[[19] Hugging Face. Hugging Face: The AI community building the future. https://huggingface.co,](https://huggingface.co)\n2016. 16\n\n[20] M. Huh, B. Cheung, T. Wang, and P. Isola. Position: The platonic representation hypothesis. In _ICML_,\n2024. 2, 3, 9\n\n12\n\n[21] A. Khosla, N. Jayadevaprakash, B. Yao, and L. Fei-Fei. Novel dataset for fine-grained image categorization.\nIn _CVPR_, 2011. 5, 9, 16, 17, 32\n\n[22] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015. 15\n\n[23] P. W. Koh, S. Sagawa, H. Marklund, S. M. Xie, M. Zhang, A. Balsubramani, W. Hu, M. Yasunaga, R. L.\nPhillips, I. Gao, T. Lee, E. David, I. Stavness, W. Guo, B. A. Earnshaw, I. S. Haque, S. Beery, J. Leskovec,\nA. Kundaje, E. Pierson, S. Levine, C. Finn, and P. Liang. WILDS: A benchmark of in-the-wild distribution\nshifts. In _ICML_, 2021. 16\n\n[24] P. Liao, X. Li, X. Liu, and K. Keutzer. The artbench dataset: Benchmarking generative models with\nartworks. _arXiv preprint arXiv:2206.11404_, 2022. 5, 10, 16\n\n[25] D. Liu, J. Gu, H. Cao, C. Trinitis, and M. Schulz. Dataset distillation by automatic training trajectories. In\n_ECCV_, 2024. 1\n\n[26] A. Lynch, G. J.-S. Dovonon, J. Kaddour, and R. Silva. Spawrious: A benchmark for fine control of spurious\ncorrelation biases, 2023. 5, 8, 16\n\n[27] T. Nguyen, Z. Chen, and J. Lee. Dataset meta-learning from kernel ridge-regression. In _ICLR_, 2020. 1, 2\n\n[28] T. Nguyen, R. Novak, L. Xiao, and J. Lee. Dataset distillation with infinitely wide convolutional networks.\nIn _NeurIPS_, 2021. 1\n\n[29] M.-E. Nilsback and A. Zisserman. Automated flower classification over a large number of classes. In\n_Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP)_,\nKolkata, India, 2008. 16, 17, 35\n\n[30] C. Olah, A. Mordvintsev, and L. Schubert. Feature visualization. _Distill_, 2017.\nhttps://distill.pub/2017/feature-visualization. 4, 7\n\n[31] M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. HAZIZA,\nF. Massa, A. El-Nouby, M. Assran, N. Ballas, W. Galuba, R. Howes, P.-Y. Huang, S.-W. Li, I. Misra,\nM. Rabbat, V. Sharma, G. Synnaeve, H. Xu, H. Jegou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski.\nDINOv2: Learning robust visual features without supervision. _TMLR_, 2024. 3, 5, 6, 8, 16, 17, 19, 22, 23,\n24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36\n\n[32] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and\nA. Lerer. Automatic differentiation in pytorch. In _ICLR Workshop_, 2017. 3\n\n[33] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,\nL. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,\nL. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In\n_NeurIPS_ . 2019. 15, 16\n\n[34] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021. 3,\n5, 16, 17, 18\n\n[[35] J. Resig. Ukiyo-e search. https://ukiyo-e.org/, 2012. 16](https://ukiyo-e.org/)\n\n[36] E. Riba, D. Mishkin, D. Ponsa, E. Rublee, and G. Bradski. Kornia: an open source differentiable computer\nvision library for pytorch. In _WACV_, 2020. 16\n\n[37] C. Ryali, Y.-T. Hu, D. Bolya, C. Wei, H. Fan, P.-Y. Huang, V. Aggarwal, A. Chowdhury, O. Poursaeed,\nJ. Hoffman, et al. Hiera: A hierarchical vision transformer without the bells-and-whistles. In _ICML_, 2023.\n3\n\n[38] S. Sagawa, P. W. Koh*, T. B. Hashimoto, and P. Liang. Distributionally robust neural networks. In _ICLR_,\n2020. 5, 8, 16\n\n[39] S. Sagawa, P. W. Koh, T. Lee, I. Gao, S. M. Xie, K. Shen, A. Kumar, W. Hu, M. Yasunaga, H. Marklund,\nS. Beery, E. David, I. Stavness, W. Guo, J. Leskovec, K. Saenko, T. Hashimoto, S. Levine, C. Finn, and\nP. Liang. Extending the wilds benchmark for unsupervised adaptation. In _ICLR_, 2022. 16\n\n[40] B. Son, Y. Oh, D. Baek, and B. Ham. Fyi: Flip your images for dataset distillation. In _ECCV_, 2024. 1, 3\n\n[41] Q. Sun, Y. Fang, L. Wu, X. Wang, and Y. Cao. Eva-clip: Improved training techniques for clip at scale.\n_arXiv preprint arXiv:2303.15389_, 2023. 3\n\n13\n\n[42] Y. Tian, D. Krishnan, and P. Isola. Contrastive multiview coding. In _ECCV_, 2020. 5, 16, 17, 18, 19, 20, 21\n\n[[43] TorchVision maintainers and contributors. Torchvision: Pytorch\u2019s computer vision library. https:](https://github.com/pytorch/vision)\n[//github.com/pytorch/vision, 2016. 16](https://github.com/pytorch/vision)\n\n[44] M. Tschannen, A. Gritsenko, X. Wang, M. F. Naeem, I. Alabdulmohsin, N. Parthasarathy, T. Evans,\nL. Beyer, Y. Xia, B. Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic\nunderstanding, localization, and dense features. _arXiv preprint arXiv:2502.14786_, 2025. 3\n\n[45] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Caltech-UCSD birds 200. Technical Report\nCNS-TR-2011-001, California Institute of Technology, 2011. 5, 9, 17, 33, 34\n\n[46] K. Wang, B. Zhao, X. Peng, Z. Zhu, S. Yang, S. Wang, G. Huang, H. Bilen, X. Wang, and Y. You. Cafe:\nLearning to condense dataset by aligning features. _CVPR_, 2022. 1\n\n[47] T. Wang, J.-Y. Zhu, A. Torralba, and A. A. Efros. Dataset distillation. _arXiv preprint arXiv:1811.10959_,\n2018. 1, 2\n\n[[48] R. Wightman. Pytorch image models. https://huggingface.co/timm, 2019. 16](https://huggingface.co/timm)\n\n[[49] WikiArt.org. WikiArt Visual Art Encyclopedia. https://www.wikiart.org/, 2024. 16](https://www.wikiart.org/)\n\n[50] Z. Yin and Z. Shen. Dataset distillation via curriculum data synthesis in large data era. 2024. 2\n\n[51] Z. Yin, E. Xing, and Z. Shen. Squeeze, recover and relabel: Dataset condensation at imagenet scale from a\nnew perspective. In _NeurIPS_, 2023. 1, 2\n\n[52] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-training. In _ICCV_,\n2023. 3\n\n[53] B. Zhao and H. Bilen. Dataset condensation with differentiable siamese augmentation. In _ICML_, 2021. 1,\n\n4, 7\n\n[54] B. Zhao and H. Bilen. Dataset condensation with distribution matching. _WACV_, 2023. 1, 2\n\n[55] B. Zhao, K. R. Mopuri, and H. Bilen. Dataset condensation with gradient matching. In _ICLR_, 2020. 2, 3, 4\n\n[56] Y. Zhou, E. Nezhadarya, and J. Ba. Dataset distillation using neural feature regression. _NeurIPS_, 2022. 1, 2\n\n14\n\n**Appendix**\n\nTo begin the appendix, we first speak on our method\u2019s limitations, broader impact, and compute\nbudget. In the following sections, we include more information that was omitted from the main\npaper due to space constraints. In Section A, we provide a more detailed description of our method\u2019s\nimplementation details. In Section C, we include visualizations of complete distilled datasets.\n\n**Limitations** Our method is largely limited by memory and data loading. For example, we were limited in our ImageNet-1k experiments to only using 3 rounds of augmentations per batch due to a data\nloading bottleneck. We hope to eventually develop a method that does not require the loading of thousands of real images per step. Additionally, due to the bi-level optimization, we are limited to using Pytorch\u2019s [2, 33] nn.DistributedParallel, rather than nn.DistributedDataParallel,\nwhich causes significant slowdowns due to our very large batch sizes. Using a different automatic\ndifferentiation framework, such as Jax [4], might alleviate this issue, but this would also require\nporting all our self-supervised backbones to the new framework.\n\n**Broader Impact** While our method quantitatively out-performs the real image baselines, the\nbroader impacts likely lie in the introduction of this new task as well as the resulting images acting as\nan interpretability tool. As shown in the paper, the distilled images offer insights into things such as\nmodel alignment and how different networks see the world in varying ways.\n\nFurthermore, the ultimate goal of dataset distillation is the reduction of training time, which equates\nto less energy spent. A linear probe trained on our distilled datasets can be trained to convergence in\njust a few minutes while training on the full dataset can take up to an entire day.\n\n**Compute Budget** We used a variety of GPUs for this work depending on what was available on\nthe shared cluster. Specifically, we used a combination of H200, A100, L40s, Ada6000, and 4090\nGPUs. Distilling ImageNet-100 with the default settings takes about 3 hours using 1 H200 GPU, and\nImageNet-1k takes about 12 hours using 4 H200 GPUs.\n\n**A** **Implementation Details**\n\nIn this section, we provide a more detailed explanation of our methodological and experimental\nimplementation details.\n\n**A.1** **Distillation**\n\n[We implement our method in Pytorch [2, 33], which has a unique license to which we comply.](https://github.com/pytorch/pytorch/tree/main?tab=License-1-ov-file#readme)\n\nWe optimize our pyramid representations using Adam [22] with a learning rate of 0.002.\n\nWe distill for 5000 iterations and add a new level to the pyramids every 200 iterations until we reach\nthe maximum resolution (256).\n\nEach level of the pyramid is initialized with _N_ (0 _,_ 1) normalized by the current number of levels in\nthe pyramid. When a new level is added, the existing levels are appropriately re-normalized.\n\nAt each distillation iteration, we first sample a new linear classifier consisting of a weight matrix _W_\nand bias vector **b** using the default initialization for Pytorch\u2019s nn.Linear. For simplicity, we omit\nthe bias in the main paper.\n\nThen, we reconstruct the synthetic images from their pyramid representations using the following\nequation (copied from the main paper for convenience):\n\n\ufffd\n\n_X_ = [1]\n\n[1]\n\n2 [+ 1]\n\n2 [tanh]\n\n(4)\n\n\ufffd\ufffd _r\u2208\u03c1_\n\nresize256( _Pr_ )\n\n_r\u2208\u03c1_\n\nThe synthetic images are then augmented several (10 by default) _different_ (not sequential) times and\nconcatenated together to form the full synthetic batch. We obtain our synthetic loss ( _\u2113_ syn) by passing\nthis batch through the feature extractor _\u03d5_ and linear classifier ( _W,_ **b** ) and then compute the gradient of\nthe synthetic loss with respect to the linear classifier (and bias). These gradients are then vectorized.\n\n15\n\nFor our real batch, we sample a number of real images equal to the length of the _full_ synthetic batch\n(i.e., after the different augmentations and concatenation) and augment them once (since the batch\nsize already matches that of the full synthetic batch). We then get the real loss and gradients via the\nsame procedure. The meta loss is then calculated as the cosine distance between the real and synthetic\nvectorized gradients.\n\n**A.2** **Augmentation**\n\nFor our differentiable augmentations, we use a custom re-implementation of torchvision transforms that run on GPU and support different random transforms for each element in a batch. We initially used the Kornia [36] implementations, but found that they cause CPU bottlenecks with very large\nbatch sizes. We use RandomHorizontalFlip with default parameters, RandomResizedCrop\nwith size=(224, 224), and RandomGaussianNoise with std=0.2.\n\n**A.3** **Evaluation**\n\n**Linear Probes.** To evaluate a given distilled dataset (or selected coreset for the real baselines) on\na given target model, we first randomly initialize a new linear classifier. We then train the linear\nclassifier for 1000 epochs with a batch size of 100. We use an Adam optimizer with a learning rate of\n0 _._ 001 _/_ 256 (taken from the DINO-v1 [5] evaluation protocol) along with a cosine decay learning rate\nschedule. We stop training early if the test accuracy has not improved over the last 50 epochs. For the\ntraining set, we perform the same set of augmentations as during distillation (horizontal flip, random\nresized crop, and Gaussian noise). The output of the random resized crop is of size 224 _\u00d7_ 224. For the\ntest set, we resize the shortest side to 256 and then do a center crop of 224 _\u00d7_ 224.\n\n**Nearest Neighbors.** For our nearest neighbor evaluation, we first find the train set embeddings\nby resizing the shortest side of each test image to 256, taking a 224 _\u00d7_ 224 center crop, and passing\nthrough the feature extractor. We then do the same for the test set and find the training embedding\nclosest to each test embedding (by cosine distance) and report the class of said neighbor. We only do\none nearest neighbor since we distill down to one image per class.\n\n**A.4** **Datasets**\n\nFor ImageNet [12] and ImageNet [42], we build on the Torchvision [43] implementation as part of the\n[Pytorch [2, 33] ecosystem. ImageNet has a unique non-commercial license, to which we have agreed.](https://image-net.org/download.php)\n\n[Spawrious [26] is taken from the paper\u2019s official GitHub repository and uses a CC0-1.0 license.](https://github.com/aengusl/spawrious)\n\nWaterbirds [38] is taken from the WILDS [23, 39] benchmark. Both the Waterbirds and WILDS\nrepositories use an MIT license.\n\n[Stanford Dogs [21] is taken from the project website. It presumably uses the same license as](http://vision.stanford.edu/aditya86/ImageNetDogs)\nImageNet [12].\n\n[Caltech-UCSD-200-2011 is taken from the project website. No license is given.](http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz)\n\n[ArtBench [24] is taken from the paper\u2019s GitHub repository. The repository itself uses an MIT license.](https://github.com/liaopeiyuan/artbench)\nThe data is sourced from WikiArt [49], Ukiyo-e Search [35], and The Surrealism Website [1], all of\nwhich have Fair Use licenses.\n\nFlowers-102 [29] is taken from Torchvision. No license is given.\n\nFood-101 [3] is taken from Torchvision. No license is given.\n\n**A.5** **Models**\n\n[For CLIP [34], we use the official GitHub repository which has an MIT license.](https://github.com/openai/CLIP)\n\n[For DINO-v1 [5], we use the official GitHub repository which has an Apache-2.0 license.](https://github.com/facebookresearch/dino)\n\n[For DINO-v2 [31], we use the official GitHub repository which has an Apache-2.0 license.](https://github.com/facebookresearch/dinov2)\n\n[For EVA-02 [13], we use the Pytorch Image Models [48] implementation hosted on Hugging Face [19]](https://huggingface.co/timm/eva02_base_patch14_224.mim_in22k)\nwhich has an MIT license.\n\n16\n\nTrain Set Flowers-102 Food-101\n(1 Img/Cls)\n\nCLIP DINO-v2 EVA-02 MoCo-v3 Average CLIP DINO-v2 EVA-02 MoCo-v3 Average\n\nDistilled (Ours) **79.5** _\u00b1_ **1.2 99.6** _\u00b1_ **0.0 97.8** _\u00b1_ **0.1 56.4** _\u00b1_ **3.1 83.3** _\u00b1_ **1.1 78.7** _\u00b1_ **1.6 83.7** _\u00b1_ **0.3 83.9** _\u00b1_ **0.2 31.6** _\u00b1_ **3.2 69.5** _\u00b1_ **1.3**\nNeighbors 69.4 _\u00b1_ 0.8 99.4 _\u00b1_ 0.2 90.8 _\u00b1_ 0.2 48.6 _\u00b1_ 1.8 77.0 _\u00b1_ 0.7 58.8 _\u00b1_ 1.3 73.8 _\u00b1_ 0.4 71.5 _\u00b1_ 0.3 25.2 _\u00b1_ 1.3 57.3 _\u00b1_ 0.8\nCentroids 77.5 _\u00b1_ 0.7 99.4 _\u00b1_ 0.1 95.6 _\u00b1_ 0.3 48.2 _\u00b1_ 2.6 80.2 _\u00b1_ 0.9 74.1 _\u00b1_ 0.6 75.6 _\u00b1_ 0.1 76.4 _\u00b1_ 0.4 22.7 _\u00b1_ 2.0 62.2 _\u00b1_ 0.8\n\nRandom 67.9 _\u00b1_ 1.1 98.8 _\u00b1_ 0.4 91.0 _\u00b1_ 0.8 36.8 _\u00b1_ 2.2 73.6 _\u00b1_ 1.1 48.5 _\u00b1_ 1.0 52.8 _\u00b1_ 1.9 49.9 _\u00b1_ 0.6 13.5 _\u00b1_ 1.7 41.2 _\u00b1_ 1.3\n\nFull Dataset 93.4 _\u00b1_ 0.1 99.7 _\u00b1_ 0.0 98.9 _\u00b1_ 0.0 82.2 _\u00b1_ 0.1 93.5 _\u00b1_ 0.1 91.9 _\u00b1_ 0.0 92.7 _\u00b1_ 0.1 92.0 _\u00b1_ 0.0 78.3 _\u00b1_ 0.0 88.7 _\u00b1_ 0.0\n\nTable B.1: **Additional Datasets:** Distilling Flowers-102 [29] and Food-101 [3] show similar results; our distilled\ndataset consistently out-perform the real-image baselines.\n\n[For MoCo-v3 [10], we use the official GitHub repository which has a Creative Commons Attribution-](https://github.com/facebookresearch/moco-v3)\nNonCommercial 4.0 International Public License.\n\n**B** **Additional Results**\n\nIn Table B.1, we include results for Flowers-102 [29] and Food-102 [3]. We see results following the\nsame trends as the rest of the paper; the distilled datasets out-perform the real-image baselines across\nthe board.\n\n**C** **Visualizing Full Datasets**\n\nIn Figures C.1, C.2, C.3, and C.4 we present the entirety of ImageNet-100 [42] distilled with\nCLIP [34], DINO-v2 [31], EVA-02 [14], and MoCo-v3 [10] respectively. In Figures C.5\u2013C.14, we\nshow ImageNet-1k [12] distilled with DINO-v2 and omit the other models for the sake of brevity\n(and ink).\n\nWe also include Stanford Dogs [21] (Figure C.15), CUB-200-2011 [45] (Figures C.16 and C.17),\nFlowers-102 [29] (Figure C.18), and Food-101 [3] (Figure C.19) distilled with DINO-v2 and once\nagain omit the other models. All distilled datasets, including those not inculded here, can be viewed\n[on our project page.](http://georgecazenavette.github.io/linear-gm)\n\n17\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-17-0.png)\n\nFigure C.1: ImageNet-100 [42] distilled using CLIP [34]\n\n18\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-18-0.png)\n\nFigure C.2: ImageNet-100 [42] distilled using DINO-v2 [31]\n\n19\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-19-0.png)\n\nFigure C.3: ImageNet-100 [42] distilled using EVA-02 [13]\n\n20\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-20-0.png)\n\nFigure C.4: ImageNet-100 [42] distilled using MoCo-v3 [10]\n\n21\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-21-0.png)\n\nFigure C.5: ImageNet-1k [12] distilled with DINO-v2 [31] classes [0-99]\n\n22\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-22-0.png)\n\nFigure C.6: ImageNet-1k [12] distilled with DINO-v2 [31] classes [100-199]\n\n23\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-23-0.png)\n\nFigure C.7: ImageNet-1k [12] distilled with DINO-v2 [31] classes [200-299]\n\n24\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-24-0.png)\n\nFigure C.8: ImageNet-1k [12] distilled with DINO-v2 [31] classes [300-399]\n\n25\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-25-0.png)\n\nFigure C.9: ImageNet-1k [12] distilled with DINO-v2 [31] classes [400-499]\n\n26\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-26-0.png)\n\nFigure C.10: ImageNet-1k [12] distilled with DINO-v2 [31] classes [500-599]\n\n27\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-27-0.png)\n\nFigure C.11: ImageNet-1k [12] distilled with DINO-v2 [31] classes [600-699]\n\n28\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-28-0.png)\n\nFigure C.12: ImageNet-1k [12] distilled with DINO-v2 [31] classes [700-799]\n\n29\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-29-0.png)\n\nFigure C.13: ImageNet-1k [12] distilled with DINO-v2 [31] classes [800-899]\n\n30\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-30-0.png)\n\nFigure C.14: ImageNet-1k [12] distilled with DINO-v2 [31] classes [900-999]\n\n31\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-31-0.png)\n\nFigure C.15: Stanford Dogs [21] distilled using DINO-v2 [31]\n\n32\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-32-0.png)\n\nFigure C.16: CUB-200-2011 [45] distilled using DINO-v2 [31] classes [0-99]\n\n33\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-33-0.png)\n\nFigure C.17: CUB-200-2011 [45] distilled using DINO-v2 [31] classes [100-200]\n\n34\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-34-0.png)\n\nFigure C.18: Flowers-102 [29] distilled using DINO-v2 [31]\n\n35\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-35-0.png)\n\nFigure C.19: Food-101 [3] distilled using DINO-v2 [31]\n\n36"}, {"role": "assistant", "content": "**Analogy:** Imagine you want to teach a robot how to tell cats from dogs. Normally, you'd show it millions of real cat and dog pictures. But that takes a very, very long time! What if you could make just one super-duper perfect cartoon drawing of a cat and one of a dog? These special drawings are so good that if the robot learns from them, it becomes almost as smart as if it saw all the real pictures.\n\n**Explanation:** This paper figured out how to make those super-duper perfect cartoon drawings, called 'synthetic images'. They are tiny, tiny datasets designed to teach an already smart robot (a 'pre-trained self-supervised vision model') how to label new pictures very quickly using a simple 'labeler' (a 'linear probe'). The trick is to make sure these cartoon drawings teach the robot the exact same lessons as the real pictures, like when you learn from a perfect little cheat sheet instead of a giant textbook. This way, the robot can learn to tell cats from dogs in minutes instead of a whole day, and we also get to see how the robot 'thinks' about pictures!"}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n**Introduction**\n\nHumans are capable of extrapolating from their existing knowledge to unfamiliar scenarios and generating\nnew knowledge\u2014a process that constitutes reasoning (Lombrozo, 2024). In contrast, large language models\n(LLMs) exhibit a form of cognitive dissonance unprecedented in human cognition: they master complex skills\n(Chervonyi et al., 2025; Jiang et al., 2024) while lacking simpler prerequisite ones (McCoy et al., 2024; Dziri\net al., 2023; Berglund et al., 2023), and solve challenging problems while failing on trivial variants (Shao et al.,\n2025; Li et al., 2025b). This dissociation between high benchmark performance and the lack of generalization\nsuggests that models may be achieving correct outputs through mechanisms fundamentally different from\nthe robust cognitive structures underlying human reasoning. Current training and evaluation paradigms\nreward reasoning outcomes without examining the cognitive processes that produce them (Lambert et al.,\n2024), and therefore cannot distinguish between genuine reasoning and memorization (Wu et al., 2025). This\ncreates a measurement crisis: we lack the conceptual framework to characterize what reasoning behaviors\nshould manifest in models and the empirical methods to assess whether they do.\n\nTo understand how cognitive elements manifest in reasoning behaviors, consider a child playing with LEGO\nblocks. Figure 1 illustrates how the child orchestrates multiple cognitive elements: establishing goals, decomposing the task into parts, verifying connections, monitoring progress, and reformulating failed approaches.\nThis flexible coordination of diverse cognitive processes characterizes reasoning. However, contemporary\nresearch on LLM reasoning only attempts scattered investigations of specific behaviors such as verification\n(Gandhi et al., 2025), decomposition (Xu et al., 2025), and self-monitoring (Marjanovi\u0107 et al., 2025). Our\nmeta-analysis of 1,598 arXiv LLM reasoning papers on LLM reasoning reveals this fragmentation quantita\n\n2\n\n![](papers/images/Cognitive-Foundations-for-Reasoning-and-Their-Manifestation-in-LLMs.pdf-2-0.png)\n\n**Reasoning Representation**\n\nSpatial representation\n\n**Reasoning Operations**\n\n**Reasoning Invariants**\n\nConceptual processing Productivity\n\n**Meta-Cognitive Control**\n\nGoal management\n\nEvaluation\n\n![](papers/images/Cognitive-Foundations-for-Reasoning-and-Their-Manifestation-in-LLMs.pdf-2-1.png)\n\nVerification Adaptive detail management Backtracking\n\nFigure 1: An example of reasoning behaviors present in a reasoning trace for building a LEGO spaceship.\nWe organize the different behaviors along the four dimensions of our taxonomy, as shown in Table 1.\n\ntively (Figure 2): 55% examine sequential organization (e.g. reasoning step-by-step), while only 8% study\nbackward chaining (e.g. reverse engineering the solution from the answer). This concentration on easily\nquantifiable behaviors creates a striking asymmetry when compared with human reasoning research, which\nencompasses diverse phenomena, including analogical transfer (Gentner, 1983; Holyoak & Thagard, 1989),\ncausal inference (Sloman & Sloman, 2009; Gopnik et al., 2004), representational flexibility (Ohlsson, 1992;\nKnoblich et al., 1999), and meta-cognitive control (Nelson, 1990; Flavell, 1979). Each of these captures\ndifferent facets of how humans navigate uncertainty, integrate knowledge, and adapt to novel situations.\nWithout a comprehensive framework spanning this range of cognitive elements, we risk optimizing what we\nmeasure rather than what matters, potentially mistaking narrow competence for broad reasoning capability.\n\nTo address this gap, we **introduce** **a** Distribution of Cognitive Elements Considered by LLM Reasoning Papers\n\n![](papers/images/Cognitive-Foundations-for-Reasoning-and-Their-Manifestation-in-LLMs.pdf-2-2.png)\n**unified taxonomy of cognitive foun-** **70** ~~Score~~\n**dations** by synthesizing established theo- **60** ~~Present~~\nries of problem-solving, mental represen- **50** Partially Present\ntation, and meta-cognition (Sweller, 1988; **40**\nJohnson-Laird, 1983; Fodor, 1975). The **3020**\ntaxonomy includes four dimensions: **1)**"}, {"role": "assistant", "content": "**Analogy:** Imagine if you had a super fancy toy robot that could build a really tall, complicated LEGO castle perfectly. But then, if you just gave it two simple LEGO bricks and asked it to put them together, and they were a little bit different than usual, it would get confused and couldn't do it! And maybe it built the castle by just remembering exactly how to put that one specific castle together, not by really understanding how all LEGOs connect.\n\n**Explanation:** The grown-ups are saying that big computer brains (LLMs) are like that robot. They can do really hard puzzles, but sometimes they get stuck on super easy ones, or they solve things by just remembering lots and lots of answers, not by truly figuring things out like you do when you play. It's like they're good at copying, but not always good at thinking for themselves. The grown-ups want to make a special checklist, like a map, to make sure they test all the ways the computer brains should be able to think, not just a few easy things. This way, they can really tell if the computer brain is smart, or just has a really good memory."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\nS( _\u20d7\u03b3k_ ) _\u0338_ = _\u2205_ **then** Continue\n**else** Stop\n\n**4** Compute **E** _k_ and _\u20d7\u03b1k_ from S( _\u20d7\u03b3k_ ) and _v_\n\n**5** Determine **Q** _k_ and **a** _k_ from **E** _k_ and _\u20d7\u03b1k_\n\n**6** Calculate by Formula (3.15) **x**\n\n**7** _k \u2190\u2212_ _k_ + 1\n\n**8** _\u20d7\u03b3k_ +1 _\u2190\u2212_ **x**\n\n**9** Determine S( _\u20d7\u03b3k_ +1)\n**until** S( _\u20d7\u03b3k_ +1) = S( _\u20d7\u03b3k_ )\n\n**end**\n\nMeinhardt (2013, Theorem 8.1.2) establishes that this iterative procedure converges toward a pre-kernel\npoint. In view of Meinhardt (2013, Theorem 9.1.2), we even know that at most \ufffd _n_ 2\ufffd _\u2212_ 1-iteration steps are\nsufficient to successfully terminate the search process. However, we have some empirical evidence that\ngenerically a maximum of _n_ + 1-iteration steps are needed to determine an element from the pre-kernel\n\nset.\n\n### 4 CHARACTERIZATION OF EFFICIENTLY SOLVABLE PROBLEMS\n\nResume that an algorithm is called to be of polynomial-time if its running time is upper bounded by a\npolynomial expression in the size of the input, as _O_ ( _n_ _[k]_ ), where _k_ is a constant. This means that the\nalgorithm can be considered efficient or tractable. In contrast, an algorithm is said to be of exponentialtime if the running time is upper bounded by _O_ (2 _[poly]_ [(] _[n]_ [)] ). For instance, if _poly_ ( _n_ ) = _n_, then we have\n_O_ (2 _[n]_ )\n\nTo measure the input, at least the data of a cooperative game in the total number of players must be\nsupplied to the model. Depending on the required precision and analysis of the model, more input data\nmust be provided. For instance, arbitrary precision is required for a rational number model to encode the\ninput as a finite string of bits. This implies that the number of elementary bit operations must be counted\nfor a rational number model to grasp the underlying bit complexity. This is not needed for a real Random\nAccess Machine (RAM) model, where it is assumed that the real number can be represented by infinite\nprecision so that each operation on a real number takes constant time, regardless of precision.\n\nFor estimating the computational complexity of an optimization problem, we also have to care about\nstrongly polynomial-time algorithms, for which it is stated that\n\n1. it consists of the basic operations, viz., addition, subtraction, multiplication, division, as well as\ncomparison; and\n\n2. these basic operations are carried out on rationals/integers of size polynomially bounded in the\ndimension of the input; and\n\n11\n\n_Polynomial-Time Algorithms for Computing the Nucleolus_\n\n3. the number of these operations is polynomially bounded in the dimension of the input.\n\nRoughly speaking, an algorithm is called strongly polynomial whenever its running time is a polynomial function of the number of _n_ the size of the input and independent of the magnitude of the input\nvalues.\n\nMoreover, an algorithm is called **NP** -hard if it refers to a problem that is at least as difficult to solve\nas the hardest problems in the complexity class **NP** (class of nondeterministic polynomial-time problems).\nIn particular, computing the (pre-)nucleolus or a (pre-)kernel point is **NP** -hard in general. This is due to\nthe fact that the best-known general algorithms for computing the (pre-)nucleolus involve, for instance,\nsolving a sequence of linear programming problems, which have a running time that is exponential in the\nnumber of players _n_, since there are 2 _[n]_ possible coalitions (exponential complexity).\n\n### 5 ELLIPSOID METHOD OF COMPUTING THE NUCLEOLUS\n\nFaigle et al. (2001) proposed a first polynomial-time algorithm for computing the nucleolus of a TU game\nwhile attacking an allocation in the intersection of the least-core with the pre-kernel. For TU games for\nwhich the pre-kernel contains exactly one core element, one can conclude that even the nucleolus was\ncomputed due to the following result:\n\n**Theorem 5.1** (Single Point Intersection of Core and Per-Kernel) **.** _Let \u27e8_ _N, v \u27e9_ _be a TU game. If_ C( _N, v_ ) _\u2229_\nPK( _N, v_ ) = _{_ **x** _[\u2217]_ _} is satisfied, i.e., the intersection of the core and pre-kernel just consists of a single point,_\n_then it holds \u03bd_ ( _N, v_ ) = **x** _[\u2217]_ _. Hence, the single intersection point is the nucleolus of the game \u27e8_ _N, v \u27e9._\n\nThus, the method of Faigle et al. (2001) for computing the nucleolus of a TU game can be applied to\nconvex games, where the pre-kernel is a singleton, or minimum cost spanning tree games, where the core\nconsists of a single point. But not to the class of permutation games, where the pre-kernel is inscribed in\nthe least-core of the game (for more details, we refer the reader to Solymosi (2014); Meinhardt (2014)).\n\nIn order to determine this exposed point in the intersection of the core with the pre-kernel of a TU\ngame, Faigle et al. (2001) imposed a combination of the ellipsoid method for solving sequences of linear programming problems with an approximation scheme for a (pre-)kernel point. This approximation\nscheme was first postulated by M. Maschler at the 1965 Jerusalem Conference on Game Theory, where\nthe convergence was given as an open problem. In the context of the ellipsoid method, sufficiently many\nsteps of Maschler\u2019s approximation scheme must be invoked to find suitable separating hyperplanes to apply the ellipsoid method. Stearns (1968) provided for Maschler\u2019s approximation scheme of the kernel a\nconvergence proof using topological arguments while imposing the crucial condition that there exists an\ninfinite number of transfers between a pair of players that are maximal. The required boundedness on the\ntransfer to assure convergence was inherited by the compactness of the imputation set and the definition of\nthe kernel. By Faigle et al. (2001, Theorem 4.1), this result was extended to the pre-kernel without using\nany topological argument. Here, it was sufficient to establish that the imposed transfer scheme is bounded\non the pre-imputation set, which is not a compact set. Nevertheless, their proof is logically flawed since\nthe authors confound a proof by contradiction with a proof by contraposition (for further details, we refer\nto Meinhardt (2021)). Fortunately for them, their proof works out.\n\nNotice that the main result of Faigle et al. in establishing a polynomial-time computation of the nucleolus relies on the crucial assumption that there exists a polynomial-time algorithm for computing all\nsurpluses _sij_ ( **x** _, v_ ) over all distinct pairs of players for any allocation **x** _\u2208_ R _[n]_ . This is equivalent to imposing the assumption that the minimum excess _e_ _[v]_ min [(] **[x]** [)][ for any allocation] **[ x]** _[ \u2208]_ [R] _[n]_ [ can also be efficiently]\n\n12\n\n_Polynomial-Time Algorithms for Computing the Nucleolus_\n\ncomputed by an algorithm, viz., in polynomial-time. This allows the authors to disregard the problematic\nissue of **NP** -hardness.\n\nDue to the known drawback of numerical instability in a real data model (unboundness of complexity;\nsee below for more details) of the ellipsoid method, Faigle et al. studied the ellipsoid method in the field\nof rational numbers. This imposes several requirements on the input data that must be considered for\na rational number model. Firstly, the imposed arbitrary precision of a rational number model requires\nknowledge of the encoding length of the largest number in the model that imposes an upper bound on the\ninput size. In addition, it requires that the authors need to estimate the facet complexity of the polyhedron\nspanned by the system of linear inequalities of the linear programming problem that represents the leastcore or core of the TU game. The facet complexity indicates the total number of bits required to encode\nthe system of linear constraints (polyhedron) in the model. Moreover, the computational efficiency of\nthe ellipsoid method stems from its ability to deal implicitly with an exponential number of constraints\nof a linear programming problem through a compact separation oracle. A separation oracle is a routine\nthat, given a point, either confirms that the point is in the feasible set or provides a separating hyperplane.\nFor real-valued linear programming problems, the separation oracle is just a matter of checking all the\nconstraints, which is done efficiently. The complexity of the separation oracle needs to be accounted for\nin the overall runtime. In this respect, an oracle must query to compute and output the coalitional value as\na rational number.\n\nHowever, because of numerical instability issues of the ellipsoid method with floating-point arithmetic,\nthe approach of Faigle et al. has practical limitations with floating-point arithmetic (for more details, see\nbelow).\n\n### 6 SKETCH OF THE ELLIPSOID METHOD\n\nThe algorithm for the ellipsoid method, pioneered by Khachiyan (1979) for linear programming problems,\nworks through iteratively creating smaller ellipsoids that contain a convex body like a polyhedron (feasible\nregion). The process is started with an ellipsoid that contains the feasible region. In each subsequent step,\nthe volume of the ellipsoid is reduced by a constant factor. This is done by calculating in a first step\nfor each current ellipsoid its center of gravity. In the second step, all objective values that direct in the\ndirection where all of them are becoming worse off are thrown away since the bad region cannot contain\nthe optimum. By contrast, all of those objective values that are becoming better off are put in a new\nellipsoid containing still the polyhedron, allowing the volume to be reduced by a constant factor.\n\nIn case the center of gravity of the current ellipsoid is feasible for the original linear programming\nproblem, the bad values are thrown away as before. Otherwise, the center of gravity is not feasible; there\nis still a region with bad values. The bad region of the current ellipsoid is removed as before. A new\nellipsoid is created that is smaller than the current ellipsoid. The number of iterations is governed by how\nmany such reductions are needed to guarantee that the ellipsoid is either empty or contains a point in the\nfeasible region (for more details, see Gr\u00f6tschel et al. (2012, Chapter 3)).\n\nBy the work of Gr\u00f6tschel et al. (1981, Thm. 3.1), Gr\u00f6tschel et al. (2012, Chapter 6), it has been established that the problems of separation and optimization for any given polyhedron P are computationally\nequivalent since there is a polynomial-time algorithm to solve the separation problem of a polyhedron\nP if, and only if, there is a polynomial-time algorithm to solve the linear optimization problem for the\npolyhedron P. In fact, they provide a sharpened separation result to even deal with non-full-dimensional\npolyhedrons. Thus, the problem of strictly separating any given polyhedron P by a point **x** _\u0338\u2208_ P s.t.\n**b** _[\u22a4]_ _\u00b7_ **x** _<_ **b** _[\u22a4]_ _\u00b7_ **y** for all **y** _\u2208_ P with **b** _\u2208_ Q _[n]_ holds true is computationally equivalent to solving the linear\n\n13\n\n_Polynomial-Time Algorithms for Computing the Nucleolus_\n\noptimization problem related to polyhedron P.\n\n### 7 APPROXIMATION SCHEME OF THE KERNEL\n\nThe reader may confront our Approach 3.1 based on the Fenchel-Moreau conjugation with the Standard\nMethod 7.1 from the literature for computing a kernel element while iteratively carrying out a sequence\nof side-payments between pairs of players to reduce the differences in the maximum surpluses between\nthem. Thus, the algorithm is based on the idea to iteratively carrying out bilateral maximal transfers\nbetween pairs of players to approximate a kernel element, instead of a transfer scheme that simultaneously\nadjust the payoffs of all players involved in the game.\n\n**Algorithm 7.1:** Stearns\u2019 Transfer Scheme to converge to a Kernel Element\n\n**Data:** Arbitrary TU Game _\u27e8_ _N, v \u27e9_, and a payoff vector _\u20d7\u03b3_ 0 _\u2208_ _dom h_ .\n**Result:** A payoff vector s.t. _\u20d7\u03b3k_ +1 _\u2208_ K( _v_ ).\n**begin**\n\n**0** _k \u2190\u2212_ 0\n\n**1** Select an arbitrary starting point _\u20d7\u03b3_ 0.\n\n**2** Pre-specify a tolerance value _\u03f5_ .\n**repeat**\n\n**if** _\u20d7\u03b3k \u2208_ _dom h_ **then** Continue\n**else** Stop\n\n**3** Compute the maximum surplus _sij_ ( _\u20d7\u03b3k_ ) for all _i, j \u2208_ _N, i \u0338_ = _j_ by (2.5).\n\n**4** _\u03b4_ _[\u2217]_ _\u2190\u2212_ max _ij\u2208N,i_ = _\u0338_ _j {sij_ ( _\u20d7\u03b3k_ ) _\u2212_ _sji_ ( _\u20d7\u03b3k_ ) _}_ .\n\n**5** Select a player pair ( _i_ _[\u2217]_ _, j_ _[\u2217]_ ) s.t. ( _i_ _[\u2217]_ _, j_ _[\u2217]_ ) _\u2208{i, j \u2208_ _N | \u03b4_ _[\u2217]_ = _sij_ ( _\u20d7\u03b3k_ ) _\u2212_ _sji_ ( _\u20d7\u03b3k_ ) _}_ .\n\n_\u0338_"}, {"role": "assistant", "content": "**Analogy:** Imagine you and your friends are sharing a big pile of candy. You want to make sure everyone gets a really fair share so no one feels left out or angry.\n\n**Explanation:** This paper talks about how computers try to figure out the *most* fair way to share things in games, like dividing up candy. First, it describes a special game where you keep making tiny little adjustments to who gets what, over and over, until everyone is perfectly happy and no one wants to change anything anymore. It's like playing 'give and take' until everyone agrees it's just right. Sometimes, the computer can find this fair way super, super fast, like a speedy race car. We call that 'polynomial-time'. But sometimes, if there are too many friends or too many candies, it can be super, super slow, like a snail trying to cross a whole city! That's 'exponential-time'. One clever trick the computer uses is like a magic shrinking bubble. You start with a big bubble that you know has the perfect sharing spot inside. Then, in each step, the bubble gets smaller and smaller, cutting off all the unfair parts, until only the perfect sharing spot is left inside! But this magic bubble needs to be super, super careful with its numbers, like trying to draw a perfect circle with a wobbly hand, or it might get lost. Another way is like a 'sharing helper' that looks at the two friends who are most unhappy and makes a tiny swap between *just them* to make things a little better, repeating this until everyone is as happy as possible."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\nIn this work, we introduce a new task in the area of _Dataset Distillation_ : learning a tiny set of synthetic\nimages designed to train **linear probes** on top of **pre-trained self-supervised vision models** . Our\nproposed solution, _Linear Gradient Matching_, optimizes a meta gradient objective to ensure that our\nsynthetic images induce training updates similar to those obtained from the real data. Quantitatively,\nour method outperforms all baselines, enabling, for example, a DINO-v2 linear probe to reach 75%\ntest accuracy on ImageNet-1k while only having ever seen **one labeled image** per class.\n\nWe also showcase the importance of secondary aspects of our Linear Gradient Matching method, such\nas the pyramid representation and differentiable augmentations, and highlight their importance in\nlearning highly efficacious distilled datasets. Furthermore, we evaluate our method on challenging finegrained datasets and show that it out-performs the real-image baselines by an even larger margin than\non the standard ImageNet benchmarks. Lastly, our method yields several interesting interpretability\nresults, such as giving insights into how these pre-trained models \u201csee,\u201d predicting how well different\nmodels align, or elucidating a model\u2019s ability to generalize beyond its training distribution.\n\nWe hope our work brings to light the interesting set of problems posed by this new task and inspires\n[others to continue in this area. Code and distilled datasets can be found on our project page.](http://georgecazenavette.github.io/linear-gm)\n\n10\n\n**Acknowledgments**\n\nThis work was supported by the National Science Foundation under Grant No. 2211259, by the\nSingapore DSTA under DST00OECI20300823 (New Representations for Vision, 3D Self-Supervised\nLearning for Label-Efficient Vision), by the Intelligence Advanced Research Projects Activity\n(IARPA) via Department of Interior/Interior Business Center (DOI/IBC) under 140D0423C0075, by\nONR MURI grant #033697-00007, by the Amazon Science Hub, by the MIT-Google Program for\nComputing Innovation, by Sony Interactive Entertainment, and by a 2025 MIT ORCD Seed Fund\nCompute Grant.\n\n11\n\n**References**\n\n[[1] Adam McLean. The surrealism website. https://surrealism.website/. 16](https://surrealism.website/)\n\n[2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski,\nG. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong,\nM. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang,\nY. Lu, C. K. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, S. Zhang,\nM. Suo, P. Tillet, X. Zhao, E. Wang, K. Zhou, R. Zou, X. Wang, A. Mathews, W. Wen, G. Chanan, P. Wu,\nand S. Chintala. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and\ngraph compilation. In _Proceedings of the 29th ACM International Conference on Architectural Support for_\n_Programming Languages and Operating Systems, Volume 2_, ASPLOS \u201924, New York, NY, USA, 2024.\nAssociation for Computing Machinery. 15, 16\n\n[3] L. Bossard, M. Guillaumin, and L. Van Gool. Food-101 \u2013 mining discriminative components with random\nforests. In _ECCV_, 2014. 16, 17, 36\n\n[4] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke,\nJ. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy\nprograms, 2018. 15\n\n[5] M. Caron, H. Touvron, I. Misra, H. J\u00e9gou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in\nself-supervised vision transformers. In _ICCV_, 2021. 3, 10, 16\n\n[6] G. Cazenavette, T. Wang, A. Torralba, A. A. Efros, and J.-Y. Zhu. Dataset distillation by matching training\ntrajectories. In _CVPR_, 2022. 1, 2, 3, 4, 6\n\n[7] G. Cazenavette, T. Wang, A. Torralba, A. A. Efros, and J.-Y. Zhu. Generalizing dataset distillation via\ndeep generative prior. In _CVPR_, 2023. 1, 3, 4, 6, 7\n\n[8] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual\nrepresentations. In _ICML_, 2020. 3\n\n[9] X. Chen, H. Fan, R. Girshick, and K. He. Improved baselines with momentum contrastive learning. _arXiv_\n_preprint arXiv:2003.04297_, 2020.\n\n[10] X. Chen, S. Xie, and K. He. An empirical study of training self-supervised vision transformers. In _ICCV_,\n2021. 3, 5, 8, 17, 21\n\n[11] J. Cui, R. Wang, S. Si, and C.-J. Hsieh. Scaling up dataset distillation to imagenet-1k with constant memory.\nIn _ICML_, 2023. 1, 3\n\n[12] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In _CVPR_, 2009. 2, 5, 8, 10, 16, 17, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n\n[13] Y. Fang, Q. Sun, X. Wang, T. Huang, X. Wang, and Y. Cao. Eva-02: A visual representation for neon\ngenesis. _Image and Vision Computing_, 2024. 3, 16, 20\n\n[14] Y. Fang, W. Wang, B. Xie, Q. Sun, L. Wu, X. Wang, T. Huang, X. Wang, and Y. Cao. Eva: Exploring the\nlimits of masked visual representation learning at scale. In _CVPR_, 2023. 3, 5, 17\n\n[15] S. Fort and J. Whitaker. Direct ascent synthesis: Revealing hidden generative capabilities in discriminative\nmodels. _arXiv preprint arXiv:2502.07753_, 2025. 4\n\n[16] Z. Guo, K. Wang, G. Cazenavette, H. LI, K. Zhang, and Y. You. Towards lossless dataset distillation via\ndifficulty-aligned trajectory matching. In _ICLR_, 2024. 1, 3\n\n[17] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick. Masked autoencoders are scalable vision learners.\nIn _CVPR_, 2022. 3\n\n[18] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation\nlearning. In _CVPR_, 2020. 3\n\n[[19] Hugging Face. Hugging Face: The AI community building the future. https://huggingface.co,](https://huggingface.co)\n2016. 16\n\n[20] M. Huh, B. Cheung, T. Wang, and P. Isola. Position: The platonic representation hypothesis. In _ICML_,\n2024. 2, 3, 9\n\n12\n\n[21] A. Khosla, N. Jayadevaprakash, B. Yao, and L. Fei-Fei. Novel dataset for fine-grained image categorization.\nIn _CVPR_, 2011. 5, 9, 16, 17, 32\n\n[22] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015. 15\n\n[23] P. W. Koh, S. Sagawa, H. Marklund, S. M. Xie, M. Zhang, A. Balsubramani, W. Hu, M. Yasunaga, R. L.\nPhillips, I. Gao, T. Lee, E. David, I. Stavness, W. Guo, B. A. Earnshaw, I. S. Haque, S. Beery, J. Leskovec,\nA. Kundaje, E. Pierson, S. Levine, C. Finn, and P. Liang. WILDS: A benchmark of in-the-wild distribution\nshifts. In _ICML_, 2021. 16\n\n[24] P. Liao, X. Li, X. Liu, and K. Keutzer. The artbench dataset: Benchmarking generative models with\nartworks. _arXiv preprint arXiv:2206.11404_, 2022. 5, 10, 16\n\n[25] D. Liu, J. Gu, H. Cao, C. Trinitis, and M. Schulz. Dataset distillation by automatic training trajectories. In\n_ECCV_, 2024. 1\n\n[26] A. Lynch, G. J.-S. Dovonon, J. Kaddour, and R. Silva. Spawrious: A benchmark for fine control of spurious\ncorrelation biases, 2023. 5, 8, 16\n\n[27] T. Nguyen, Z. Chen, and J. Lee. Dataset meta-learning from kernel ridge-regression. In _ICLR_, 2020. 1, 2\n\n[28] T. Nguyen, R. Novak, L. Xiao, and J. Lee. Dataset distillation with infinitely wide convolutional networks.\nIn _NeurIPS_, 2021. 1\n\n[29] M.-E. Nilsback and A. Zisserman. Automated flower classification over a large number of classes. In\n_Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP)_,\nKolkata, India, 2008. 16, 17, 35\n\n[30] C. Olah, A. Mordvintsev, and L. Schubert. Feature visualization. _Distill_, 2017.\nhttps://distill.pub/2017/feature-visualization. 4, 7\n\n[31] M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. HAZIZA,\nF. Massa, A. El-Nouby, M. Assran, N. Ballas, W. Galuba, R. Howes, P.-Y. Huang, S.-W. Li, I. Misra,\nM. Rabbat, V. Sharma, G. Synnaeve, H. Xu, H. Jegou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski.\nDINOv2: Learning robust visual features without supervision. _TMLR_, 2024. 3, 5, 6, 8, 16, 17, 19, 22, 23,\n24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36\n\n[32] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and\nA. Lerer. Automatic differentiation in pytorch. In _ICLR Workshop_, 2017. 3\n\n[33] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,\nL. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,\nL. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In\n_NeurIPS_ . 2019. 15, 16\n\n[34] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021. 3,\n5, 16, 17, 18\n\n[[35] J. Resig. Ukiyo-e search. https://ukiyo-e.org/, 2012. 16](https://ukiyo-e.org/)\n\n[36] E. Riba, D. Mishkin, D. Ponsa, E. Rublee, and G. Bradski. Kornia: an open source differentiable computer\nvision library for pytorch. In _WACV_, 2020. 16\n\n[37] C. Ryali, Y.-T. Hu, D. Bolya, C. Wei, H. Fan, P.-Y. Huang, V. Aggarwal, A. Chowdhury, O. Poursaeed,\nJ. Hoffman, et al. Hiera: A hierarchical vision transformer without the bells-and-whistles. In _ICML_, 2023.\n3\n\n[38] S. Sagawa, P. W. Koh*, T. B. Hashimoto, and P. Liang. Distributionally robust neural networks. In _ICLR_,\n2020. 5, 8, 16\n\n[39] S. Sagawa, P. W. Koh, T. Lee, I. Gao, S. M. Xie, K. Shen, A. Kumar, W. Hu, M. Yasunaga, H. Marklund,\nS. Beery, E. David, I. Stavness, W. Guo, J. Leskovec, K. Saenko, T. Hashimoto, S. Levine, C. Finn, and\nP. Liang. Extending the wilds benchmark for unsupervised adaptation. In _ICLR_, 2022. 16\n\n[40] B. Son, Y. Oh, D. Baek, and B. Ham. Fyi: Flip your images for dataset distillation. In _ECCV_, 2024. 1, 3\n\n[41] Q. Sun, Y. Fang, L. Wu, X. Wang, and Y. Cao. Eva-clip: Improved training techniques for clip at scale.\n_arXiv preprint arXiv:2303.15389_, 2023. 3\n\n13\n\n[42] Y. Tian, D. Krishnan, and P. Isola. Contrastive multiview coding. In _ECCV_, 2020. 5, 16, 17, 18, 19, 20, 21\n\n[[43] TorchVision maintainers and contributors. Torchvision: Pytorch\u2019s computer vision library. https:](https://github.com/pytorch/vision)\n[//github.com/pytorch/vision, 2016. 16](https://github.com/pytorch/vision)\n\n[44] M. Tschannen, A. Gritsenko, X. Wang, M. F. Naeem, I. Alabdulmohsin, N. Parthasarathy, T. Evans,\nL. Beyer, Y. Xia, B. Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic\nunderstanding, localization, and dense features. _arXiv preprint arXiv:2502.14786_, 2025. 3\n\n[45] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Caltech-UCSD birds 200. Technical Report\nCNS-TR-2011-001, California Institute of Technology, 2011. 5, 9, 17, 33, 34\n\n[46] K. Wang, B. Zhao, X. Peng, Z. Zhu, S. Yang, S. Wang, G. Huang, H. Bilen, X. Wang, and Y. You. Cafe:\nLearning to condense dataset by aligning features. _CVPR_, 2022. 1\n\n[47] T. Wang, J.-Y. Zhu, A. Torralba, and A. A. Efros. Dataset distillation. _arXiv preprint arXiv:1811.10959_,\n2018. 1, 2\n\n[[48] R. Wightman. Pytorch image models. https://huggingface.co/timm, 2019. 16](https://huggingface.co/timm)\n\n[[49] WikiArt.org. WikiArt Visual Art Encyclopedia. https://www.wikiart.org/, 2024. 16](https://www.wikiart.org/)\n\n[50] Z. Yin and Z. Shen. Dataset distillation via curriculum data synthesis in large data era. 2024. 2\n\n[51] Z. Yin, E. Xing, and Z. Shen. Squeeze, recover and relabel: Dataset condensation at imagenet scale from a\nnew perspective. In _NeurIPS_, 2023. 1, 2\n\n[52] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-training. In _ICCV_,\n2023. 3\n\n[53] B. Zhao and H. Bilen. Dataset condensation with differentiable siamese augmentation. In _ICML_, 2021. 1,\n\n4, 7\n\n[54] B. Zhao and H. Bilen. Dataset condensation with distribution matching. _WACV_, 2023. 1, 2\n\n[55] B. Zhao, K. R. Mopuri, and H. Bilen. Dataset condensation with gradient matching. In _ICLR_, 2020. 2, 3, 4\n\n[56] Y. Zhou, E. Nezhadarya, and J. Ba. Dataset distillation using neural feature regression. _NeurIPS_, 2022. 1, 2\n\n14\n\n**Appendix**\n\nTo begin the appendix, we first speak on our method\u2019s limitations, broader impact, and compute\nbudget. In the following sections, we include more information that was omitted from the main\npaper due to space constraints. In Section A, we provide a more detailed description of our method\u2019s\nimplementation details. In Section C, we include visualizations of complete distilled datasets.\n\n**Limitations** Our method is largely limited by memory and data loading. For example, we were limited in our ImageNet-1k experiments to only using 3 rounds of augmentations per batch due to a data\nloading bottleneck. We hope to eventually develop a method that does not require the loading of thousands of real images per step. Additionally, due to the bi-level optimization, we are limited to using Pytorch\u2019s [2, 33] nn.DistributedParallel, rather than nn.DistributedDataParallel,\nwhich causes significant slowdowns due to our very large batch sizes. Using a different automatic\ndifferentiation framework, such as Jax [4], might alleviate this issue, but this would also require\nporting all our self-supervised backbones to the new framework.\n\n**Broader Impact** While our method quantitatively out-performs the real image baselines, the\nbroader impacts likely lie in the introduction of this new task as well as the resulting images acting as\nan interpretability tool. As shown in the paper, the distilled images offer insights into things such as\nmodel alignment and how different networks see the world in varying ways.\n\nFurthermore, the ultimate goal of dataset distillation is the reduction of training time, which equates\nto less energy spent. A linear probe trained on our distilled datasets can be trained to convergence in\njust a few minutes while training on the full dataset can take up to an entire day.\n\n**Compute Budget** We used a variety of GPUs for this work depending on what was available on\nthe shared cluster. Specifically, we used a combination of H200, A100, L40s, Ada6000, and 4090\nGPUs. Distilling ImageNet-100 with the default settings takes about 3 hours using 1 H200 GPU, and\nImageNet-1k takes about 12 hours using 4 H200 GPUs.\n\n**A** **Implementation Details**\n\nIn this section, we provide a more detailed explanation of our methodological and experimental\nimplementation details.\n\n**A.1** **Distillation**\n\n[We implement our method in Pytorch [2, 33], which has a unique license to which we comply.](https://github.com/pytorch/pytorch/tree/main?tab=License-1-ov-file#readme)\n\nWe optimize our pyramid representations using Adam [22] with a learning rate of 0.002.\n\nWe distill for 5000 iterations and add a new level to the pyramids every 200 iterations until we reach\nthe maximum resolution (256).\n\nEach level of the pyramid is initialized with _N_ (0 _,_ 1) normalized by the current number of levels in\nthe pyramid. When a new level is added, the existing levels are appropriately re-normalized.\n\nAt each distillation iteration, we first sample a new linear classifier consisting of a weight matrix _W_\nand bias vector **b** using the default initialization for Pytorch\u2019s nn.Linear. For simplicity, we omit\nthe bias in the main paper.\n\nThen, we reconstruct the synthetic images from their pyramid representations using the following\nequation (copied from the main paper for convenience):\n\n\ufffd\n\n_X_ = [1]\n\n[1]\n\n2 [+ 1]\n\n2 [tanh]\n\n(4)\n\n\ufffd\ufffd _r\u2208\u03c1_\n\nresize256( _Pr_ )\n\n_r\u2208\u03c1_\n\nThe synthetic images are then augmented several (10 by default) _different_ (not sequential) times and\nconcatenated together to form the full synthetic batch. We obtain our synthetic loss ( _\u2113_ syn) by passing\nthis batch through the feature extractor _\u03d5_ and linear classifier ( _W,_ **b** ) and then compute the gradient of\nthe synthetic loss with respect to the linear classifier (and bias). These gradients are then vectorized.\n\n15\n\nFor our real batch, we sample a number of real images equal to the length of the _full_ synthetic batch\n(i.e., after the different augmentations and concatenation) and augment them once (since the batch\nsize already matches that of the full synthetic batch). We then get the real loss and gradients via the\nsame procedure. The meta loss is then calculated as the cosine distance between the real and synthetic\nvectorized gradients.\n\n**A.2** **Augmentation**\n\nFor our differentiable augmentations, we use a custom re-implementation of torchvision transforms that run on GPU and support different random transforms for each element in a batch. We initially used the Kornia [36] implementations, but found that they cause CPU bottlenecks with very large\nbatch sizes. We use RandomHorizontalFlip with default parameters, RandomResizedCrop\nwith size=(224, 224), and RandomGaussianNoise with std=0.2.\n\n**A.3** **Evaluation**\n\n**Linear Probes.** To evaluate a given distilled dataset (or selected coreset for the real baselines) on\na given target model, we first randomly initialize a new linear classifier. We then train the linear\nclassifier for 1000 epochs with a batch size of 100. We use an Adam optimizer with a learning rate of\n0 _._ 001 _/_ 256 (taken from the DINO-v1 [5] evaluation protocol) along with a cosine decay learning rate\nschedule. We stop training early if the test accuracy has not improved over the last 50 epochs. For the\ntraining set, we perform the same set of augmentations as during distillation (horizontal flip, random\nresized crop, and Gaussian noise). The output of the random resized crop is of size 224 _\u00d7_ 224. For the\ntest set, we resize the shortest side to 256 and then do a center crop of 224 _\u00d7_ 224.\n\n**Nearest Neighbors.** For our nearest neighbor evaluation, we first find the train set embeddings\nby resizing the shortest side of each test image to 256, taking a 224 _\u00d7_ 224 center crop, and passing\nthrough the feature extractor. We then do the same for the test set and find the training embedding\nclosest to each test embedding (by cosine distance) and report the class of said neighbor. We only do\none nearest neighbor since we distill down to one image per class.\n\n**A.4** **Datasets**\n\nFor ImageNet [12] and ImageNet [42], we build on the Torchvision [43] implementation as part of the\n[Pytorch [2, 33] ecosystem. ImageNet has a unique non-commercial license, to which we have agreed.](https://image-net.org/download.php)\n\n[Spawrious [26] is taken from the paper\u2019s official GitHub repository and uses a CC0-1.0 license.](https://github.com/aengusl/spawrious)\n\nWaterbirds [38] is taken from the WILDS [23, 39] benchmark. Both the Waterbirds and WILDS\nrepositories use an MIT license.\n\n[Stanford Dogs [21] is taken from the project website. It presumably uses the same license as](http://vision.stanford.edu/aditya86/ImageNetDogs)\nImageNet [12].\n\n[Caltech-UCSD-200-2011 is taken from the project website. No license is given.](http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz)\n\n[ArtBench [24] is taken from the paper\u2019s GitHub repository. The repository itself uses an MIT license.](https://github.com/liaopeiyuan/artbench)\nThe data is sourced from WikiArt [49], Ukiyo-e Search [35], and The Surrealism Website [1], all of\nwhich have Fair Use licenses.\n\nFlowers-102 [29] is taken from Torchvision. No license is given.\n\nFood-101 [3] is taken from Torchvision. No license is given.\n\n**A.5** **Models**\n\n[For CLIP [34], we use the official GitHub repository which has an MIT license.](https://github.com/openai/CLIP)\n\n[For DINO-v1 [5], we use the official GitHub repository which has an Apache-2.0 license.](https://github.com/facebookresearch/dino)\n\n[For DINO-v2 [31], we use the official GitHub repository which has an Apache-2.0 license.](https://github.com/facebookresearch/dinov2)\n\n[For EVA-02 [13], we use the Pytorch Image Models [48] implementation hosted on Hugging Face [19]](https://huggingface.co/timm/eva02_base_patch14_224.mim_in22k)\nwhich has an MIT license.\n\n16\n\nTrain Set Flowers-102 Food-101\n(1 Img/Cls)\n\nCLIP DINO-v2 EVA-02 MoCo-v3 Average CLIP DINO-v2 EVA-02 MoCo-v3 Average\n\nDistilled (Ours) **79.5** _\u00b1_ **1.2 99.6** _\u00b1_ **0.0 97.8** _\u00b1_ **0.1 56.4** _\u00b1_ **3.1 83.3** _\u00b1_ **1.1 78.7** _\u00b1_ **1.6 83.7** _\u00b1_ **0.3 83.9** _\u00b1_ **0.2 31.6** _\u00b1_ **3.2 69.5** _\u00b1_ **1.3**\nNeighbors 69.4 _\u00b1_ 0.8 99.4 _\u00b1_ 0.2 90.8 _\u00b1_ 0.2 48.6 _\u00b1_ 1.8 77.0 _\u00b1_ 0.7 58.8 _\u00b1_ 1.3 73.8 _\u00b1_ 0.4 71.5 _\u00b1_ 0.3 25.2 _\u00b1_ 1.3 57.3 _\u00b1_ 0.8\nCentroids 77.5 _\u00b1_ 0.7 99.4 _\u00b1_ 0.1 95.6 _\u00b1_ 0.3 48.2 _\u00b1_ 2.6 80.2 _\u00b1_ 0.9 74.1 _\u00b1_ 0.6 75.6 _\u00b1_ 0.1 76.4 _\u00b1_ 0.4 22.7 _\u00b1_ 2.0 62.2 _\u00b1_ 0.8\n\nRandom 67.9 _\u00b1_ 1.1 98.8 _\u00b1_ 0.4 91.0 _\u00b1_ 0.8 36.8 _\u00b1_ 2.2 73.6 _\u00b1_ 1.1 48.5 _\u00b1_ 1.0 52.8 _\u00b1_ 1.9 49.9 _\u00b1_ 0.6 13.5 _\u00b1_ 1.7 41.2 _\u00b1_ 1.3\n\nFull Dataset 93.4 _\u00b1_ 0.1 99.7 _\u00b1_ 0.0 98.9 _\u00b1_ 0.0 82.2 _\u00b1_ 0.1 93.5 _\u00b1_ 0.1 91.9 _\u00b1_ 0.0 92.7 _\u00b1_ 0.1 92.0 _\u00b1_ 0.0 78.3 _\u00b1_ 0.0 88.7 _\u00b1_ 0.0\n\nTable B.1: **Additional Datasets:** Distilling Flowers-102 [29] and Food-101 [3] show similar results; our distilled\ndataset consistently out-perform the real-image baselines.\n\n[For MoCo-v3 [10], we use the official GitHub repository which has a Creative Commons Attribution-](https://github.com/facebookresearch/moco-v3)\nNonCommercial 4.0 International Public License.\n\n**B** **Additional Results**\n\nIn Table B.1, we include results for Flowers-102 [29] and Food-102 [3]. We see results following the\nsame trends as the rest of the paper; the distilled datasets out-perform the real-image baselines across\nthe board.\n\n**C** **Visualizing Full Datasets**\n\nIn Figures C.1, C.2, C.3, and C.4 we present the entirety of ImageNet-100 [42] distilled with\nCLIP [34], DINO-v2 [31], EVA-02 [14], and MoCo-v3 [10] respectively. In Figures C.5\u2013C.14, we\nshow ImageNet-1k [12] distilled with DINO-v2 and omit the other models for the sake of brevity\n(and ink).\n\nWe also include Stanford Dogs [21] (Figure C.15), CUB-200-2011 [45] (Figures C.16 and C.17),\nFlowers-102 [29] (Figure C.18), and Food-101 [3] (Figure C.19) distilled with DINO-v2 and once\nagain omit the other models. All distilled datasets, including those not inculded here, can be viewed\n[on our project page.](http://georgecazenavette.github.io/linear-gm)\n\n17\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-17-0.png)\n\nFigure C.1: ImageNet-100 [42] distilled using CLIP [34]\n\n18\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-18-0.png)\n\nFigure C.2: ImageNet-100 [42] distilled using DINO-v2 [31]\n\n19\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-19-0.png)\n\nFigure C.3: ImageNet-100 [42] distilled using EVA-02 [13]\n\n20\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-20-0.png)\n\nFigure C.4: ImageNet-100 [42] distilled using MoCo-v3 [10]\n\n21\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-21-0.png)\n\nFigure C.5: ImageNet-1k [12] distilled with DINO-v2 [31] classes [0-99]\n\n22\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-22-0.png)\n\nFigure C.6: ImageNet-1k [12] distilled with DINO-v2 [31] classes [100-199]\n\n23\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-23-0.png)\n\nFigure C.7: ImageNet-1k [12] distilled with DINO-v2 [31] classes [200-299]\n\n24\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-24-0.png)\n\nFigure C.8: ImageNet-1k [12] distilled with DINO-v2 [31] classes [300-399]\n\n25\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-25-0.png)\n\nFigure C.9: ImageNet-1k [12] distilled with DINO-v2 [31] classes [400-499]\n\n26\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-26-0.png)\n\nFigure C.10: ImageNet-1k [12] distilled with DINO-v2 [31] classes [500-599]\n\n27\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-27-0.png)\n\nFigure C.11: ImageNet-1k [12] distilled with DINO-v2 [31] classes [600-699]\n\n28\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-28-0.png)\n\nFigure C.12: ImageNet-1k [12] distilled with DINO-v2 [31] classes [700-799]\n\n29\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-29-0.png)\n\nFigure C.13: ImageNet-1k [12] distilled with DINO-v2 [31] classes [800-899]\n\n30\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-30-0.png)\n\nFigure C.14: ImageNet-1k [12] distilled with DINO-v2 [31] classes [900-999]\n\n31\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-31-0.png)\n\nFigure C.15: Stanford Dogs [21] distilled using DINO-v2 [31]\n\n32\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-32-0.png)\n\nFigure C.16: CUB-200-2011 [45] distilled using DINO-v2 [31] classes [0-99]\n\n33\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-33-0.png)\n\nFigure C.17: CUB-200-2011 [45] distilled using DINO-v2 [31] classes [100-200]\n\n34\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-34-0.png)\n\nFigure C.18: Flowers-102 [29] distilled using DINO-v2 [31]\n\n35\n\n![](papers/images/Dataset-Distillation-for-PreTrained-SelfSupervised-Vision-Models.pdf-35-0.png)\n\nFigure C.19: Food-101 [3] distilled using DINO-v2 [31]\n\n36"}, {"role": "assistant", "content": "**Analogy:** Imagine you have a super smart robot who loves looking at pictures, and you want to teach it to recognize different types of flowers. Instead of showing it millions of real flower photos, which takes forever, what if you could make just ONE special, magic drawing for each flower type? This drawing isn't real, but it's so good that it teaches the robot everything it needs to know about that flower, super fast, just like it saw all the real photos!\n\n**Explanation:** This paper shows how to make those magic drawings, which they call 'synthetic images.' These tiny, fake pictures are designed specifically to teach a part of the robot's 'brain' that already knows a lot about images from looking at other pictures. By making sure the robot learns the same way from these few fake pictures as it would from many real ones, they can teach it much quicker. They even found that their magic drawings sometimes teach the robot even better than showing it real pictures, especially for hard-to-find things like tiny differences in birds!"}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\nWe followed the qualitative case study guidelines established by\nRuneson and H\u00f6st et al. [30], and aligned our study design and\nreporting with the empirical standards proposed by Ralph et al. [28]\nto ensure transparency, triangulation, and validity (see Figure 1).\n\n**Figure 1: Research method followed.**\n\n2\n\n![](papers/images/Data-Annotation-Quality-Problems-in-AIEnabled-Perception-System-Development.pdf-1-0.png)\nData Annotation Quality Problems in AI-Enabled Perception System Development,,\n\n**Research Context** This study is part of Project X, which aims\nto enhance academia-industry collaboration by developing concepts, models, and techniques for effective and safe AIePS productions. We conducted 19 semi-structured interviews [30] with 20\nparticipants from industry and academia. These interviews were\nperformed online between October 2024 and April 2025, each lasting \u2265 120 minutes. ID16 participated in two separate interviews\n(labelled 16A and 16B), each lasting approximately four hours. In\ncontrast, participants ID18 and ID19 took part in a single focus\ngroup session, counted as one interview (see Table 1). In total, this\namounts to over _**50 hours of rich transcripts**_ . Using purposeful\nsampling [28], we selected participants based on their roles, organisational context, and experience with data annotation or AI model\ndevelopment in autonomous driving. Interviews were recorded,\ntranscribed, manually corrected, and concluded with participant\nfeedback. This approach ensured diverse and in-depth insights into\nannotation quality, associated errors, and their impact on the AIePS\ndevelopment life cycle.\n**Companies Context** To capture diverse perspectives, we conducted fourteen interviews with representatives from six companies engaged in AIePS development. We included the entire supply\nchain: one OEM, three Tier-1 suppliers, two Tier-2 suppliers, a\nperception training company, and a code analysis firm, all based in\ntwo European countries and the UK.\nAdditionally, five interviewees from four research institutes,\nincluding a state-owned European institute and leading universities from Europe and the UK, helped bridge theory and practice\nacross AI, data annotation, quality, and regulation. Thematic Saturation was reached with Interview 15, with no new information or\ncandidate themes emerging in Interviews 16, 17, 18, and 19.\n**Participants** As shown in Table 1, targeted experts included\nleading contributors to AIePS, such as safety standard developers,\nfounders of relevant companies, and the chief investigator of toptier research labs, ensuring deep domain expertise. This is a niche\ndomain where only a few globally recognised organisations and\nexperts are active. We deliberately reached out to these leading\nactors and succeeded in including the most relevant and established\nexperts in this emerging field.\n**Interview Preparation:** A semi-structured interview guide [28]\nwas developed iteratively by four researchers in collaboration with\nthree industrial experts experienced in data annotation and AIePS,\nthereby establishing content validity. The guide featured openended, neutral, and clear questions aligned with the research questions and relevant literature presented in the background and related work section 2. To enhance clarity, structure, and duration, the\nguide was piloted with two independent industry partners (not part\nof the final study), representing annotation requirements (Tier 1)\nand production (Tier 2), whose feedback informed refinements. (For\ndetails, see: \u2192 [Interview Guide)](https://dataverse.harvard.edu/previewurl.xhtml?token=da9478c0-7653-48cb-bc5f-b8ae1d0810e8)\n**Data Analysis:** We applied a six-phase thematic analysis process following Braun and Clarke _et al._ [5]. The analysis was guided\nby a combined deductive\u2013inductive codebook, and the units of\nanalysis were 19 interview transcripts, each coded in text chunks\nof approximately three sentences. Across all transcripts, about 1050\ntext segments (coded units) were analysed.\n**(1) Familiarisation:** Two researchers transcribed, anonymised,\nand validated all interviews to ensure ethical integrity and data\n\n**Table 1: Categorisation of interviewees based on company**\n**type and specialisation, including their roles and years of**\n**experience**\n\n**Company ID: Type** **Focus Area** **Expert (Years of Experience)**\n\nA: Tier 2 Data Annotation ID1: Annotation Expert (6)\nID2: Perception Expert (10)\nID3: Quality Expert (9)\n\nB: Tier 1 Safety Software ID4: Machine Learning Expert (7)\nID5: Data Scientist (11)\n\nC: University Research ID6: Senior Researcher (9)\n\nD: University Research ID7: Researcher (8)\n\nE: University Research ID8: Researcher (5)\n\nF: Research Institute Research ID9: Researcher (10)\nID10: Researcher (20)\n\nG: OEM Automotive ID11: Machine Learning Expert (5)\nID12: V&V Expert (5)\nID13: Data Engineer (4)\nID14: Researcher (3)\n\nH: Tier 1 Safety Software ID15: Researcher (10)\n\nI: Tier 2 Quality Assurance ID16 A: Quality Expert (18)\nI: Tier 2 Quality Assurance ID16 B: Quality Expert (18)\n\nJ: Tier 1 Digital Solution ID17: Head of Research (17)\nID18,19: Research Engineer (1,2)\n\naccuracy. Through multiple readings, they familiarised themselves\nwith the data, developing an initial understanding of emerging\npatterns and potential themes.\n**(2) Initial Coding:** A deductive codebook was first developed\nbased on the research questions and interview guide (see:\u2192 [Code-](https://dataverse.harvard.edu/previewurl.xhtml?token=da9478c0-7653-48cb-bc5f-b8ae1d0810e8)\n[book and \u2192](https://dataverse.harvard.edu/previewurl.xhtml?token=da9478c0-7653-48cb-bc5f-b8ae1d0810e8) [Codebook-Aligned Initial Codes). During open coding,](https://dataverse.harvard.edu/previewurl.xhtml?token=da9478c0-7653-48cb-bc5f-b8ae1d0810e8)\ninductive codes were added as new insights emerged. Each transcript (ID1\u2013ID19) was coded systematically, resulting in a compre[hensive set of detailed codes extracted from all interviews (see: \u2192](https://dataverse.harvard.edu/previewurl.xhtml?token=da9478c0-7653-48cb-bc5f-b8ae1d0810e8)\n[Complete List of Emerging Codes from Transcripts).](https://dataverse.harvard.edu/previewurl.xhtml?token=da9478c0-7653-48cb-bc5f-b8ae1d0810e8)\n**(3) Code Validation:** Two researchers independently coded all\ntranscripts and cross-verified results, achieving a strong inter-coder\nreliability (Cohen\u2019s _\ud835\udf05_ = 0 _._ 8) corresponding to 80% agreement. The\nremaining minor differences (about 20%) were resolved through\ndiscussion and consensus, ensuring the consistency and credibility\nof the coding process.\n**(4) Theme Development:** All codes were clustered into potential themes, related to the three key data quality dimensions\n_Completeness, Consistency, and Accuracy_ and aligned with the predefined research questions (see: \u2192 [Detailed Themes Development).](https://dataverse.harvard.edu/previewurl.xhtml?token=da9478c0-7653-48cb-bc5f-b8ae1d0810e8)\nA consolidated analysis produced an initial _Master List of Data_\n_Annotation Errors_, capturing all identified errors across the AIePS\ndevelopment lifecycle. The master list compiles identified errors organised by their nature, frequency, and impact, with representative\nexamples forming the basis for the final taxonomy of annotation\n\nerrors.\n\n**(5) Theme Refinement:** The five researchers collaboratively\nreviewed, merged, and refined overlapping codes through a threestage iterative process to ensure analytical robustness. In the first\nround, all codes within each theme were compiled, and their frequencies ( _i.e.,_ the number of mentions across interviews were\nrecorded. Across the three quality dimensions, we identified 23\ncodes under _Completeness_, 23 under _Consistency_, and 18 under _Ac-_\n_curacy_ . In the second round, codes mentioned three times or fewer\n\n3\n\n,, Saeeda et al.\n\nwere classified as low-frequency. In the final round, these were\nremoved, retaining only the most recurrent data annotation errors\nconsistently mentioned by multiple experts. This resulted in six\nhigh-frequency codes for _Completeness_, seven for _Consistency_, and\nfive for _Accuracy_ (see: \u2192 [Final Refine Data Annotation Errors List).](https://dataverse.harvard.edu/previewurl.xhtml?token=da9478c0-7653-48cb-bc5f-b8ae1d0810e8)\n**(6) Reporting:** The final themes and associated codes were\nverified, documented, and linked with representative quotes to\nmaintain transparency and traceability. Five researchers collaboratively reviewed the complete analysis using Excel, OneDrive, and\nMiro, ensuring structured documentation and analytic rigour.\n**Replication Package:** We make the interview guide, codebook,\nthematic analysis themes, and validation survey available to support\nfuture researchers in replicating the study. The replication package\nis available on HARVARD Dataverse [1] ."}, {"role": "assistant", "content": "**Analogy:** It's like when you want to know why your toy car keeps falling over. You don't just guess; you ask all your friends who are super good at playing with toy cars, \"Why do you think it falls?\"\n\n**Explanation:** The grown-ups wanted to find out why the 'eyes' of smart robots, like self-driving cars, sometimes make mistakes. So, they talked to many very smart people who build these robot eyes, like 20 of them! They asked them lots of questions for a very, very long time, like listening to stories for over 50 hours! Then, two grown-ups read all the stories and wrote down all the problems they heard, like making lists of \"red blocks\" and \"blue blocks\". If both grown-ups agreed on a problem, they knew it was important. They kept only the problems that many, many smart people talked about, so they knew which ones were the biggest worries for the robot eyes."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n**Algorithmic framework and results** **23**\n5.1 Robust bisection algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n5.2 Robust initialization for symmetry breaking . . . . . . . . . . . . . . . . . . . 25\n5.3 Boosting via pairwise majority voting . . . . . . . . . . . . . . . . . . . . . . 25"}, {"role": "assistant", "content": "**Analogy:** Imagine you're playing a game with your friends to find the best candy.\n\n**Explanation:** The 'Robust bisection algorithm' is like when you want to find a hidden toy in a long line of boxes. Instead of opening every box, you open the middle one. If the toy isn't there, you know it's either in the boxes to the left or the right, so you throw away half the boxes you don't need! You keep doing this until you find it. 'Robust' means it works even if some boxes are a bit squishy or hard to tell apart. 'Robust initialization for symmetry breaking' is like when you and your friend both want to start a race from the exact same spot. If you both start exactly the same, you might bump into each other. So, you each take a tiny, special step to the side before the 'Go!' so you can run your own race without crashing. 'Robust' means these little steps always make sure you don't get stuck together. 'Boosting via pairwise majority voting' is like when you want to pick the best ice cream flavor. Instead of everyone yelling their favorite, you compare two flavors at a time (like vanilla versus chocolate). The one that gets more votes wins that little match. You do this for all possible pairs, and the flavor that wins the most little matches overall becomes the big winner!"}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n_rpsi,j_ ( _si_ ( _j_ ) _, sj_ ( _i_ )) = _i for all j \u2208_ _`UndecidedNeighbors`_ **then**\n\n**8** Output 1 (join MIS and terminate);\n\n```\n   /* Round 3: Observe Undecided Neighbors\u2019 Outputs */\n```"}, {"role": "assistant", "content": "**Analogy:** Imagine a game where a kid wants to join a special 'Leader Club,' but no two leaders can sit right next to each other. This kid looks at all their friends who haven't decided yet if *they* want to be leaders.\n\n**Explanation:** If *every single one* of those undecided friends points at this kid and says, 'Yes! You should be in the Leader Club!', then this kid gets to join the club right away and stops playing that round. They are finished with the game."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n**Design Principles & Goals** . To facilitate practical adoption,\nTLT adheres to four key design principles:\n(a) _Lossless Guarantee_ . System optimizations must be lossless,\npreserving the mathematical equivalence in both rollout and\ntraining stages compared to the original algorithm, thereby\navoiding asynchronous staleness or lossy approximations.\n(b) _Interference-Free_ . The adaptive drafter workload must not\ninterfere with the primary RL workload. Drafter updates\nare performed opportunistically (as spot tasks) and can be\nsmoothly preempted to minimize impact on the RL workload.\n(c) _Automatic and Simple_ . Manually setting up the draft model\nand configuring its speculative hyperparameters are burdensome and often lead to suboptimal performance. Thus, TLT\nfeatures an automated workflow for ease of use.\n\n(d) _Generalizable and Scalable_ . The design must be adaptable\nto various popular reasoning RL algorithms (e.g., GRPO [51],\nRLOO [1]). Furthermore, TLT should scale effectively and deliver continuous system performance improvements across\ndifferent model sizes, architectures, and cluster scales.\nIn addition, TLT has three primary objectives: (1) Mitigate\nthe long-tail problem to accelerate reasoning RL training; (2)\nEnhance cluster utilization and minimize resource waste; (3)\nProduce a high-performance draft model for future deployment with speculative decoding at no extra cost.\n\n4\n\n**Architecture & Workflow** . Figure 6 illustrates the overall\narchitecture of TLT and its workflow. The system is composed of two tightly integrated components: the Adaptive\nDrafter (\u00a74) and the Adaptive Rollout Engine (\u00a75).\nThe rollout process exhibits two key characteristics: a\nlong-tail distribution of sequence lengths and progressively\nshrinking batch sizes as responses complete. To adapt to\nthese dynamics, the Adaptive Rollout Engine maintains a\npool of pre-captured CUDAGraphs for both target and draft\nmodels, managed by the Adaptive SD Manager. This manager\nleverages the proposed BEG-MAB tuner to automatically\nselect appropriate SD strategies for each input batch. As\nshown in the top of Figure 6, the selected drafter CUDAGraph\ngenerates candidate tokens (\u2460), which are then passed to the\ncorresponding target CUDAGraph for parallel verification\n\n  (\u2461 \u2462). The engine measures acceptance length and step\nlatency (\u2463), feeding these signals back to the SD Manager\n(\u2464) to refine strategy selection in real time.\nMeanwhile, the Adaptive Drafter module is responsible for\ncontinuously updating the lightweight draft model without\ninterfering with the main rollout workload. A centralized\nWorker Coordinator monitors the state of rollout workers.\n\nWhen idle resources appear during the long-tail stage, the\ncoordinator opportunistically launches Spot Trainer tasks.\nThe training tasks use the Online DataBuffer, which caches\nhidden states and input embeddings from ongoing and prior\nsteps\u2019 rollout. The draft model receives efficient updates\nusing techniques such as zero-padding packing and selective asynchronous checkpointing. This design ensures that\ndrafter updates can be preempted and resumed seamlessly,\nexploiting wasted bubbles.\nBy systematically combining adaptive rollout with opportunistic drafter training, TLT achieves substantial speedups\nfor reasoning RL without compromising model quality, yielding a valuable draft model as a free by-product."}, {"role": "assistant", "content": "**Analogy:** Imagine you're building a super tall tower with Lego blocks, but you have two builders. One builder is super-fast but sometimes makes tiny mistakes (the 'helper' builder). The other builder is very careful and always perfect but a bit slower (the 'main' builder). The helper builder quickly puts together a whole section of the tower, and then the main builder just has to check if it's all good, instead of building block by block. If it's mostly right, they save a lot of time! When the main builder is resting or thinking, the helper builder secretly practices building new sections to get even better, so they don't bother the main builder.\n\n**Explanation:** This system helps computers learn tricky things much faster, like how to solve puzzles. It does this by having a quick 'helper' computer that tries to guess the next few steps in the puzzle solution all at once. Then, a more careful 'main' computer just checks if the guesses are right, which is much quicker than figuring out each step from scratch. The helper computer also learns new tricks when the main computer isn't super busy, making the whole learning process speedy and smart without making any mistakes."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n**3.1** **Overview**\n\nDynaExq provides adaptive expert precision control for MoE inference by treating precision as a runtime-managed resource rather\nthan a fixed configuration. Instead of relying on offline PTQ decisions that assume static activation distributions, DynaExq continuously monitors expert usage and adjusts precision according to\nlong-term activation salience. This enables hot experts to retain accurate high-precision weights, while cold experts are aggressively\ncompressed to reduce the overall HBM footprint without affecting\nthe forward path.\nAs shown in Fig. 2, DynaExq consists of four coordinated components:\n\n - (1) **Expert Precision Controller**, which aggregates router\nstatistics, identifies persistently activated experts, and issues\npromotion or demotion commands.\n\n - (2) **Asynchronous Precision Switching Pipeline**, which\ntransfers expert weights across SSD, DRAM, and HBM with full\ncompute\u2013communication overlap.\n\n - (3) **Memory Pool Manager**, which maintains deterministic,\nfragmentation-free memory allocation for mixed-precision experts.\n\n - (4) **Resource-Aware Initialization Module**, which establishes\nper-layer precision budgets that guarantee memory stability\nunder variable workloads.\n\nTogether, these components support accuracy-aware precision allocation, non-blocking transitions, and stable throughput. By decoupling precision switching from the critical compute path, DynaExq\nsustains aggressive memory savings even when expert usage shifts\n\nover time.\n\n**3.2** **Runtime Expert Precision Controller**\n\nThe controller maintains a lightweight profile of expert behavior by\nsampling router outputs during inference. After each forward pass,\nDynaExq records the selected experts and accumulates gating probabilities, providing a workload-driven view of expert importance\nrather than relying on synthetic calibration data.\nFor each expert _\ud835\udc56_, the exponential-moving-average (EMA) hotness score within a window of length _\ud835\udc47_ is:\n\n_\ud835\udc46\ud835\udc56_ [(] _[\ud835\udc61]_ [)] = _\ud835\udefc\ud835\udc46\ud835\udc56_ [(] _[\ud835\udc61]_ [\u2212][1][)] + (1 \u2212 _\ud835\udefc_ ) _\ud835\udc54\ud835\udc56_ ( _\ud835\udc65\ud835\udc61_ ) _,_ (2)\n\n![](papers/images/Dynamic-Expert-Quantization-for-Scalable-MixtureofExperts-Inference.pdf-2-0.png)\n\n|U|Col2|\n|---|---|\n|High-precision|Low-precision|\n|High-precision|KV-Cache|\n\n**Figure 2: System architecture of DynaExq.**\n\n**Algorithm 1** Runtime Precision Scheduling for MoE Experts\n\n1: **Input:** Router probabilities _\ud835\udc54\ud835\udc56_ ( _\ud835\udc65\ud835\udc61_ ) for experts _\ud835\udc56_ = 1 _..\ud835\udc41_\n\n2: **State:** EMA hotness scores _\ud835\udc46\ud835\udc56_, precision state state[ _\ud835\udc56_ ] \u2208\n{HIGH _,_ LOW}\n\n3: **Parameter:** Update period _\ud835\udc47_, EMA factor _\ud835\udefc_, threshold _\ud835\udf0f_ (derived from memory budget)\n\n4: **procedure** UpdateHotness( _\ud835\udc61_ )\n\n5: **for** each activated expert _\ud835\udc56_ at step _\ud835\udc61_ **do**\n\n6: _\ud835\udc46\ud835\udc56_ \u2190 _\ud835\udefc\ud835\udc46\ud835\udc56_ + (1 \u2212 _\ud835\udefc_ ) _\ud835\udc54\ud835\udc56_ ( _\ud835\udc65\ud835\udc61_ )\n\n7: **end for**\n\n8: **for** each inactive expert _\ud835\udc57_ **do**\n\n9: _\ud835\udc46_ _\ud835\udc57_ \u2190 _\ud835\udefc\ud835\udc46_ _\ud835\udc57_ _\u22b2_ passive decay\n\n10: **end for**\n\n11: **end procedure**\n\n12: **procedure** PrecisionSchedule( _\ud835\udc61_ )\n\n13: **if** _\ud835\udc61_ mod _\ud835\udc47_ = 0 **then**\n\n14: Sort experts by _\ud835\udc46\ud835\udc56_ in descending order\n\n15: H \u2190 top- _\ud835\udc3e_ experts with _\ud835\udc46\ud835\udc56_ \u2265 _\ud835\udf0f_\n\n16: **for** each expert _\ud835\udc56_ **do**\n\n17: **if** _\ud835\udc56_ \u2208H and state[ _\ud835\udc56_ ] = LOW **then**\n\n18: EnqeueUpgrade( _\ud835\udc56_ )\n\n19: **else if** _\ud835\udc56_ \u2209 H and state[ _\ud835\udc56_ ] = HIGH **then**\n\n20: EnqeueDowngrade( _\ud835\udc56_ )\n\n21: **end if**\n\n22: **end for**\n\n23: **end if**\n\n24: **end procedure**\n\nwhere _\ud835\udc54\ud835\udc56_ ( _\ud835\udc65\ud835\udc61_ ) is the router probability at step _\ud835\udc61_ . This formulation\ncaptures both activation frequency and magnitude, while smoothing short-term spikes. Experts with _\ud835\udc46\ud835\udc56_ above the high-precision\nthreshold _\ud835\udf0f\u210e_ are promoted, while the remaining experts form the\nlow-precision candidate pool (as shown in Algo-1).\nTo ensure robust scheduling under fluctuating workloads, the\ncontroller operates at coarse granularity and applies smoothing to\navoid oscillations. Precision updates occur periodically rather than\nper batch, preventing thrashing when gating behavior is unstable.\n\n**MoE**\n\n**Inference**\n\n**H2D**\n\n**Transfer**\n\n**SSD**\n\n**Prefetch**\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n||**ei**|**ej**|**ek**|**em**|**en**|||\n|||||||||\n\n|Col1|Col2|Col3|downgrade|Col5|Col6|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n||**upgradeek**||||**upgrade e**|||\n||||**en**|**en**|**x**|**x**|**x**|\n\n|Col1|Col2|Col3|Col4|\n|---|---|---|---|\n||**Fetche**|||\n||**n**|**n**|**n**|\n\n**Figure 3: An example of Asynchronous Expert Swapping**\n**Pipeline.**\n\nBecause all operations execute on the CPU and do not block GPU\nkernels, the controller imposes no latency overhead on inference.\nThese mechanisms allow DynaExq to detect persistent hot experts reliably, enforce stable precision decisions, and preserve routing fidelity under strict memory constraints.\n\n**3.3** **Asynchronous Expert Swapping Pipeline**\n\nTo support continuous inference while precision changes occur\nin the background, DynaExq employs a multi-stage asynchronous\npipeline that overlaps promotion, demotion, and prefetching with\nMoE computation. High-precision and low-precision weights are\nstored on SSD and cached in DRAM, and all transfers to HBM use\n\ndedicated CUDA streams.\n\nWhen a cold expert becomes hot, the runtime triggers a nonblocking upgrade sequence: the high-precision weight is fetched\nfrom SSD (if not cached), buffered in DRAM, and streamed into\nHBM. After the transfer completes, a lightweight registration step\nbinds the new weight to the expert\u2019s compute context, and the\nlow-precision version is reclaimed immediately. Demotion mirrors\nthis process in reverse.\nDuring inference, DynaExq always uses the last stable version\nof each expert. If a promotion is in flight, the previous representation remains active, ensuring that pipeline latency never affects\nthe forward pass. This provenance-consistent strategy prevents\ncorrectness issues and avoids compute stalls even under highly\ndynamic workloads.\nTo further hide data movement latency, DynaExq proactively\nprefetches experts predicted to become hot by leveraging crosslayer activation correlations. As illustrated in Fig. 3, SSD prefetch,\nDRAM buffering, H2D transfers, and demotions proceed concurrently beneath ongoing computation, enabling smooth precision\n\ntransitions.\n\n**3.4** **Memory Pool Management**\n\nBecause precision switching changes expert storage sizes, na\u00efvely\nusing the GPU allocator can cause fragmentation and unpredictable\nmemory growth. To prevent this, DynaExq partitions HBM into\ntwo dedicated pools for high- and low-precision experts. Each pool\nis subdivided into fixed-size blocks aligned with the corresponding data format (e.g., FP16 or INT4), enabling constant-time and\nfragmentation-free allocation.\nDuring precision transitions, DynaExq immediately reclaims the\nold representation once the new weight is successfully registered,\nensuring that no expert holds dual copies longer than necessary.\nA small transient buffer absorbs transfer bursts, allowing multiple\nin-flight promotions without exceeding the memory budget.\n\nKexin Chu, Dawei Xiang, Zixu Shen, Yiwei Yang, Zecheng Lin, and Wei Zhang\n\nA compact CPU-side index records expert locations, precision\ntiers, and pool occupancy. Because this index is managed outside the\nGPU allocator, DynaExq avoids runtime cudaMalloc calls and eliminates allocator-induced jitter. This deterministic memory structure\nmaintains stable throughput even when experts frequently change\nprecision.\nBy imposing structure on the memory hierarchy and ensuring deterministic allocation rules, DynaExq provides predictable\nmemory behavior and safe operation under aggressive dynamic\nquantization.\n\n**3.5** **Resource-Aware Threshold Initialization**\n\nBefore inference begins, DynaExq computes safe precision budgets\nfor each MoE layer to ensure that dynamic switching never violates\nHBM constraints. Let _\ud835\udc40_ HBM denote available memory after reserving space for KV-cache, activations, and shared weights. Given the\naverage storage sizes of high-precision and low-precision experts,\n_\ud835\udc46_ [hot] and _\ud835\udc46_ [cold], and the number of experts _\ud835\udc41_, DynaExq computes\nfeasible values of _\ud835\udc5b_ hot and _\ud835\udc5b_ cold that satisfy:\n\n_\ud835\udc5b_ hot \u00b7 _\ud835\udc46_ [hot] + _\ud835\udc5b_ cold \u00b7 _\ud835\udc46_ [cold] \u2264 _\ud835\udc40_ HBM _,_\n\nWhere _\ud835\udc5b_ hot + _\ud835\udc5b_ cold = _\ud835\udc41_ and _\ud835\udf0f\u210e_ = _\ud835\udc61\ud835\udc5c\ud835\udc5d_ ( _\ud835\udc5b\u210e\ud835\udc5c\ud835\udc61,\ud835\udc46\ud835\udc56_ ). A brief warmup\nphase collects initial hotness scores, and the top- _\ud835\udc5b_ hot experts initialize the high-precision set. The threshold _\ud835\udf0f\u210e_ remains fixed while\nexpert identities may change at runtime based on observed activity.\nBy separating memory feasibility analysis from runtime scheduling, DynaExq guarantees that all precision transitions remain\nmemory-safe even under shifting workloads. This initialization step\nforms the foundation for predictable and stable mixed-precision\n\nexecution."}, {"role": "assistant", "content": "**Analogy:** Imagine a big toy box (that's your computer's special fast memory, HBM) where you keep all your toys (these are like 'expert programs'). Some toys are your absolute favorites (these are 'hot experts'), and you want them to be shiny and perfect, right at the top, easy to grab. Other toys are ones you don't play with very often (these are 'cold experts'), so your grown-up might put them in smaller, simpler boxes or even in a closet (slower memory like DRAM or SSD) to make more room in the main toy box. This grown-up (DynaExq) watches which toys you play with a lot. If a toy becomes your new favorite, they quickly make it shiny and bring it to the top. If you stop playing with a toy, they carefully put it away. They do all this moving around without ever stopping you from playing with your current toy!\n\n**Explanation:** This system helps computers save space in their fastest memory (HBM) by figuring out which 'expert programs' are used a lot and keeping them super clear and ready. The 'experts' not used often are made smaller or stored in a different, slower place. This grown-up-like helper has four special helpers: one that watches which toys are 'hot' or 'cold', another that moves toys around quickly and quietly without stopping your play, one that makes sure the toy box never gets messy when toys are moved, and finally, one that figures out beforehand how many shiny toys and how many stored-away toys can fit safely in the toy box without overflowing."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n_\ud835\udc61\ud835\udc50\ud835\udc5d_\ud835\udc53\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc60\ud835\udc60_ **then** _\ud835\udc50\ud835\udc62\ud835\udc4f\ud835\udc56\ud835\udc50_ _ _\ud835\udc61\ud835\udc50\ud835\udc5d_ _ _\ud835\udc53\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc60\ud835\udc60_ ()\n\n**32** _\ud835\udc50\ud835\udc62\ud835\udc4f\ud835\udc56\ud835\udc50_ _ _\ud835\udc61\ud835\udc50\ud835\udc5d_ _ _\ud835\udc53\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc60\ud835\udc60_ () :\n**begin**\n\n**33** _\ud835\udc4a\ud835\udc61\ud835\udc50\ud835\udc5d_ \u2190\u2212 _\ud835\udc4a\ud835\udc61\ud835\udc50\ud835\udc5d_ + 23\u2212 _\ud835\udefd\ud835\udefd_ [\u2217] _[\ud835\udc4e\ud835\udc50\ud835\udc58]_ _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ [_] _[\ud835\udc50\ud835\udc5b\ud835\udc61]_\n\n**34** _\ud835\udc4e\ud835\udc50\ud835\udc58_ _ _\ud835\udc50\ud835\udc5b\ud835\udc61_ \u2190\u2212 0"}, {"role": "assistant", "content": "**Analogy:** Imagine you are building a LEGO tower with your friends, and everyone wants to add their own bricks. This rule is like saying, 'Every time you successfully put a brick on your tower without it falling over, you get to try and add a *tiny bit more* next time.'\n\n**Explanation:** When your computer sends information, it's like sending little toy trucks down a road. When a truck gets to its destination and sends a 'Got it!' postcard back (that's an 'ack'), your computer thinks, 'Good! I can try to send a *little* bit more next time.' This special rule helps the computer figure out just how much more to send so it doesn't send too many trucks and cause a big traffic jam for everyone else on the road. After it decides, it clears its count of 'Got it!' postcards and waits for new ones."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\nWe now analyse how fast the Gaussian score approximation from Eq. (7) converges to the true Gaussian\nES matrix gradient in Eq. (4). We introduce the following formal regularity assumption for the fitness\nfunction:\n**Assumption 2.** _Assume that f_ ( _M_ ) _is bounded, that is_ sup _M |f_ ( _M_ ) _| < \u221e._\n\nDenote the true full-rank Gaussian ES gradient as _g_ True := _\u2207\u00b5J_ ( _\u00b5_ ). Our key theoretical result characterises\nthe error rate between the Gaussian score approximator in the low-rank update \u02c6 _g_ LR _[r]_ [from Eq. (][5][) and the true]\ngradient using the matrix Frobenius norm:\n**Theorem 2.** _Let Assumptions 1 and 2 hold and set \u03c3_ 0 = 1 _, then:_\n\n\u02c6 1\n_\u2225g_ LR _[r]_ _[\u2212]_ _[g]_ [True] _[\u2225][F]_ [=] _[ O]_\n\ufffd _r_\n\n_._ (9)\n\ufffd\n\n_r_\n\nThe convergence rate in Eq. (9) is faster than the\ntypical _O_ ( [1] _/_ ~~_[\u221a]_~~ _r_ ) rate dictated by the general parametric central limit theorem. Our analysis shows that\nthis is due to the symmetry in our problem under\nAssumption 1. To obtain our results, we make an\nEdgeworth expansion (Bhattacharya & Ranga Rao,\n1976) of the density _p_ ( _Z_ _[r]_ ), which expands _p_ ( _Z_ _[r]_ )\nas the limiting Gaussian distribution plus a sum of\ndecaying terms that are controlled by the 3rd order\nand higher cumulants of _p_ ( _Z_ _[r]_ ). Each _i_ th order cumulant term is multiplied by a factor that decays at\nrate _O_ _r_ _[\u2212]_ _[i][\u2212]_ 2 [2] . For symmetric zero-mean distribu\ufffd \ufffd\n\n|r=1|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|~~r=2~~<br>r=3<br>r=5<br>r=10||||||\n|r=50<br>r=100<br>r||||||\n|||||||\n|||||||\n|||||||\n\nFigure 3: Plot of Marginal Score Multiplied by Density for\nIncreasing _r_\n\nrate _O_ _r_ _[\u2212]_ _[i][\u2212]_ 2 [2] . For symmetric zero-mean distribu\ufffd \ufffd Figure 3: Plot of Marginal Score Multiplied by Density for\n\ntions, all odd cumulants are zero (for the same reason Increasing _r_\nthat all odd moments of a symmetric distribution are\nzero). Hence, the rate of convergence to the limiting distribution is controlled by the 4th order term, which has\nrate _O_ \ufffd _r_ _[\u2212]_ [1][\ufffd] .\n\nAlthough the full distribution _p_ ( _Z_ _[r]_ ) has no general closed-form solution, the distribution over marginals\n_p_ ( _zi,j_ ) is more amenable to analysis. We derive _p_ ( _zi,j_ ) for generalised Gaussian distributed _ai,j_ and _bi,j_ in\nSection B. To illustrate the fast converge rate, we plot the negative density _\u00d7_ score function _p_ ( _zi,j_ ) _zi,j_ for the\nmarginal distribution _p_ ( _zi,j_ ) in Fig. 3 using Gaussian distributed _ai,j_ and _bi,j_ with _\u03c3_ 0 [2] [= 1][ (see Theorem][ 4][ for]\n\n2\n\na derivation). The figure shows that _p_ ( _zi,j_ ) _zi,j_ quickly converges to the limiting function ~~_\u221a_~~ _[z][i]_ 2 _[,j]_ _\u03c0_ [exp] \ufffd _\u2212_ _[z][i]_ 2 _[,j]_ \ufffd,\n\nrecovering the Gaussian form from the true natural policy gradient update. Even at _r_ = 1, the function is not a\npoor approximation. After _r_ = 10, the function has nearly converged and after _r_ = 50, the function is visually\nindistinguishable from the limit, providing evidence for the hypothesis that the low-rank approximation is\naccurate even for very low rank regimes _r \u226a_ min( _m, n_ ).\n\n2\n\n_[,j]_ _\u2212_ _[z][i][,j]_\n\n2 _\u03c0_ [exp] \ufffd 2\n\na derivation). The figure shows that _p_ ( _zi,j_ ) _zi,j_ quickly converges to the limiting function ~~_\u221a_~~ _[z][i]_ 2 _[,j]_\n\n8"}, {"role": "assistant", "content": "**Analogy:** Imagine you're drawing a picture of a very detailed castle. Usually, if you want it to look good quickly, you need lots of tiny little pen strokes. But for this special castle, because it's perfectly balanced on both sides, you don't need to draw all those tiny strokes. You can use much bigger, simpler strokes, and it still looks like a great castle super fast!\n\n**Explanation:** This paper says they found a super speedy way to make a simplified drawing (a math picture) of something very complicated. They call the speedy way 'low-rank approximation,' and 'r' is like how many big strokes they use. Because their drawing is special and perfectly balanced, they don't need many strokes at all. Even with just a few (small 'r'), the drawing looks almost exactly like the real, complicated one, much faster than other ways."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n**5.1** **Experimental Settings**\n\n**Datasets.** We employ 10 real-world datasets including various type\nof graphs, primarily focusing on collaboration networks, social\nnetworks and road networks. All datasets can be downloaded from\n\n[15, 23, 42]. Since Kemeny constant is defined on connected graphs,\nwe focus on the largest connected component for each graph in\nthis study. The detailed statistics of each dataset are summarized\nin Table 2. Following previous studies [25, 51], we approximate\nKC using ApproxKemeny [52] with _\ud835\udf16_ = 0 _._ 15 as the ground truth\nfor small graphs ( _\ud835\udc5b_ _<_ 10 [6] ). For larger graphs, ApproxKemeny fails\nto provide results within an acceptable time, so we employ our\n\nproposed method TTF with a large sample size _\ud835\udc47_ ( _\ud835\udc47_ = 10 [5] ) to\nobtain a sufficiently accurate result as the ground truth.\n\n**Dynamic Updates.** To simulate dynamic graph updates, we consider both edge insertions and deletions. For the insertion scenario,\nwe first select 90% of the original graph as the base graph and sample 100 edges from the remaining 10% as candidate insertions. For\nthe deletion scenario, we sample 100 edges directly from the original graph as deletions. To better reflect realistic update behavior,\nwe further adopt a power-law update model, where most updates\noccur on a small fraction of edges. Specifically, an edge ( _\ud835\udc62, \ud835\udc63_ ) is\nchosen for insertion or deletion with probability proportional to\nthe product of the degrees of _\ud835\udc62_ and _\ud835\udc63_, ensuring that edges incident\nto high-degree nodes are more likely to be updated.\n\n**Different Algorithms.** For static graphs, we compare our proposed method TTF with several representative approaches, including the LERW-based method ForestMC [51], the UST-based\nmethod SpanTree [29], the truncated random walks-based methods DynamicMC [25] and RefinedMC [51], and the matrix-related\nmethod ApproxKemeny [52]. For dynamic graphs, we select the\nthree most competitive static methods, TTF, ForestMC, and SpanTree,\nand apply them by re-running after each update as baseline methods, comparing with our two proposed sample-maintenance approaches, BSM and ISM.\n\n**Experimental Environment.** All algorithms used in our experiment are implemented in C++ and compiled with g++ 11.2.0 using\nthe -O3 optimization flag, except for ApproxKemeny [52], which\nis implemented in Julia. We conduct all experiments on a Linux\nserver with a 64-core 2.9GHz AMD Threadripper 3990X CPU and\n128GB memory. Each experiment is repeated five times to avoid\naccidental anomalies.\n\n**5.2** **Experiment Results on Static Graphs**\n\n**Exp-I: Error vs. Time on Static Graphs.** In this experiment, we\nevaluate the performance of static algorithms across all datasets\nby comparing running time against relative error. As shown in\nFig. 9, ApproxKemeny can achieve very accurate results, but its\nruntime and memory usage are prohibitively high, making it infeasible for large graphs. Methods based on truncated random walks\nperform reasonably well on some datasets but struggle on road\nnetworks, where even extended runtimes fail to achieve high precision. ForestMC exhibits good accuracy on social networks, but\nits performance degrades on other types of graphs. In contrast,\nTTF consistently achieves lower error in less time across nearly all\ndatasets, demonstrating both high efficiency and accuracy.\n\n**Exp-II: Effect of Path Selection.** In Algorithm 1, we use BFS\nto construct a spanning tree that defines the set of paths used for\npath mapping. To investigate the impact of different path selection\nstrategies, we also experiment with DFS and Wilson\u2019s algorithm.\nThe results are presented in Fig. 10 and Fig. 11. We observe that\nwhile the choice of path selection method has a negligible impact on\nrunning time, it significantly affects the accuracy of the estimation.\nAs discussed in Theorem 3.9, the estimation value is bounded by\nthe maximum length of paths. When the spanning tree is generated\nvia BFS, the maximum path length is bounded by the diameter of\nthe graph, while it can be substantially longer when using DFS\n\nCheng Li, Meihao Liao, Rong-Hua Li, Guoren Wang\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-13-13.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-13-14.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-13-12.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-13-11.png)\n\n|Re<br>101<br>102|lative Error|\n|---|---|\n|101<br>102<br>103<br><br>Running Time(Sec)<br>10<br>4<br>10<br>3<br>(e) Amazon|101<br>102<br>103<br><br>Running Time(Sec)<br>10<br>4<br>10<br>3<br>(e) Amazon|\n\n|10 10 Rel|01 102 103 1 Running Time(Sec) (e) Amazon lative Error|\n|---|---|\n|10<br>4<br>Re|lative Error|\n|10<br>4<br>Re||\n|101<br>102<br>103<br>104<br>Running Time(Sec)<br>(j) Orkut|101<br>102<br>103<br>104<br>Running Time(Sec)<br>(j) Orkut|\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-13-16.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-13-17.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-13-18.png)\n\n**Figure 9: Relative error vs running time for different algorithms**\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-13-15.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-13-19.png)\n\nor randomly sampled trees. Consequently, shorter paths tend to\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-13-20.png)\n\n**different path selection Methods**\n\n**different path selection Methods**\n\ndomly select root nodes with degrees close to _[\ud835\udc51]_ [max] 5 _[,]_ [ 2] _[\ud835\udc51]_ [max] 5 _, . . .,\ud835\udc51_ max,\n\n|Col1|Col2|Col3|Col4|\n|---|---|---|---|\n|||||\n|||||\n|ph<br>Email-enron<br>Amazon<br>DBLP<br>Youtube<br>roadNe<br>** ng time of** TTF** on each datas**<br>** ection Methods**|t-PA<br>roa<br>**  ets**|dNet<br>**  w**|-CA<br>Orkut<br>**  ith three**|\n|Wilson||||\n|||||\n|||||\n|||dNet<br>**  w**<br>  tic<br>  t n<br>    le<br>    fec<br>ax<br>|dNet<br>**  w**<br>  tic<br>  t n<br>    le<br>    fec<br>ax<br>|\n\nand measure the average running time. The results, illustrated in\nFig. 12, show that in most cases choosing the node with the maximum degree yields the fastest performance. Therefore, we adopt\nthe highest-degree node as the default root in our experiments.\n\n**Exp-IV: Scalability on Synthetically Generated Graphs.** In\nthis experiment, we evaluate the scalability of different methods\n\non randomly generated graphs. We employ two standard random\ngraph models: (i) Erd\u0151s\u2013R\u00e9nyi graphs [8], where each possible edge\nis independently included with probability _\ud835\udc5d_ = 5 \u00d7 10 [\u2212][4], and (ii)\nChung-Lu model [1], where the probability of creating an edge\nis proportional to the product of the endpoints\u2019 expected degrees,\nand the degree distribution follows a power law with exponent\n_\ud835\udefe_ = 3 _._ 5. The synthetic graphs are generated using the Networkit\ntoolkit [44], with their number of nodes in the largest connected\ncomponent varying from 10 [3] to 10 [6] . Fig. 13a and Fig. 13b show\nthat TTF and ForestMC scale better, as the running time of other\nmethods increases more rapidly.\n\n**Exp-V: Scalability on Huge Real-World Graphs.** In this experiment, we evaluate the scalability of different methods on two huge\nreal-world datasets: Friendster (65,608,366 nodes and 1,806,067,135\nedges) and Twitter (21,297,772 nodes and 265,025,809 edges). We run\nTTF with sufficient sample size _\ud835\udc47_ = 1000 to approximate the ground\ntruth. For all methods, we set the same and smaller sample size, and\ngradually increase it to compare their relative error and running\ntime, omitting results that exceed 24 hours. Fig. 14 show that TTF\nand ForestMC remain feasible even on such extremely large graphs,\nwhereas other methods either become prohibitively slow or fail to\nproduce accurate results. Overall, TTF achieves the best trade-off\nbetween accuracy and efficiency, demonstrating strong scalability.\n\n**Exp-VI: Parallelization.** We evaluate the parallel performance of\ndifferent algorithms using 64 threads against their single-threaded\nperformance. All methods except ApproxKemeny naturally parallelize across samples. To ensure fairness, we control the number of\nsamples to make the error levels of all algorithms comparable. As\nshown in Fig. 15, all algorithms benefit significantly from parallel\nexecution. Although TTF does not achieve the highest speedup, it\nremains among the most efficient methods.\n\n**5.3** **Experiment Results on Dynamic Graphs**\n\n**Exp-VII: Index Size.** This experiment evaluates the index size of\nboth BSM and ISM, compared to the size of graphs. The experimental results are shown in Fig. 16. We observe that the index sizes\nof both methods grow proportionally with the graph size, and the\n\nScalable and Provable Kemeny Constant Computation on Static and Dynamic Graphs: A 2-Forest Sampling Approach\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-14-4.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-14-5.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-14-6.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-14-7.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-14-8.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-14-14.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-14-15.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-14-16.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-14-17.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-14-18.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-14-28.png)\n\n**Figure 12: Impact of root node with different degrees on the running time of** TTF\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-14-24.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-14-25.png)\n\n**(a) Erd\u0151s\u2013R\u00e9nyi model** **(b) Chung-Lu model**\n\n**Figure 13: Scalability performance of different algorithms**\n**on random generated graphs**\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-14-26.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-14-27.png)\n\n**(a)** Twitter\n\n**(b)** Friendster\n\n**Figure 17: Running time of different algorithms for edges**\n**insertions on dynamic graphs**\n\n**Exp-VIII: Performance for Edge Insertion on Dynamic Graphs.**\nWe evaluate the performance of the basic and improved sample\nmaintenance methods, BSM and ISM, for edge insertions, comparing their accuracy and running time against three static methods\nthat recompute KC after each update. The results are shown in\nFig. 17 and Fig. 18. As expected, both BSM and ISM significantly\noutperform the static methods in terms of time efficiency. On social networks like Orkut, BSM and ISM are an order of magnitude\nfaster than TTF. However, on road networks such as roadNet-PA\nand roadNet-CA, the speed-up of BSM is limited, as the average\neffective resistance of edges is close to one, which means nearly\nall samples are required to be resampled. Additionally, sampling a\nUST that includes a specific edge might be slower regular Wilson\nalgorithm. In contrast, ISM still achieves notable speed-ups even\non road networks, since link-cut operations are significantly faster\nthan tree sampling, though it produces slightly higher error.\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-14-29.png)\n\n**Figure 15: Parallel speedup of different algorithms**\n\ngrowth rate remains stable across all datasets, which empirically\nvalidates the _\ud835\udc42_ ( _\ud835\udc5b_ ) space complexity discussed in Section 4. In all\ntested datasets, the maximum index size is 4.9GB, demonstrating\nthat our proposed dynamic algorithms are highly space-efficient\nwhile supporting real-time sample maintenance.\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-15-0.png)\n\nCheng Li, Meihao Liao, Rong-Hua Li, Guoren Wang\n\n**Figure 21: Normalized KC across real-world graphs**\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-15-1.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-15-6.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-15-7.png)\n\n![](papers/images/Scalable-and-Provable-Kemeny-Constant-Computation-on-Static-and-Dynamic-Graphs-A-2Forest-Sampling-Approach.pdf-15-12.png)\n\n|erG ri<br>P ow<br>re18:<br>sinse<br>LE<br>Sp<br>TT|d H ep - T<br>Acc<br>rtio<br>walk<br>anTree<br>F|h<br>u<br>n|ph<br>A Estr - o m a - il<br>racyperf<br>sondyn<br>BSM(De<br>ISM(Del|Aenron m azon D B LP<br>ormanceofd<br>amicgraphs<br>l)<br>)|Col6|utub roade<br>o Y<br>ifferen|Col8|N e - t<br>ta|roadPA<br>lgo|N e<br>ri|- t<br>t|C A<br>h|O r u k t<br>msfo|Col15|Col16|Col17|Col18|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||||||||\n|PowerGri<br>**re 19:**<br>**ions**|Hep-T<br>** Ru**<br>** on d**|h<br>** n**<br>** y**|Astro-ph<br>Email-<br>** ning tim**<br>** namic g**<br>LEwal<br>~~SpanT~~|enron<br>Amazon<br>D<br>**  e of dife**<br>**  raphs**<br>k<br>~~ee~~<br>BSM(<br>~~ISM(~~|BLP<br><br>**   rent**<br>Del)<br>~~Del)~~|Youtube<br><br>**   alg**|road<br>**   or**|Net-<br>**   ith**|PA<br>road<br>**   m**|Ne<br>**   s f**|t-<br>**    o**|C<br>|**    r**|O<br>**    e**|rku<br>**    d**|t<br>**    ge**||\n||||TTF|||||||||||||||\n|||||||||||||||||||\n\n**Figure 20: Accuracy performance of different algorithms for**\n**edges deletions on dynamic graphs**\n\n**Exp-VIII: Performance for Edge Deletion on Dynamic Graphs.**\nWe also evaluate the performance of BSM and ISM for edge deletions. The results, shown in Fig. 19 and Fig. 20, are generally consistent with those from the insertion scenario. Notably, BSM shows\nbetter performance improvements on road networks during deletions than insertions, since it can employ the standard Wilson algorithm for tree sampling. Moreover, both BSM and ISM are slightly\nfaster for deletions than insertions. This is likely because edge insertions require computing effective resistance, which introduces\nadditional computational overhead, whereas deletions do not.\n\n**5.4** **Case Studies**\n\n**Case Study: Graph Classification.** We conducted a case study to\ninvestigate the relationship between the Kemeny constant and different types of graphs. For each dataset, we normalized the Kemeny\nconstant as _\ud835\udf05_ ( _\ud835\udc3a_ )/ _\ud835\udc5b_ and plotted the results. As shown in Fig .21,\ndifferent graph types exhibit distinct patterns in their normalized\nKemeny constants. Social networks and collaboration networks\ntypically have _\ud835\udf05_ ( _\ud835\udc3a_ )/ _\ud835\udc5b_ \u2248 1, whereas road networks generally show\nlarger values. Intuitively, a smaller Kemeny constant indicates that\nnodes are more tightly connected, which facilitates reaching other\nnodes from any starting point. This demonstrates that the Kemeny\nconstant is informative for understanding graph connectivity and\ncan be leveraged as a feature for graph classification.\n\n**Figure 22: Evolution of Kemeny constant over time**\n\n**Case Study: Kemeny Constant in Dynamic Graphs.** We further investigate the evolution of the Kemeny constant in dynamic\ngraphs using the HighSchool dataset [15], which captures contacts\namong students over a single day. Each student is represented as a\nnode, and an edge is created when two students are in close and\nface-to-face proximity during a 20-second time window. We divided\nthe data into 5-minute windows and computed the Kemeny constant for each window, updating every minute. Figure 22 shows\nthe number of edges and the corresponding Kemeny constant over\ntime. Although the number of edges fluctuates considerably, the\nKemeny constant remains largely stable, with notable changes only\nduring periods such as class breaks or lunch, when the network\nstructure undergoes significant alterations. This demonstrates that\nthe Kemeny constant can effectively capture structural changes in\ndynamic networks."}, {"role": "assistant", "content": "**Analogy:** Imagine our big playground has lots of kids and ropes. The \"Kemeny Constant\" is like how easy it is for any kid to swing from their spot to any other kid's spot on the playground. If it's a small number, everyone can get around super easily.\n\n**Explanation:** The paper found a super-duper fast way (called TTF) to figure out how easy it is for kids to get around on really big playgrounds, much faster than other ways. This way is also very good at getting the right answer. When kids add new ropes or take some down (like making the playground change), they also found clever tricks (called BSM and ISM) to quickly figure out the new \"easy-to-get-around\" number without having to start all over again. They even learned that how easy it is to get around tells us if the playground is like a schoolyard or a big road map, and how it changes when lots of kids go to lunch!"}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n_\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53_ _> \ud835\udc4e\ud835\udc59\ud835\udc5d\u210e\ud835\udc4e_ [ _\ud835\udc5f_ ] **then**\n\n**18** _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ [ _\ud835\udc5f_ ] \u2190 _\ud835\udc50\ud835\udc64\ud835\udc5b\ud835\udc51_ [ _\ud835\udc5f_ ] \u2212 1 ;\n\n/* try to drain link queues if needed */\n\n**19** _\ud835\udc5e_ \u2190 _\ud835\udc5f\ud835\udc61\ud835\udc61_ \u2212 _\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc45\ud835\udc47\ud835\udc47_ [ _\ud835\udc5f_ ] ; // current queuing delay"}, {"role": "assistant", "content": "**Analogy:** Imagine a group of kids trying to walk through a narrow door. If too many kids are bumping into each other at the door (our 'bump-detector' sees too much 'diff'), one kid steps back and waits a bit (we decrease 'cwnd'). We also check how long it takes for a kid to go through the door now compared to when nobody was there (our 'rtt' minus 'baseRTT').\n\n**Explanation:** When too many toys (packets) are trying to go through a small door (network link) at once, they bump into each other and make everyone slow down. If our special 'bump-detector' sees too much bumping, we tell one toy to wait a moment. This helps clear the path so other toys can move faster. We also keep an eye on how long it takes a toy to make a round trip - like going to the kitchen and back - and compare it to how fast it usually goes when there's no one else. This difference tells us how much extra time the toys are spending waiting in line."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\nIn this work, we asked a simple question: _do current spatial supersensing benchmarks and methods in fact require_\n_spatial supersensing?_ For VSI-Super and Cambrian-S, the answer is largely _no_ . We uncover behaviors that\nillustrate benchmark\u2013model co-adaptation. VSI-Super rewards bespoke inference recipes\u2014one for recall and one for\ncounting\u2014that match the dataset\u2019s generative assumptions. A model can score well by encoding rules such as \u201cthere\nare four object insertions\u201d or \u201crooms never repeat\u201d, rather than by learning capabilities that transfer when these\nassumptions change. On the other hand, the success of _NoSense_ shows the strength of contrastive vision\u2013language\nrepresentations as perception components of streaming systems. A simple SigLIP-based baseline can reproduce\nmuch of what current spatial supersensing methods achieve, with little machinery beyond pretrained vision and text\nencoders. Taken together, these findings indicate VSI-Super does not yet operationalize spatial supersensing: trivial\n\n8\n\nadmissible baselines can saturate key tasks without performing any long-horizon spatial integration. A genuine first\nstep toward supersensing requires demonstrating at least one capability that trivial retrieval-based methods cannot\nachieve. Until then, we should view performance of future methods on VSI-Super with caution for claims about\nspatial supersensing or long-horizon world modeling.\n\nGoing forward, we view _world-model evaluation_ as an important research problem, alongside world-modeling itself.\nProgress in spatial supersensing will require benchmarks that are robust to trivial transformations, explicitly require\nrevisits to previous states and long-horizon integration, along with routinely stress-testing with adversarial baselines\nand perturbations. Our analyses move in this direction by exposing the fragility of current evaluations and by\npointing to concrete, more faithful tests of video world models.\n\n**Limitations**\n\nOur analysis has two main limitations. Empirically, we study a single benchmark family (VSI-Super) and one\nprimary method (Cambrian-S). Other datasets or algorithms may behave differently, so our conclusions about\nspatial supersensing may not generalize. Conceptually, we diagnose failure modes and propose design principles, but\nwe do not introduce a validated replacement benchmark. Building such benchmarks remains an open challenge and\nwill require iterative cycles of task design, adversarial probing, and model development.\n\n**Acknowledgments**\n\nThe authors would like to thank (in no particular order): Sebastian Dziadzio, Adhiraj Ghosh, Shashwat Goel,\nMatthias K\u00a8ummerer, Thao Nguyen, and Ronald Skorobogat for helpful feedback and comments on our draft. We\nthank the authors of Cambrian-S for engaging with our work and for their constructive feedback. This work is\nfunded by the Federal Ministry of Education and Research (BMBF), FKZ: 01IS24079A. VU was supported by\na Google PhD Fellowship in Machine Intelligence. VU also thanks the European Laboratory for Learning and\nIntelligent Systems (ELLIS) PhD program for support. AH and VU thank the International Max Planck Research\nSchool for Intelligent Systems (IMPRS-IS) for support. AH acknowledges funding by the Federal Ministry of\nEducation and Research (BMBF), FKZ: 01IS24079A. AP and MB acknowledge acknowledges financial support\nby the Federal Ministry of Research, Technology and Space (BMFTR), FKZ: 16IS24085B and Open Philanthropy\nFoundation funded by the Good Ventures Foundation. This research utilized compute resources at the T\u00a8ubingen\nMachine Learning Cloud, DFG FKZ INST 37/1057-1 FUGG.\n\n**References**\n\nNishant Balepur, Rachel Rudinger, and Jordan Lee Boyd-Graber. Which of these best describes multiple choice evaluation\nwith llms? a) forced b) flawed c) fixable d) all of the above. _arXiv preprint arXiv:2502.14127_, 2025. 8\n\nLucas Beyer, Olivier J H\u00b4enaff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00a8aron van den Oord. Are we done with imagenet?\n_arXiv preprint arXiv:2006.07159_, 2020. 8\n\nEllis Brown, Jihan Yang, Shusheng Yang, Rob Fergus, and Saining Xie. Benchmark designers should\u201d train on the test set\u201d\nto expose exploitable non-visual shortcuts. _arXiv preprint arXiv:2511.04655_, 2025. 2\n\nShyamal Buch, Crist\u00b4obal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, and Juan Carlos Niebles. Revisiting the\u201d video\u201d\nin video-language understanding. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_,\npages 2917\u20132927, 2022. 2\n\nNikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, and Jonas Geiping. Answer matching outperforms multiple\nchoice for language model evaluation. _arXiv preprint arXiv:2507.02856_, 2025. 8\n\nMostafa Dehghani, Yi Tay, Alexey A Gritsenko, Zhe Zhao, Neil Houlsby, Fernando Diaz, Donald Metzler, and Oriol Vinyals.\nThe benchmark lottery. _arXiv preprint arXiv:2107.07002_, 2021. 2\n\nRobert Geirhos, J\u00a8orn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A\nWichmann. Shortcut learning in deep neural networks. _Nature Machine Intelligence_, 2020. 2\n\n9\n\nAryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena,\nXuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, et al. Are we done with mmlu? In _Proceedings of_\n_the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human_\n_Language Technologies (Volume 1: Long Papers)_, pages 5069\u20135096, 2025. 8\n\nAdhiraj Ghosh, Sebastian Dziadzio, Ameya Prabhu, Vishaal Udandarao, Samuel Albanie, and Matthias Bethge. Onebench\nto test them all: Sample-level benchmarking over open-ended capabilities. In _Proceedings of the 63rd Annual Meeting of_\n_the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 32445\u201332481, 2025. 8\n\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the\nrole of image understanding in visual question answering. In _Proceedings of the IEEE conference on computer vision and_\n_pattern recognition_, pages 6904\u20136913, 2017. 2\n\nKristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh,\nVijay Baiyya, Siddhant Bansal, Bikram Boote, Eugene Byrne, Zach Chavis, Joya Chen, Feng Cheng, Fu-Jen Chu, Sean\nCrane, Avijit Dasgupta, Jing Dong, Maria Escobar, Cristhian Forigua, Abrham Gebreselasie, Sanjay Haresh, Jing Huang,\nMd Mohaiminul Islam, Suyog Jain, Rawal Khirodkar, Devansh Kukreja, Kevin J Liang, Jia-Wei Liu, Sagnik Majumder,\nYongsen Mao, Miguel Martin, Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ragusa, Santhosh Kumar Ramakrishnan,\nLuigi Seminara, Arjun Somayazulu, Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu Zhang, Angela Castillo, Changan\nChen, Xinzhu Fu, Ryosuke Furuta, Cristina Gonzalez, Prince Gupta, Jiabo Hu, Yifei Huang, Yiming Huang, Weslie\nKhoo, Anush Kumar, Robert Kuo, Sach Lakhavani, Miao Liu, Mi Luo, Zhengyi Luo, Brighid Meredith, Austin Miller,\nOluwatumininu Oguntola, Xiaqing Pan, Penny Peng, Shraman Pramanick, Merey Ramazanova, Fiona Ryan, Wei Shan,\nKiran Somasundaram, Chenan Song, Audrey Southerland, Masatoshi Tateno, Huiyu Wang, Yuchen Wang, Takuma Yagi,\nMingfei Yan, Xitong Yang, Zecheng Yu, Shengxin Cindy Zha, Chen Zhao, Ziwei Zhao, Zhifan Zhu, Jeff Zhuo, Pablo\nArbelaez, Gedas Bertasius, Dima Damen, Jakob Engel, Giovanni Maria Farinella, Antonino Furnari, Bernard Ghanem,\nJudy Hoffman, C.V. Jawahar, Richard Newcombe, Hyun Soo Park, James M. Rehg, Yoichi Sato, Manolis Savva, Jianbo\nShi, Mike Zheng Shou, and Michael Wray. Ego-exo4d: Understanding skilled human activity from first- and third-person\nperspectives. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages\n19383\u201319400, June 2024. 8\n\nHasan Abed Al Kader Hammoud, Ameya Prabhu, Ser-Nam Lim, Philip HS Torr, Adel Bibi, and Bernard Ghanem. Rapid\nadaptation in online continual learning: Are we evaluating it right? In _2023 IEEE/CVF International Conference on_\n_Computer Vision (ICCV)_, pages 18806\u201318815. IEEE, 2023. 2\n\nAndreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, and Matthias Bethge. A\nsober look at progress in language model reasoning: Pitfalls and paths to reproducibility. _arXiv preprint arXiv:2504.07086_,\n2025. 8\n\nCheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable\nbenchmarks for vision-language compositionality. _arXiv preprint arXiv:2306.14610_, 2023. 8\n\nJie Lei, Tamara Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning. In _Proceedings of the_\n_61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 487\u2013507, 2023. 2\n\nBo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei\nLiu, et al. Llava-onevision: Easy visual task transfer. _arXiv preprint arXiv:2408.03326_, 2024. 5\n\nYingwei Li, Yi Li, and Nuno Vasconcelos. Resound: Towards action recognition without representation bias. In _ECCV_, 2018.\n\n2\n\nDivyam Madaan, Taro Makino, Sumit Chopra, and Kyunghyun Cho. Jointly modeling inter-& intra-modality dependencies\nfor multi-modal learning. _Advances in Neural Information Processing Systems_, 37:116084\u2013116105, 2024. 8\n\nDivyam Madaan, Varshan Muhunthan, Kyunghyun Cho, and Sumit Chopra. Multi-modal data spectrum: Multi-modal\ndatasets are multi-dimensional. _arXiv preprint arXiv:2509.23499_, 2025. 8\n\nMarianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, and Jenia Jitsev. Alice in wonderland: Simple tasks showing\ncomplete reasoning breakdown in state-of-the-art large language models. _arXiv preprint arXiv:2406.02061_, 2024. 8\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\nAskell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In\n_International Conference on Machine Learning (ICML)_, 2021. 3, 5\n\n10\n\nKrishna Kumar Singh, Kayvon Fatahalian, and Alexei A Efros. Krishnacam: Using a longitudinal, single-person, egocentric\ndataset for scene understanding tasks. In _2016 IEEE Winter Conference on Applications of Computer Vision (WACV)_,\npages 1\u20139. IEEE, 2016. 8\n\nEnxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye,\nYanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In _CVPR_, 2024. 5\n\nMatthias Tangemann, Matthias K\u00a8ummerer, Thomas SA Wallis, and Matthias Bethge. Measuring the importance of temporal\nfeatures in video saliency. In _European Conference on Computer Vision_, pages 667\u2013684. Springer, 2020. 2\n\nVishaal Udandarao, Mehdi Cherti, Shyamgopal Karthik, Jenia Jitsev, Samuel Albanie, and Matthias Bethge. A good\ncrepe needs more than just sugar: Investigating biases in compositional vision-language benchmarks. _arXiv preprint_\n_arXiv:2506.08227_, 2025. 8\n\nJihan Yang, Shusheng Yang, Anjali W Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal\nlarge language models see, remember, and recall spaces. In _Proceedings of the Computer Vision and Pattern Recognition_\n_Conference_, pages 10632\u201310643, 2025a. 6\n\nShusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu,\nMuhan Wang, et al. Cambrian-s: Towards spatial supersensing in video. _arXiv preprint arXiv:2511.04670_, 2025b. 2, 6, 8\n\nJian Yao, Ran Cheng, and Kay Chen Tan. Var-math: Probing true mathematical reasoning in llms via symbolic multi-instance\nbenchmarks. _arXiv preprint arXiv:2507.12885_, 2025. 8\n\nMert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language\nmodels behave like bag-of-words models, and what to do about it? _arXiv preprint arXiv:2210.01936_, 2022. 5\n\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In\n_Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 11975\u201311986, 2023. 3\n\nHaoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin. Flash-vstream: Memory-based\nreal-time understanding for long video streams. _arXiv preprint arXiv:2406.08085_, 2024. 5\n\nDaniil Zverev, Thadd\u00a8aus Wiedemer, Ameya Prabhu, Matthias Bethge, Wieland Brendel, and A Koepke. Vggsounder:\nAudio-visual evaluations for foundation models. In _Proceedings of the IEEE/CVF International Conference on Computer_\n_Vision_, pages 1027\u20131037, 2025. 8\n\n11\n\n**A** **Feedback from Cambrian-S Authors**\n\nWe aim to foster open discussion in the spirit of journals such as _Behavioral and Brain Sciences_ (BBS), which use\nan open peer commentary format for controversial work. Before releasing this work, we emailed the Cambrian-S\nauthors to share our findings. We present their response to give readers a more balanced perspective on this work:\n\n![](papers/images/Solving-Spatial-Supersensing-Without-Spatial-Supersensing.pdf-11-0.png)\n\n12"}, {"role": "assistant", "content": "**Analogy:** Imagine we're playing a game where you have to find all the red cars in a big parking lot. But instead of really looking for all the red cars, you just know that every time we play, there are always exactly four red cars, or that the first parking spot always has a red car. So you just say 'four red cars' or 'the first spot' without actually looking. That's like the trick the computers are doing.\n\n**Explanation:** Hey kids! We have these special 'super-seeing' games for computers, like a treasure hunt where they need to remember where things are in a big house. But guess what? Some of these games are a bit tricky. It turns out the computers are not really 'super-seeing' and remembering everything like a map in their head. Instead, they're playing a trick! It's like if I always hide four cookies, and you just know to say 'four cookies' every time without actually counting them. Or if I always hide the toy in the blue box, and you just go straight to the blue box without looking for other clues. The computers are just learning these simple 'rules' about where things usually are in *this specific game*, not truly understanding the whole house. So, they look super smart, but they're just using a simple cheat sheet! We need to make new, harder games where they really have to remember where everything is, even if we change the rules a little bit, so they can't use their cheat sheets anymore."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\nThis section is dedicated to the overall evaluation of the packet scheduling and congestion control algorithms. We\n\nfirst present a subset of figures, illustrating how the MPTCP packet scheduling algorithms performed across different\n\ncongestion control algorithm (CCA) combinations, under some representative scenarios which resemble real-world\n\nnetwork conditions. Then, we define two new metrics to assess the performance of the packet scheduling and the\n\ncongestion control algorithms, that is, the _PS_score_ and the _CCA_score_, respectively. The _PS_score_ metric is based on\n\nthe normalized goodput in order to assess the performance level of each packet scheduling algorithm within each\n\nscenario family. The _CCA_score_ metric, which is based on the normalized _PS_score_, is used to assess the performance\n\nof the congestion control algorithms across all sccenario families. This metric provides the means to quantify the\n\nEvaluating the Impact of Packet Scheduling and Congestion Control Algorithms on MPTCP Performance over\nHeterogeneous Networks 27\n\nperformance of each CCA through the normalized score that the underlying packet scheduling algorithms achieved\n\nusing the respective CCA. The _CCA_score_ indicates, in essence, how much \"space\" each CCA provides to the underlying\n\npacket scheduling algorithms to unveil their sophisticated scheduling logic. If a congestion control algorithm suppresses\n\nthe congestion window of a subflow, then the packet scheduling algorithm cannot perform any better than applying\n\nits logic on the resources that the CCA makes available to the scheduler; this means that a bad congestion control\n\nalgorithmic logic is anticipated to incur a poor performance outcome, regardless of the packet scheduling algorithm\u2019s\n\nintrinsic properties.\n\nOnce we identify the \"best\" three congestion control algorithms, we proceed with a fine-grained assessment of the\n\nvarious packet scheduling algorithms when combined with each of the highest-score CCAs. The _normalized goodput_ and\n\nthe _per-packet delay_ are the main metrics against which all packet scheduling algorithms are evaluated. The Matplotlib\n\npython library has been used for the visualization of the results [45].\n\nNoteworthy here is that we have deliberately decided to maintain different axes\u2019 scale when plotting the values\n\nof some parameters across the various CCAs, such as the SRTT, the MPTCP OFO queue size, and the number of\n\nretransmissions. This allows to increase the granularity and, consequently, the visibility of the results, preventing subtle\n\nvariations from being shadowed by any neighboring CCA\u2019s more coarse results within the same figure.\n\n**4.1** **Cross-CCA packet scheduling overview**\n\nFrom the 1,015 unique tests executed (5,075 tests / 5 iterations per test), we have selected to present in this work a\n\nsubset of the most representative scenarios. To this end, we hereby present:\n\ni. a scenario involving homogeneous paths,\n\nii. a scenario involving paths experiencing intense heterogeneity, and\n\niii. a scenario involving path conditions of mixed heterogeneity.\n\n_Homogeneous scenario._ Figure 5 presents the results of a scenario derived from the _homogeneous_ scenario family,\n\nwhere both subflows traverse paths which experience common networking characteristics. When an equal delay of\n\n5ms is present on both paths, minRTT, ECF, and RR outperform LLHD and BLEST under LIA CC, and its variations\n\nOLIA and BALIA. All schedulers follow the same performance pattern across these three CCs. It is evident that even\n\nwhen the CCA (i.e., LIA/OLIA/BALIA) leaves the congestion window of both subflows open, BLEST, which bases its\n\ndecisions on send window blocking estimations, selects to equally limit the use of both subflows, underutilizing the\n\navailable bandwidth of both paths, and eventually underperforming. wVegas, which is a delay-based CC, is highly\n\nsensitive to delay, interpreting it to congestion and capping aggressively the congestion window of both subflows; the\n\naggregate goodput achieved by all schedulers is limited compared to the one achieved under the use of other CCAs. On\n\nthe contrary, the use of Cubic, which is a loss-based uncoupled CCA, provides a clear benefit to all schedulers. BLEST\u2019s\n\nperformance improvements can be credited to the uncoupled nature of Cubic, that is, there is no upper bound for the\n\naggregate congestion window of the two subflows, as is the case for coupled CCAs. Thus, each of the subflows maintains\n\nits individual congestion window which is treated by the employed CCA as if it were for a singe path flow belonging to\n\na normal TCP connection. This makes MPTCP unfair to single path TCP flows, since each MPTCP subflow can allocate\n\na portion of BW equal to that of a single path TCP flow, ultimately leading single path TCP flows to starvation when\n\nsingle path TCP and MPTCP subflows coexist in the same bottleneck link. In this sub-scenario, BBR allows all schedulers\n\nto reach high and stable performance; it is also noticeable that BBR\u2019s accurate estimations of the bottleneck bandwidth\n\nand RTT lead to scheduling decisions which incur the lowest SRTT and very low OFO queue occupancy across all the\n\n28 D. Dimopoulos, et al.\n\nemployed CCAs. C-MPBBR, which is a coupled variant of BBR for MPTCP (ensuring that MPTCP has a clear benefit over\n\nand is fair to SP-TCP), incurs a lower goodput aggregation to all schedulers\u2019 performance, when compared to uncoupled\n\nBBR. The fairness C-MPBBR achieves across the two MPTCP subflows traversing the same bottleneck link, can be\n\nclearly seen in the measured SRTT of the subflows for each scheduler; however, its coupled nature and fairness benefits\n\ncome at the cost of a more restricted aggregate congestion window which limits the performance of all schedulers over\n\nthe two subflows. The overall _\ud835\udc52_ 1_ _\u210e\ud835\udc5c\ud835\udc5a_ _ _\ud835\udc4f\ud835\udc64_ _ _\ud835\udc51\ud835\udc52\ud835\udc59\ud835\udc4e\ud835\udc66_ scenario\u2019s verdict can be summarized as follows:\n\n   - minRTT, ECF, and RR exhibit high performance under LIA/OLIA/BALIA CCAs\n\n   - BLEST is sensitive to delay and underperforms the rest when any one of the LIA/OLIA/BALIA is the employed\n\nCCA\n\n   - wVegas is a delay-based CCA and reacts more aggressively to delay, restricting the available congestion window,\n\nand leading all schedulers to low goodput aggregation over the two paths\n\n   - all schedulers perform comparably well under Cubic\n\n   - all schedulers perform comparably well and stably under BBR\n\n   - C-MPBBR bounds the coupled congestion window in favor of fairness and MPTCP\u2019s benefit over SPTCP; this has\n\na considerable impact on the performance of all schedulers when compared to that achieved with C-MPBBR\u2019s\n\nrivals (i.e., uncoupled BBR and Cubic)\n\nFig. 5. Homogeneous scenario - BW/RTT/PLR - _\ud835\udc46\ud835\udc39_ 1 : 100 _\ud835\udc40\ud835\udc4f\ud835\udc5d\ud835\udc60_ /5 _\ud835\udc5a\ud835\udc60_ /0% _, \ud835\udc46\ud835\udc39_ 2 : 100 _\ud835\udc40\ud835\udc4f\ud835\udc5d\ud835\udc60_ /5 _\ud835\udc5a\ud835\udc60_ /0%\n\n_Intense_heterogeneity scenario._ Figure 6 presents the results of a scenario derived from the _intense_heterogeneity_\n\nscenario family, where the two subflows traverse paths with intense-heterogeneity networking characteristics. The\n\nadditional presence of delay on a path (i.e., SF2) on top of intense bandwidth restrictions and packet loss, has a\n\ndetrimental effect on the goodput of all schedulers, when combined with any one of the traditional loss- and delay-based\n\nCCAs. All schedulers fail to efficiently utilize the available bandwidth of the affected path (SF2) when combined with\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-27-0.png)\nEvaluating the Impact of Packet Scheduling and Congestion Control Algorithms on MPTCP Performance over\nHeterogeneous Networks 29\n\nany one of the LIA/OLIA/BALIA/wVegas/Cubic; this result is mainly credited to the effects of the congestion control\n\napproaches that the aforementioned CCAs follow; that is, increasing the congestion window during the CA (congestion\n\navoidance) phase, which in turn allows intermediate buffers to be filled up, and then trying to tackle the congestion\n\nthey created by drastically decreasing the congestion window, which deteriorates the already adverse conditions even\n\nfurther. Under such circumstances, schedulers cannot perform any better than trying to efficiently utilize the resources\n\nthat the CCA makes available; CCA has a great impact (either positive or negative) on the final performance. The\n\ncraftsmanship in the design of BBR and C-MPBBR, lies in the way they estimate the bottleneck bandwidth and the\n\nround-trip propagation time, thus pacing traffic (via the cwnd) at the highest possible rate which, however, does not lead\n\nto buffer formation; when buffers start being occupied, BBR can detect the RTT increase and adjust the rate efficiently\n\nuntil the queue is drained and a lower RTT is measured. This different logic allows BBR to sustain traffic rate close\n\nto Kleinrock\u2019s optimal operating point, avoiding the post-congestion control operation of traditional CCAs (AIMD).\n\nBBR and C-MPBBR are rather adjusting traffic smoothly by avoiding high queue occupancies and the subsequently\n\nincreased congestion; this allows all schedulers to unleash their full potential and apply their scheduling logic towards\n\nhigh goodput rates. In this sub-scenario, minRTT, ECF, RR, and LLHD perform comparably well, achieving close to\n\noptimal goodput; BLEST is not limited by the CCA decisions, but rather applies its algorithmic logic, which is based on\n\nlimiting or eliminating the use of a slow path if this is anticipated to cause HoL and, consequently, MPTCP send window\n\nblocking (due to inflight data carried over the slower path). According to this logic, BLEST selects not to use the affected\n\npath (SF2), underutilizing its available bandwidth and thus exhibiting the lowest performance among schedulers. The\n\noverall _\ud835\udc52_ 7_ _\ud835\udc56\ud835\udc5b\ud835\udc61_ _ _\u210e\ud835\udc52\ud835\udc61_ _ _\ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60_ _ _\ud835\udc4f\ud835\udc64_ _ _\ud835\udc51\ud835\udc52\ud835\udc59\ud835\udc4e\ud835\udc66_ scenario\u2019s verdict can be summarized as follows:\n\n   - the decisions of traditional delay- and loss-based CCAs (i.e., LIA/OLIA/BALIA/wVegas/Cubic) have a severe\n\nimpact on any scheduler\u2019s performance; when these CCAs have to deal with intense delay in conjunction with\n\ncapped bandwidth and packet loss, their decisions have a disastrous effect on the final goodput, irrespective of\n\nthe employed scheduler\n\n   - BBR (and its MPTCP coupled variant C-MPBBR) are designed in a way that does not aggressively suppress the\n\ncongestion window of an already adversely impacted flow, thus allowing schedulers to achieve and sustain\n\nhigh goodput\n\n   - BLEST avoids the use of the slow path, either because the congestion window of the respective subflow is\n\nsuppressed by a traditional CCA (i.e., LIA/OLIA/BALIA/wVegas/Cubic), or because of its own algorithmic logic\n\nto avoid slow paths in case this is expected to cause send window blocking\n\n   - RR experiences the highest number of retransmissions among schedulers\n\n_Mixed_heterogeneity scenario._ Figure 7 presents the results of a scenario derived from the _mixed_heterogeneity_\n\nscenario family, where the two subflows traverse paths experiencing mixed-heterogeneity networking conditions [13] . In\n\nthis particular mixed delay- and loss-induced heterogeneity sub-scenario, there is a clear distinction between schedulers\u2019\n\nachieved goodput across CCAs, illustrating the effect that the various CCAs may have in scheduling. The highest\n\nscheduling performance is achieved when schedulers are combined with BBR; in that case, all schedulers (except for\n\nRR) achieve comparably high goodput over both subflows. Equally high goodput rates (to those observed with BBR) are\n\nreached by schedulers in case of uncoupled loss-based CCA Cubic; RR however is severely degraded. The next higher\n\n13Mixed heterogeneity scenarios emulate complex networking conditions where each of the paths is superior to the other when considering only a single\nfactor; be it the allocated bandwidth, or the experienced RTT, or the packet loss rate. In such cases, however, it is sometimes not evident beforehand\nwhich path is eventually going to render an overall better performance.\n\n30 D. Dimopoulos, et al.\n\nFig. 6. Intense heterogeneity scenario - BW/RTT/PLR - _\ud835\udc46\ud835\udc39_ 1 : 100 _\ud835\udc40\ud835\udc4f\ud835\udc5d\ud835\udc60_ /0 _\ud835\udc5a\ud835\udc60_ /0% _, \ud835\udc46\ud835\udc39_ 2 : 50 _\ud835\udc40\ud835\udc4f\ud835\udc5d\ud835\udc60_ /20 _\ud835\udc5a\ud835\udc60_ /0 _._ 5%\n\ngoodput rates are achieved when schedulers are combined with C-MPBBR; all (except RR) are performing comparably\n\nhigh, but they experience some goodput restriction over the higher-delay path (SF1), as compared to that achieved\n\nwith BBR. This can be credited to the restrictions that C-MPBBR\u2019s coupled nature imposes on the subflows that are\n\nconsidered to be traversing the same bottleneck path. With the couped loss-based OLIA, the goodput achieved over SF2\n\nis slightly restricted (compared to the one achieved with BBR / C-MPBBR / Cubic), since the coupled nature of OLIA in\n\nconjunction with its more aggressive reaction to the packet loss detected over SF2, lead to further cwnd restrictions. The\n\nsame goodput pattern among schedulers, with a higher goodput restriction over SF2, is also visible with BALIA, while\n\nwith LIA, SF2\u2019s goodput is limited even further. Finally, wVegas has a detrimental effect on schedulers\u2019 performance; all\n\nschedulers underperform over both subflows, being able to eventually reach an aggregate goodput which is less than a\n\nhalf of the one achieved with BBR. The overall _\ud835\udc52_ 5_ _\ud835\udc5a\ud835\udc56\ud835\udc65_ _ _\u210e\ud835\udc52\ud835\udc61_ _ _\ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60_ _ _\ud835\udc51\ud835\udc52\ud835\udc59\ud835\udc4e\ud835\udc66_ scenario\u2019s verdict can be summarized as follows:\n\n   - minRTT, BLEST, ECF, and LLHD perform comparably to one another, within each CCA combination\n\n   - BBR, C-MPBBR, and Cubic provide ideal conditions to the underlying schedulers, allowing them to achieve\n\nvery high goodput rates\n\n   - OLIA is the third best (in terms of schedulers\u2019 performance) CCA, allowing all schedulers to reach high goodput\n\nrates; goodput over the lossy path (SF2) is restricted more due to the loss-based nature of OLIA which caps the\n\ncwnd to tackle congestion, not being able to distinguish random loss from actual link congestion\n\n   - BALIA and LIA limit the SF2 cwnd further, leading all schedulers to moderate performance\n\n   - wVegas has a detrimental effect upon all schedulers\u2019 performance, due to its delay-based congestion inference\n\n   - RR experiences the highest number of retransmissions and underperforms, regardless of CCA selection\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-29-0.png)\nEvaluating the Impact of Packet Scheduling and Congestion Control Algorithms on MPTCP Performance over\nHeterogeneous Networks 31\n\nFig. 7. Mixed heterogeneity scenario - BW/RTT/PLR - _\ud835\udc46\ud835\udc39_ 1 : 100 _\ud835\udc40\ud835\udc4f\ud835\udc5d\ud835\udc60_ /10 _\ud835\udc5a\ud835\udc60_ /0% _, \ud835\udc46\ud835\udc39_ 2 : 100 _\ud835\udc40\ud835\udc4f\ud835\udc5d\ud835\udc60_ /2 _\ud835\udc5a\ud835\udc60_ /0 _._ 5%\n\n**4.2** **PS and CCA score metrics**\n\nAs mentioned earlier, to evaluate the performance of the various packet scheduling and congestion control algorithms,\n\nwe have introduced two new metrics; the _PS_score_ and the _CCA_score_, respectively.\n\n_PS_score._ For each scheduler, we first calculate the _average goodput_ it achieved in each sub-scenario of each broader\n\nscenario family, over the five identical execution runs. Then, we calculate a _PS_score_ for each packet scheduler per\n\nscenario family; the _PS_score_ is actually the _normalized aggregate goodput_ for each scheduler in each scenario family,\n\nand is given by the formula:\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-30-0.png)\n\n(20)\n\ufffd\n\n_\ud835\udc43\ud835\udc46_ _ _\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52_ =\n\u2211\ufe01\n\n# _\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc60\ud835\udc50\ud835\udc52\ud835\udc5b_\n\n_\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc60\ud835\udc50\ud835\udc52\ud835\udc5b_ _ _\ud835\udc4e\ud835\udc63\ud835\udc54_ _ _\ud835\udc3a\ud835\udc43_\n\ufffd # _\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc60\ud835\udc50\ud835\udc52\ud835\udc5b_ _ _\ud835\udc56\ud835\udc5b_ _ _\ud835\udc46\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc39\ud835\udc4e\ud835\udc5a_ \u2217 _\ud835\udc61\u210e\ud835\udc52\ud835\udc5c\ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc59_ _ _\ud835\udc5a\ud835\udc4e\ud835\udc65_ _ _\ud835\udc54\ud835\udc5d_\n\nwhere _#subscen_ denotes the number of sub-scenarios included in each respective scenario family, _subscen_avg_GP_ is\n\nthe average goodput achieved over the five identical iterations of each respective sub-scenario, _#subscen_in_ScenFam_\n\ndenotes the number of sub-scenarios included within a broader scenario family, and the _theoretical_max_gp_ denotes the\n\ntheoretical maximum aggregate goodput over the two subflows that is, 200 _\ud835\udc40\ud835\udc4f\ud835\udc5d\ud835\udc60_ . This way, _\ud835\udc43\ud835\udc46_ _ _\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52_ \u2208[0 _,_ 1) .\n\n_CCA_score._ A similar approach has been followed in order to define a metric which would allow us to assess the\n\nperformance of the congestion control algorithms. We first present how each congestion control algorithm performed\n\nwithin each broader scenario family. This kind of assessment is essentially showcasing how the packet scheduling\n\nalgorithms performed under each respective congestion control algorithm within each broader scenario family. Thus,\n\neach CCA\u2019s _CCA_score_per_ScenFam_ is computed based on the average _PS_score_ of the packet scheduling algorithms\n\n32 D. Dimopoulos, et al.\n\nwhen combined with the respective CCA. The _CCA_score_per_ScenFam_ is provided by the following formula:\n\n(21)\n\ufffd\n\n_\ud835\udc36\ud835\udc36\ud835\udc34_ _ _\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52_ _ _\ud835\udc5d\ud835\udc52\ud835\udc5f_ _ _\ud835\udc46\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc39\ud835\udc4e\ud835\udc5a_ = \u2211\ufe01\n\n# _\ud835\udc60\ud835\udc50\u210e\ud835\udc52\ud835\udc51\ud835\udc62\ud835\udc59\ud835\udc52\ud835\udc5f\ud835\udc60_\n\n_\ud835\udc43\ud835\udc46_ _ _\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52_\n\ufffd # _\ud835\udc60\ud835\udc50\u210e\ud835\udc52\ud835\udc51\ud835\udc62\ud835\udc59\ud835\udc52\ud835\udc5f\ud835\udc60_\n\nwhere _#schedulers_ denotes the number of all packet scheduling algorithms, and _PS_score_ is the respective score each\n\npacket scheduling algorithm received for this scenario family. For this metric, it also applies that _\ud835\udc36\ud835\udc36\ud835\udc34_ _ _\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52_ _ _\ud835\udc5d\ud835\udc52\ud835\udc5f_ _ _\ud835\udc46\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc39\ud835\udc4e\ud835\udc5a_ \u2208\n\n[0 _,_ 1).\n\n**4.3** **Congestion control assessment**\n\nThe _\ud835\udc36\ud835\udc36\ud835\udc34_ _ _\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52_ _ _\ud835\udc5d\ud835\udc52\ud835\udc5f_ _ _\ud835\udc46\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc39\ud835\udc4e\ud835\udc5a_ reached by each congestion control algorithm in each scenario family, is depicted in\n\nFigure 8.\n\nFig. 8. CCA score per scenario family\n\nOnce an overview had been obtained on how each congestion control algorithm performed within each scenario\n\nfamily, it was interesting to identify the three best-performing CCAs across all scenario families. To this end, we defined\n\nthe _CCA_score_ which is essentially an average over the scores each congestion control algorithm has been assigned for\n\nits performance within each scenario family. _CCA_score_ is consequently a _cross-scenario-family_ score indication, given\n\nby the formula:\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-31-0.png)\n\n(22)\n\ufffd\n\n_\ud835\udc36\ud835\udc36\ud835\udc34_ _ _\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52_ =\n\u2211\ufe01\n\n# _\ud835\udc46\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc39\ud835\udc4e\ud835\udc5a_\n\n_\ud835\udc36\ud835\udc36\ud835\udc34_ _ _\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52_ _ _\ud835\udc5d\ud835\udc52\ud835\udc5f_ _ _\ud835\udc46\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc39\ud835\udc4e\ud835\udc5a_\n\ufffd # _\ud835\udc46\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc39\ud835\udc4e\ud835\udc5a_\n\nwhere _#ScenFam_ is the number of the different scenario families.\n\nThe drawback, however, of the above-mentioned metric is the fact that it does not consider how poorly each\n\ncongestion control algorithm performed in its worst-case scenario family. Thus, to allow also for performance stability\n\nacross the various scenario families, we combined the _CCA_score_ with its respective _coefficient of variation (CV)_, which\n\nprovides insight on the extent of variability in relation to the mean. Figure 9 illustrates the coefficient of variation for\n\neach individual _CCA_score_ .\n\nEvaluating the Impact of Packet Scheduling and Congestion Control Algorithms on MPTCP Performance over\nHeterogeneous Networks 33\n\nThe lower the coefficient of variation, the higher the performance stability of each CCA across the various sub\n\nscenarios. Dividing each CCA\u2019s _CCA_score_ by its respective coefficient-of-variation value, we derive an _overall score_ for\n\neach congestion control algorithm, which allowed us to identify the best three in terms of goodput and performance\n\nstability across scenario families. The _CCA_overall_score_ is then given by the below formula:\n\n_\ud835\udc36\ud835\udc36\ud835\udc34_ _ _\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc59\ud835\udc59_ _ _\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52_ = _[\ud835\udc36\ud835\udc36\ud835\udc34]_ [_] _[\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52]_ (23)\n\n_\ud835\udc36\ud835\udc36\ud835\udc34_ _ _\ud835\udc36\ud835\udc49_\n\nFigure 10 illustrates each congestion control algorithm\u2019s _CCA_overall_score_ . This way, we have been then able to\n\nidentify the three best-performing congestion control algorithms: **C-MPBBR**, **BBR**, and **OLIA** .\n\nFig. 9. CCA_score CV Fig. 10. CCA_overall_score\n\n**4.4** **Packet scheduling assessment - Goodput**\n\nHaving identified the three best-performing CCAs, we have then been able to delve into the packet scheduling algorithms\u2019\n\nperformance evaluation. The _PS_score_ metric is indicative on how each scheduler performed within each scenario family,\n\nunder each of the best three congestion control algorithms. Figure 11 illustrates each packet scheduling algorithm\u2019s\n\nperformance score per CCA, and per scenario family.\n\nA more granular _PS_score_ analysis per scenario family is provided for each one of the BBR, C-MPBBR, and OLIA\n\nCCAs, within Figures 12, 13, and 14, respectively.\n\nAn alternative view of each packet scheduling algorithm\u2019s score per scenario family for each of the top three CCAs,\n\nis provided via the _PS_score_ heatmaps showcased within Figures 15, 16, and 17, respectively.\n\nOne of the main purposes of this work, has been to provide a clear view on how each packet scheduler performs across\n\nall scenario families for each of the best-performing CCA. To this end, Figure 18 illustrates the _Empirical Complementary_\n\n_Cumulative Distribution Function (ECCDF)_, which depicts each scheduler\u2019s probability to perform above a specific level\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-32-0.png)\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-32-1.png)\n34 D. Dimopoulos, et al.\n\nFig. 11. PS_score\n\nFig. 12. PS_score per scenario family for BBR\n\nof goodput, for each of the top-three CCAs (i.e., BBR, C-MPBBR, and OLIA); ECCDF considers goodput probability\n\nacross all scenario families and sub-scenarios.\n\n**4.5** **Packet scheduling assessment - Per-Packet Delay**\n\nBesides the evaluation in terms of goodput, we found it also useful to provide insight on how each packet scheduling\n\nalgorithm would perform in case of interactive applications. A key metric for this type of applications is the experienced\n\n_per-packet delay (PPD)_ . For this purpose, we provide a high-level view of each packet scheduling algorithm\u2019s [14] perfor\n\nmance in terms of per-packet delay in Figure 19; a more fine-grained numerical evaluation of each packet scheduling\n\nalgorithms per-packet delay for each of the best-three congestion control algorithms (i.e., BBR, C-MPBBR, and OLIA)\n\ncan be found in Figures 20, 21, and 22, respectively.\n\n14\"Packet Scheduling Algorithm\" is abbreviated as _PSA_ within figure captions.\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-33-0.png)\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-33-1.png)\nEvaluating the Impact of Packet Scheduling and Congestion Control Algorithms on MPTCP Performance over\nHeterogeneous Networks 35\n\nFig. 13. PS_score per scenario family for C-MPBBR\n\nFig. 14. PS_score per scenario family for OLIA\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-34-0.png)\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-34-1.png)\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-34-2.png)\n\nFig. 15. PS_score heatmap per scenario\nfamily for BBR\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-34-3.png)\n\nFig. 16. PS_score heatmap per scenario\nfamily for C-MPBBR\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-34-4.png)\n\nFig. 17. PS_score heatmap per scenario\nfamily for OLIA\n\nFinally, Figure 23 depicts the _Empirical Cumulative Distribution function (ECDF)_, which provides a metric of scheduler\u2019s\n\nprobability to achieve below a specific level of average per-packet delay, for each one of the top-three CCAs (i.e., BBR,\n\nC-MPBBR, and OLIA); ECDF considers the _average per-packet delay_ across all scenario families and sub-scenarios.\n\n36 D. Dimopoulos, et al.\n\nFig. 18. ECCDF depicting packet scheduler goodput probability across scenario families, for BBR, C-MPBBR, and OLIA\n\nFig. 19. Qualitative view of each PSA\u2019s PPD per scenario family for BBR, C-MPBBR, and OLIA\n\nFig. 20. PPD of each PSA per scenario family for BBR\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-35-0.png)\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-35-1.png)\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-35-2.png)\nEvaluating the Impact of Packet Scheduling and Congestion Control Algorithms on MPTCP Performance over\nHeterogeneous Networks 37\n\nFig. 21. PPD of each PSA per scenario family for C-MPBBR\n\nFig. 22. PPD of each PSA per scenario family for OLIA\n\nFig. 23. ECDF depicting packet schedulers\u2019 PPD probability across scenario families, for BBR, C-MPBBR, and OLIA.\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-36-0.png)\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-36-1.png)\n\n![](papers/images/Evaluating-the-Impact-of-Packet-Scheduling-and-Congestion-Control-Algorithms-on-MPTCP-Performance-over-Heterogeneous-Networks.pdf-36-2.png)\n38 D. Dimopoulos, et al."}, {"role": "assistant", "content": "**Analogy:** Imagine you have a big toy factory that needs to send out lots of toy trucks (data packets) to kids all over the world. These trucks need to take different roads (network paths) to get there. There are special road planners (packet schedulers) who decide which road each truck should take. There are also traffic cops (congestion control algorithms) on the roads who watch out for jams and tell trucks to slow down if there's too much traffic.\n\n**Explanation:** This paper is like watching which road planners and traffic cops work best together. Some traffic cops are really good; they know how to keep the roads clear so the road planners can send trucks super fast without getting stuck. Other traffic cops get scared easily and make all the trucks go too slowly, even if the road isn't really busy, which makes the road planners sad because they can't do their best work. We measured how well the road planners got toys delivered (that's the 'PS_score'), and how well the traffic cops helped everyone (that's the 'CCA_score'). The best traffic cops and road planners were found by seeing who delivered the most toys, and also who did it steadily, without too many ups and downs on different kinds of roads."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\nVideo understanding benchmarks should test whether models build coherent, predictive world models from long video\nstreams. This requires accumulating visual evidence, maintaining state, and tracking entities over time\u2014abilities\nthat humans use naturally but current models largely lack. In practice, many benchmarks do not meet this goal.\nPrior work shows that models can often ignore most of the temporal structure in the video, rely on a single frame,\nor even use text-only inputs and still match or surpass state-of-the-art _video_ models (Yang et al., 2025b; Tangemann\net al., 2020; Buch et al., 2022; Lei et al., 2023; Li et al., 2018; Goyal et al., 2017; Brown et al., 2025). Much of what\nwe call _video understanding_ reduces to image captioning or language modeling with minimal temporal reasoning.\n\nThis is known as shortcut learning (Geirhos et al., 2020). Benchmark designers aim for tasks that are hard to solve\nvia shortcuts, so that the easiest way to improve performance is to develop the intended capability (Brown et al.,\n2025). When this fails, the design can unintentionally nudge practitioners into baking those same shortcuts into\ntheir models (Hammoud et al., 2023; Dehghani et al., 2021). Large models often similarly exploit spurious cues\ninstead (Geirhos et al., 2020). A recent work, Cambrian-S (Yang et al., 2025b) proposes to address these issues.\n\nCambrian-S argues that benchmarks should target specific capabilities: streaming event cognition, implicit 3D\nreasoning, and _spatial supersensing_ i.e., whether models form predictive world models from raw video. To resist\nbrute-force context expansion, Cambrian-S introduces the VSI-Super benchmarks which need true spatial reasoning\nabilities. The tasks use long, multi-environment videos and questions designed to require accumulating evidence,\ntracking objects, and maintaining state over time. The promise is clear: strong performance on these benchmarks\nshould require genuine spatial cognition and memory. This motivates our central questions about VSI-Super:\n\n1. Does success on _VSI-Super-Recall_ (VSR) truly require supersensing or even spatial cognition, or can a simple\nSigLIP model which discards almost all temporal structure still perfectly solve the benchmark?\n\n2. Does the Cambrian-S inference method perform the intended event-level reasoning and spatial supersensing, or\ndoes it rely on shortcuts specific to the benchmark like _VSI-Super-Counting_ (VSC)?\n\nTo investigate these questions, we conduct two complementary analyses:\n\n1. **VSR: A Simple Baseline.** We take SigLIP-2, select the four frames most relevant to the object mentioned\nin the question, and select the most correlated answer option. We call the resulting baseline _NoSense_, short\nfor _No Supersensing_ . NoSense uses no explicit 3D representation, does not track objects, and does not use\nlanguage models. Despite this simplicity, NoSense nearly perfectly solves VSR, even on 4-hour videos, while\ndiscarding almost all temporal structure in the video. This suggests that VSI-Super-Recall, like many prior video\nbenchmarks, primarily measures image-level semantic cues and object\u2013context associations rather than spatial\nsupersensing, and only needs a few informative frames rather than long-range temporal understanding.\n\n2. **VSC: A Sanity Check.** Does the inference method of Cambrian-S, designed for each benchmark, simply\nexploit shortcuts in the benchmark? We evaluate this by creating _VSC-Repeat_ : We take each video clip in\nthe VSC benchmark and concatenate it with itself 1-5 times, creating a sequence in which every room and\nobject is revisited repeatedly. A model performing genuine supersensing should recognize that these are repeated\nobservations of the same scene; therefore, the predicted object counts should remain unchanged. However,\nrepeating the video a few times reduces the mean relative accuracy of Cambrian-S from 42% to 0%. This indicates\nthe inference algorithm inadvertently exploits the benchmark shortcuts rather than improving supersensing.\n\nOur findings do not imply that there has been no progress in improving spatial supersensing. Gemini 2.5 Flash\nachieves around 41.5% accuracy on 60-minute VSR videos, illustrating that substantial VSR performance can arise\nfrom generic long-context video training. While VSI-Super is a comprehensive, thoughtful effort, it still fails to\n\n                                                       reliably enforce the long-horizon spatial supersensing it aims to elicit. Cambrian S\u2019 VSR inference pipeline and\n_NoSense_ share a similar high-level pattern \u2013 streaming frame encoding, a compact memory of salient frames, and\nquery-guided retrieval \u2013 suggesting that a much simpler instantiation of this pattern already goes a long way on\nVSR. Our simple sanity check, _VSC-Repeat_ shows the degree to which VSC-tailored inference algorithms rely on\nshortcuts. Our two-fold analysis underscores the need for caution when interpreting performance on VSI-Super:\nrather than reflecting genuine spatial cognition, high scores may simply mean exploiting heuristic shortcuts. We\nfurther include a response from the authors (in Appendix A) to provide a balanced perspective alongside our claims.\n\n2\n\nOverall, our tools \u2013 _NoSense_ for VSR and _VSC-Repeat_ for VSC \u2013 act as simple stress tests for benchmarks and\ntraining methods. We view this kind of meta-evaluation as analogous to ablation studies for models and believe\nit should be standard for benchmarks. For any proposed spatial supersensing task, we should ask: what is the\nsimplest streaming baseline that exploits obvious regularities? Which invariances should hold, and do methods\nrespect them? We hope our analysis fosters more rigorous testing and design of superspatial sensing in the future."}, {"role": "assistant", "content": "**Analogy:** Imagine you're building a super tall LEGO tower, and the goal is to make sure it stands strong for a long time, even if you add more and more blocks. But instead of carefully building it brick by brick from the bottom up, some kids just stack a few big blocks on top of each other quickly and say, \"Look, it's a tower!\" It looks like a tower, but it's not really strong or well-built from the ground up like it should be.\n\n**Explanation:** We want computers to watch long movies, like a whole day of cartoons, and understand the story from beginning to end, remembering who went where and what happened. This is like remembering all the places you visited on a long trip. But it turns out, the games we give computers to test this aren't very good. The computers can often just look at a few quick pictures from the movie, or even just read a short description, and still guess the right answers, like peeking at the last page of a book to know the ending without reading the whole story. Someone made new, harder games for computers to really watch the whole movie and remember things, hoping they couldn't cheat anymore. But what we found is that even with these new games, the computers still found clever ways to trick us. They still didn't really remember the whole story from beginning to end, but instead found quick ways to get the answers by only looking at small parts. So, we made our own simple \"peeking\" computer to show that if even our simple \"peeking\" computer can get the right answer, then the game isn't really testing what we thought it was!"}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n**17**\n\n3.1 The Threshold Decay Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n\n3.2 Errors Corresponding to Partial Linear Systems . . . . . . . . . . . . . . . . . . . . . 19\n\n3.3 The Efficient Partial Solver . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n\n3.4 The Almost-Linear-Time SDDM Solver . . . . . . . . . . . . . . . . . . . . . . . . . 22\n\n3.4.1 Bounding the sum of sizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n\n3.4.2 Maintaining the vector _**b**_ [\ufffd] ( _t_ ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n\n3.4.3 Maintaining _I_ [(] _[t]_ [)] and _H_ [(] _[t]_ [)] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n\n3.4.4 Wrap-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26"}, {"role": "assistant", "content": "**Analogy:** Imagine you have a giant pile of LEGO bricks, and you need to build a super-duper complicated castle. The 'Threshold Decay Framework' is like having a special instruction book that tells you to focus on building the most important parts of the castle first, and not worry about tiny decorations until later. The 'solvers' are like little helper robots. A 'partial solver' is a robot that builds only one room, and the 'almost-linear-time SDDM solver' is a super-fast robot that can build almost the entire castle as quickly as you can count the bricks!\n\n**Explanation:** This part of the book is about a clever new way to solve very big math puzzles super, super quickly. It's like finding a secret shortcut to get home, even if you have a lot of errands to run. They explain how to pick out the most important pieces of the puzzle first and then use special speedy helpers to finish the rest super-fast, so you don't waste time on things that don't matter much right away. The different parts explain how these speedy helpers keep track of all the different bricks and pieces so nothing gets lost or confused."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n_j \u2208_ _`UndecidedNeighbors`_ **do**\n\n**6** Choose strategy _si_ ( _j_ ) _\u2208{_ r _,_ p _,_ s _}_ uniformly at random and send to _j_ ;\n\n```\n   /* Round 2: Join MIS if possible */\n```"}, {"role": "assistant", "content": "**Analogy:** Imagine you and your friends are playing a game where some kids haven't picked a team yet. Each kid who hasn't picked a team yet needs to choose one of three things, like picking 'rock', 'paper', or 'scissors' for a game.\n\n**Explanation:** For every friend who hasn't chosen their team, they quickly and randomly pick one of those three things, like closing their eyes and pointing. Then, they tell their friend what they picked. This is just a first step to get ready for the next part of the game, which is like trying to find special groups of friends who don't touch each other."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n_\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc60\ud835\udc4e\ud835\udc61\ud835\udc52_ == _\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc52\ud835\udc35\ud835\udc4a_ **and** _\ud835\udc35\ud835\udc35\ud835\udc45_\ud835\udc3c\ud835\udc60_\ud835\udc41\ud835\udc52\ud835\udc65\ud835\udc61_\ud835\udc36\ud835\udc66\ud835\udc50\ud835\udc59\ud835\udc52_\ud835\udc43\u210e\ud835\udc4e\ud835\udc60\ud835\udc52_ () **then**\n\n**62** _\ud835\udc35\ud835\udc35\ud835\udc45_ _ _\ud835\udc34\ud835\udc51\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52_ _ _\ud835\udc36\ud835\udc66\ud835\udc50\ud835\udc59\ud835\udc52_ _ _\ud835\udc43\u210e\ud835\udc4e\ud835\udc60\ud835\udc52_ () ;\n\n**63** BBR_Is_Next_Cycle_Phase():\n\n**64** _\ud835\udc56\ud835\udc60_ _ _\ud835\udc53\ud835\udc62\ud835\udc59\ud835\udc59_ _ _\ud835\udc59\ud835\udc52\ud835\udc5b\ud835\udc54\ud835\udc61\u210e_ = ( _\ud835\udc41\ud835\udc5c\ud835\udc64_ () \u2212 _\ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc50\ud835\udc66\ud835\udc50\ud835\udc59\ud835\udc52_ _ _\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5a\ud835\udc5d_ ) _> \ud835\udc35\ud835\udc35\ud835\udc45.\ud835\udc45\ud835\udc47\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc5d_ ;"}, {"role": "assistant", "content": "**Analogy:** Imagine you're playing a game where you have to take turns. One turn you try to see how much water your bucket can carry (that's like 'ProbeBW'), and the next turn you wait a bit for things to settle. This little piece of code is like the game's timer keeper.\n\n**Explanation:** It's like the game keeper looking at the clock. If it's your turn to figure out how much water your bucket can carry AND enough time has passed since your last turn started (like watching the sand in an hourglass), then it's time to move on to the next part of the game. It helps the game run smoothly and fairly."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n_\ud835\udf0f_ _is updated_ **then**\n\n**11** _\ud835\udc64_ ( _\ud835\udf0f_ ) \u2190 _\ud835\udc64_ ( _\ud835\udf0f_ )/ _\ud835\udc64\ud835\udc60\ud835\udc62\ud835\udc5a_ ( _\ud835\udf0f\ud835\udc60\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc52\ud835\udc51_ ) \u00b7 _\ud835\udc45_ ( _\ud835\udc62, \ud835\udc63_ );"}, {"role": "assistant", "content": "**Analogy:** Imagine you have a big box of toys, and each toy has a number on it that says how 'super' it is. When you play a game, you pick some super toys. After the game, you look at how much fun you had (that's the R(u,v)) and how 'super' all the toys you picked were together (that's wsum(\ud835\udf0fselected)). Then, for one specific toy, you change its 'super' number.\n\n**Explanation:** This part is like deciding if your toy, 'tau', should get more 'super points' or fewer. If it was part of a fun game, and it was a pretty super toy compared to the others picked, it might get even more super points for next time! It's like giving your favorite toy a few more 'star stickers' if it did really well."}]}
{"messages": [{"role": "user", "content": "Explain this like I'm 5:\n**38**\n\nG.1 Reinforcement Learning Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n\nG.2 Multi Agent Reinforcement Learning Experiments . . . . . . . . . . . . . . . . . . . . . . 39\n\nG.3 Reasoning Fine-tuning Experiments: Countdown . . . . . . . . . . . . . . . . . . . . . . . 41\n\nG.4 Reasoning Fine-tuning Experiments: GSM8K . . . . . . . . . . . . . . . . . . . . . . . . . 43\n\n18\n\n**A** **Proofs**\n\nIn our proofs, we use the integral notation \ufffd to denote the integral over the corresponding R _d_ space, for\nexample, for a matrix _E \u2208_ R _[m][\u00d7][n]_, \ufffd _f_ ( _E_ ) _dE_ = \ufffdR _[m][\u00d7][n][ f]_ [(] _[E]_ [)] _[dE]_ [ and for a vector] _[ E][ \u2208]_ [R] _[mn]_ [,] \ufffd _f_ ( _v_ ) _dv_ =\n\nexample, for a matrix _E \u2208_ R _[m][\u00d7][n]_, \ufffd _f_ ( _E_ ) _dE_ = \ufffdR _[m][\u00d7][n][ f]_ [(] _[E]_ [)] _[dE]_ [ and for a vector] _[ E][ \u2208]_ [R] _[mn]_ [,] \ufffd _f_ ( _v_ ) _dv_ =\n\n\ufffdR _[mn][ f]_ [(] _[v]_ [)] _[dv]_ [.]\n\n**A.1** **ES Matrix Gradient Deviations**\n\n**Proposition 1.** _Using the Gaussian matrix policy \u03c0_ ( _M_ _|\u00b5_ ) = _N_ ( _\u00b5, Im\u03c3_ [2] _, In\u03c3_ [2] ) _the ES objective can be_\n_written as:_\n\n_J_ ( _\u00b5_ ) = E _E\u223cP_ ( _E_ ) [ _f_ ( _M_ = _\u00b5_ + _\u03c3E_ )] _._\n\n_and the matrix gradient of the ES objective is:_\n\n_\u2207\u00b5J_ ( _\u00b5_ ) = _\u2212_ [1]\n\n_\u03c3_ [E] _[E][\u223c][P]_ [ (] _[E]_ [)][ [] _[E][ \u00b7][ f]_ [(] _[M]_ [ =] _[ \u00b5]_ [ +] _[ \u03c3E]_ [)]] _[ .]_\n\n_where P_ ( _E_ ) _is a zero-mean standard normal p_ ( _E_ ) = _N_ (0 _, Im, In_ ) _. Moreover, the natural matrix gradient is_\n_equal to \u03c3_ [2] _\u2207\u00b5J_ ( _\u00b5_ ) _, i.e. equivalent to Eq._ (4) _up to a factor of \u03c3_ [2] _._\n\n_Proof._ We start by deriving the derivative of the ES objective from Eq. (1). Taking the derivative with respect\n\nto _\u00b5_ :\n\n_\u2207\u00b5J_ ( _\u00b5_ ) = _f_ ( _M_ ) _\u2207\u00b5\u03c0_ ( _M_ _|\u00b5_ ) _dM,_\n\ufffd\n\n= _f_ ( _M_ ) _\u2207\u00b5_ log _\u03c0_ ( _M_ _|\u00b5_ ) _\u03c0_ ( _M_ _|\u00b5_ ) _dM._\n\ufffd\n\nNow,\n\n1\n_\u2207\u00b5_ log _\u03c0_ ( _M_ _|\u00b5_ ) = _\u2207\u00b5 \u2212_ 2 _\u03c3_ [2] [tr] \ufffd( _M \u2212_ _\u00b5_ ) _[\u22a4]_ ( _M \u2212_ _\u00b5_ )\ufffd _,_\n\nR _[mn][ f]_ [(] _[v]_ [)] _[dv]_ [.]\n\n= _\u2212_ [1]\n\n_\u03c3_ [2] [(] _[M][ \u2212]_ _[\u00b5]_ [)] _[,]_\n\nwhere we have used Petersen & Pedersen (2008, Eq. 103) for the matrix derivative of the trace, hence:\n\n_\u2207\u00b5J_ ( _\u00b5_ ) = _\u2212_ [1]\n\n_\u03c3_\n\n( _M \u2212_ _\u00b5_ )\n\n_f_ ( _M_ ) _\u03c0_ ( _M_ _|\u00b5_ ) _dM._\n\n\ufffd _\u03c3_\n\nWe make the transformation of variables _E_ = _[M]_ _[\u2212][\u00b5]_ :\n\n_\u03c3_\n\n_\u2207\u00b5J_ ( _\u00b5_ ) = _\u2212_ [1]\n\n_\u03c3_\n\n_E \u00b7 f_ ( _M_ ) _p_ ( _E_ ) _dE,_\n\ufffd\n\nwhere\n\n_p_ ( _E_ ) = _\u03c3_ _[mn]_ _\u03c0_ ( _M_ = _\u00b5_ + _\u03c3E|\u00b5_ )\n\n_\u2212_ [1]\nexp\n2 \ufffd 2\n\n1\n\n= ~~_mn_~~\n(2 _\u03c0_ ) 2\n\n[1] \ufffd _E_ _[\u22a4]_ _E_ \ufffd [\ufffd] _,_\n\n2 [tr]\n\n= _N_ (0 _, Im, In_ ) _,_\n\nas required. Using the same transformation of variables, the alternative form of the matrix Gaussian ES\nobjective follows immediately:\n\n_J_ ( _\u00b5_ ) = _f_ ( _M_ ) _\u03c0_ ( _M_ _|\u00b5_ ) _dM,_\n\ufffd\n\n= _f_ ( _M_ = _\u00b5_ + _\u03c3E_ ) _P_ ( _E_ ) _dE._\n\ufffd\n\n19\n\nWe now derive the natural policy gradient for a Gaussian policy to prove our final claim. It is convenient to work\nin vector space rather than matrix space. In vector form, the natural gradient is defined as _F_ ( _\u00b5v_ ) _[\u2212]_ [1] _\u2207\u00b5v_ _J_ ( _\u00b5v_ )\nwhere _F_ ( _\u00b5v_ ) is the Fisher information matrix of the distribution:\n\n_F_ ( _\u00b5v_ ) := E _v\u223c\u03c0_ ( _v|\u00b5v_ ) \ufffd _\u2207\u00b5v_ log _\u03c0_ ( _v|\u00b5v_ ) _\u2207\u00b5v_ log _\u03c0_ ( _v|\u00b5v_ ) _[\u22a4]_ [\ufffd] _,_\n\nwhere _\u00b5v_ = vec( _M_ ) and _\u03c0_ ( _v|\u00b5v_ ) = _N_ ( _\u00b5v, \u03c3_ [2] _Imn_ ) for our Gaussian case. Deriving the score function:\n\n_\u2207\u00b5v_ log _\u03c0_ ( _v|\u00b5v_ ) = _\u2212_ 2 _\u03c3_ [1] [2] _[\u2207][\u00b5][v]_ _[\u2225][\u00b5][v][ \u2212]_ _[v][\u2225]_ [2] _[,]_\n\n= [1]\n\n_\u03c3_ [2] [(] _[v][ \u2212]_ _[\u00b5][v]_ [)] _[,]_\n\nHence the Fisher information matrix is:\n\n_F_ ( _\u00b5v_ ) = _\u03c3_ [1][4] [E] _[v][\u223c][\u03c0]_ [(] _[v][|][\u00b5][v]_ [)] \ufffd( _v \u2212_ _\u00b5v_ )( _v \u2212_ _\u00b5v_ ) _[\u22a4]_ [\ufffd] _,_\n\nThis yields the natural gradient:\n\n= [1]\n\n_\u03c3_ [2] _[I][mn][.]_\n\n_F_ ( _\u00b5v_ ) _[\u2212]_ [1] _\u2207\u00b5v_ _J_ ( _\u00b5v_ ) = _\u03c3_ [2] _\u2207\u00b5v_ _J_ ( _\u00b5v_ ) _._\n\nConverting back to matrix form, the natural gradient is thus _\u03c3_ [2] _\u2207\u00b5J_ ( _\u00b5_ ) as required.\n\n**Theorem 1.** _Let Assumption 1 hold. Then for finite r:_\n\n_gLR_ := _\u2207\u00b5JLR_ ( _\u00b5_ ) = _\u2212_ [1]\n\n_\u03c3_ [E] _[Z][\u223c][p]_ [(] _[Z]_ [)][ [] _[\u2207][Z]_ [ log] _[ p]_ [ (] _[Z]_ [)] _[ f]_ [(] _[y]_ [ = (] _[\u00b5]_ [ +] _[ \u03c3Z]_ [)] _[x]_ [)]] _[ .]_\n\n_where Z is generated by sampling A \u223c_ _p_ ( _A_ ) _, B \u223c_ _p_ ( _B_ ) _, \u03f5 \u223c_ _p_ ( _\u03f5_ ) _and making the transformation Z_ =\n_AB_ _[\u22a4]_\n\n+ _\u03f5._\n~~_\u221a_~~ _r_\n\n_Proof._ We first prove that the density _p_ ( _Z_ ) of the pushforward distribution of _p_ ( _A_ ), _p_ ( _B_ ) and _p_ ( _\u03f5_ ) under the\nmapping ( _A, B, \u03f5_ ) : _\u2192_ _Z_ = _[AB]_ ~~_\u221a_~~ _r_ _[\u22a4]_ + _\u03f5_ exists. As _r_ is finite, _p_ ( _\u03f5_ ) is a Gaussian with well-defined density and _Z_\n\nis a sum two independent variables _[AB]_ ~~_\u221a_~~ _r_ _[\u22a4]_ and _\u03f5_ . The density _p_ ( _Z_ ) is thus given by the convolution (see, for\n\nexample, Durrett (2010, Theorem 2.1.11)):\n\n_p_ ( _Z_ ) = E _A\u223cp_ ( _A_ ) _,B\u223cp_ ( _B_ )\n\n_p_ _\u03f5_ = _Z \u2212_ _[AB][\u22a4]_ _,_\n\ufffd \ufffd ~~_\u221ar_~~ \ufffd\ufffd\n\nwhich integrates to 1 over R _[m][\u00d7][n]_ and, since _p_ ( _\u03f5_ ) is everywhere positive, is also everywhere positive. The\ndistribution of _Z_ is thus absolutely continuous with respect to the Lebesgue measure in R _[m][\u00d7][n]_ and the density\n_p_ ( _Z_ ) is well-defined. To derive the gradient, we make the transformation of variables _Z_ _[\u2032]_ = _\u00b5_ + _\u03c3Z_ :\n\n_J_ LR( _\u00b5_ ) = _p_ ( _Z_ _[\u2032]_ ) _f_ ( _y_ = _Z_ _[\u2032]_ _x_ ) _dZ_ _[\u2032]_ _,_\n\ufffd\n\nwhere _p_ ( _Z_ _[\u2032]_ ) = _\u03c3_ _[mn]_ 1 _[p]_ [(] _[Z]_ [ =] _[ Z][\u2032]_ _\u03c3_ _[\u2212][\u00b5]_ ) Taking derivatives with respect to _\u00b5_ :\n\n1\n_\u2207\u00b5J_ LR( _\u00b5_ ) = \ufffd _\u03c3_ _[mn]_ _[\u2207][\u00b5][p]_ \ufffd _Z_ = _[Z]_ _[\u2032][ \u2212]_ _\u03c3_ _[\u00b5]_ \ufffd _f_ ( _y_ = _Z_ _[\u2032]_ _x_ ) _dZ_ _[\u2032]_ _,_\n\n= _p_ ( _Z_ _[\u2032]_ ) _\u2207\u00b5_ log _p_ _Z_ = _[Z]_ _[\u2032][ \u2212]_ _[\u00b5]_\n\ufffd \ufffd _\u03c3_\n\n_f_ ( _y_ = _Z_ _[\u2032]_ _x_ ) _dZ_ _[\u2032]_ _._\n\ufffd\n\nFrom the chain rule:\n\n_\u2207\u00b5_ log _p_ _Z_ = _[Z]_ _[\u2032][ \u2212]_ _[\u00b5]_\n\ufffd _\u03c3_\n\n= _\u2212_ [1]\n\ufffd _\u03c3_\n\n[1] _Z_ = _[Z]_ _[\u2032][ \u2212]_ _[\u00b5]_\n\n_\u03c3_ _[\u2207][Z]_ [ log] _[ p]_ \ufffd _\u03c3_\n\n_,_\n\ufffd\n\n_\u03c3_\n\n20\n\nhence:\n\n_f_ ( _y_ = _Z_ _[\u2032]_ _x_ ) _dZ_ _[\u2032]_ _._\n\ufffd\n\n_\u2207\u00b5J_ LR( _\u00b5_ ) = _\u2212_ [1]\n\n_\u03c3_\n\n_p_ ( _Z_ _[\u2032]_ ) _\u2207Z_ log _p_ _Z_ = _[Z]_ _[\u2032][ \u2212]_ _[\u00b5]_\n\ufffd \ufffd _\u03c3_\n\nMaking the reverse transformation of variables _Z_ = _[Z][\u2032]_ _\u03c3_ _[\u2212][\u00b5]_ :\n\n_\u2207\u00b5J_ LR( _\u00b5_ ) = _\u2212_ [1]\n\n_\u03c3_\n\n_p_ ( _Z_ ) _\u2207Z_ log _p_ ( _Z_ ) _f_ ( _y_ = ( _\u00b5_ + _\u03c3Z_ ) _x_ ) _dZ,_\n\ufffd\n\n= _\u2212_ [1]\n\n_\u03c3_ [E] _[Z][\u223c][p]_ [(] _[Z]_ [)][ [] _[\u2207][Z]_ [ log] _[ p]_ [ (] _[Z]_ [)] _[ f]_ [(] _[y]_ [ = (] _[\u00b5]_ [ +] _[ \u03c3Z]_ [)] _[x]_ [)]] _[,]_\n\nas required.\n\n**A.2** **Error Rate Analysis**\n\nFor convenience, we work with random vectors in our analysis. Using the vec operator, we analyse the vector\n_v_ _[r]_ = vec( _Z_ _[r]_ ) where:\n\nvec( _M_ ) = [ _m_ 1 _,_ 1 _, . . . m_ 1 _,m, m_ 2 _,_ 1 _, . . . mm,n_ ] _[\u22a4]_ _._\n\nFor a vector _v \u2208_ R _[mn]_, we define the mat operator as:\n\n\uf8f9\n\n_,_\n\uf8fa\uf8fa\uf8fa\uf8fb\n\nmat( _v_ ) =\n\n_v_ 1 _vm_ +1 _. . ._ _v_ ( _n\u2212_ 1) _m_ +1\n\n\uf8ee _v_ 2 _vm_ +2 _. . ._ _v_ ( _n\u2212_ 1) _m_ +2\n\n... ... ... ...\n\n\uf8ef\uf8ef\uf8ef\uf8f0 _vm_ _v_ 2 _m_ _\u00b7 \u00b7 \u00b7_ _vmn_\n\nso mat(vec( _M_ )) = _M_ . Using Eq. (6), write _v_ _[r]_ as a standardised sum of _r_ independent, zero-mean random\nvectors. Let\n\n_ui_ = vec \ufffd _aib_ _[\u22a4]_ _i_ [+] _[ \u03f5][i]_ \ufffd _,_ (10)\n\nwhere recall _ai_ and _bi_ are the _i_ th column vectors of _A_ and _B_ and _\u03f5i_ is an _m\u00d7n_ random matrix with independent\nGaussian elements drawn from _N_ (0 _,_ _[\u03c3]_ _\u03f5_ [2] _/r_ ), so:\n\n1\n_v_ _[r]_ =\n~~_\u221ar_~~\n\n_r_\n\ufffd _u_ _[r]_ _i_ _[.]_\n\n_i_ =1\n\nWe will use the fact that the Frobenius norm becomes the _\u2113_ -2 norm in vector space:\n\n~~\ufffd~~ _mi,j_ [2] = ~~\ufffd~~\n\n_i,j_ \ufffd _k_\n\n_\u2225M_ _\u2225F_ = ~~\ufffd~~\n\ufffd _i,j_\n\nvec( _M_ ) _k_ 2 = _\u2225_ vec( _M_ ) _\u2225._ (11)\n\n_k_\n\nDenoting the covariance matrix of _p_ ( _u_ ) as \u03a3 _[r]_, the central limit theorem proves that the distribution of _v_ _[r]_\n\nconverges in distribution to a zero-mean Gaussian _N_ (0 _,_ \u03a3 _r_ ). In Lemma 1, derive the covariance matrix for \u03a3 _[r]_ .\nOur analysis uses an Edgeworth expansion (Bhattacharya & Ranga Rao, 1976) to characterise precisely the rate\nat which _p_ ( _v_ _[r]_ ) converges to the limiting Gaussian distribution. In Lemma 2, we make an Edgeworth expansion\nof _p_ ( _v_ _[r]_ ) to show that it is dominated by _O_ \ufffd 1 _r_ \ufffd terms and higher. These are then used to prove Lemma 3,\n\nwhich allows us to bound the integral of the remainder of the Edgeworth expansion, thereby characterising\nhow fast _p_ ( _v_ _[r]_ ) converges to the limiting Gaussian distribution.\n**Lemma 1.** _Let Assumption 1 hold and u be defined in Eq._ (10) _. Then the variable u has diagonal covariance_\n\n_matrix:_\n\n\u03a3 _[r]_ = _\u03c3_ 0 [4] [+] _[\u03c3]_ _\u03f5_ [2]\n\ufffd _r_\n\n_Imn_ =: _\u03c3r_ [2] _[I][mn][,]_\n\ufffd\n\n_Moreover, the variable u has a finite_ 4 _th-order absolute moment:_\n\nE _u\u223cp_ ( _u_ ) \ufffd _\u2225u\u2225_ [4][\ufffd] _< \u221e._\n\n21\n\n_Proof._ Under the vec operator, the vector _u_ can be written element wise as:\n\n_u_ = [ _a_ 1 _b_ 1 + _\u03f5_ 1 _,_ 1 _, a_ 2 _b_ 1 + _\u03f5_ 2 _,_ 1 _, . . . amb_ 1 + _\u03f5m,_ 1 _, a_ 1 _b_ 2 + _\u03f5_ 1 _,_ 2 _, . . . ambn_ + _\u03f5m,n_ ] _[\u22a4]_ _._\n\nWe note that all elements in the vector _u_ have zero mean, and so the covariance matrix is the expectation of the\nouter product:\n\n\u03a3 _[r]_ = E _u\u223cp_ ( _u_ ) \ufffd _uu_ _[\u22a4]_ [\ufffd] _._\n\nThe diagonal elements of \u03a3 _[r]_ are:\n\nE _ai,bj_ _,\u03f5j,i_ \ufffd( _aibj_ + _\u03f5j,i_ ) [2][\ufffd] = E _ai,bj_ _,\u03f5j,i_ \ufffd _a_ [2] _i_ _[b]_ [2] _j_ [+ 2] _[a][i][b][j][\u03f5][j,i]_ [+ (] _[\u03f5][j,i]_ [)][2][\ufffd] _,_\n\n= E _ai,bj_ \ufffd _a_ [2] _i_ _[b]_ [2] _j_ \ufffd + E _\u03f5j,i_ \ufffd( _\u03f5j,i_ ) [2][\ufffd] _,_\n\n= _\u03c3_ 0 [4] [+] _[ \u03c3]_ _r\u03f5_ [2] _[.]_ (12)\n\nAs all elements of _a_, _b_ and _\u03f5_ are zero-mean, off-diagonal elements are zero:\n\nE _ai,bj_ _,ak,bl,\u03f5j,i,\u03f5k,l_ [( _aibj_ + _\u03f5j,i_ ) ( _akbl_ + _\u03f5k,l_ )]\n\n=E _ai,bj_ _,ak,bl_ [ _aibjakbl_ ] + E _ai,bj_ _,\u03f5k,l_ [ _aibj\u03f5k,l_ ]\n\n+ E _ak,bl,\u03f5j,i_ [ _ak, bl, \u03f5j,i_ ] + E _\u03f5j,i,\u03f5l,k_ [ _\u03f5j,i\u03f5k,l_ ] _,_\n\n=E _ai,bj_ _,ak,bl_ [ _aibjakbl_ ] + E _\u03f5j,i,\u03f5l,k_ [ _\u03f5j,i\u03f5k,l_ ] _,_\n\n=0 _i \u0338_ = _k_ or _j \u0338_ = _l._ (13)\n\nUsing Eqs. (12) and (13), our first result follows:\n\n\u03a3 _[r]_ = _\u03c3_ 0 [4] [+] _[ \u03c3]_ _\u03f5_ [2]\n\ufffd _r_\n\n_Imn._\n\ufffd\n\nNow, as _u_ is a vector of elements which are sums and products of variables which all have finite 4th order\nmoments from Assumption 1, it immediately follows that _u_ has finite 4th order absolute moments.\n\nUsing Lemma 1, we see the asymptotic Gaussian density of _v_ _[r]_ is:\n\n1\n\n_\u2212_ _[\u2225][v][r][\u2225]_ [2]\nexp\n(2 _\u03c0_ ) _[mn]_ _\u03c3r_ [2] \ufffd 2 _\u03c3r_ [2]\n\n1\n_g_ ( _v_ _[r]_ ) =\n~~\ufffd~~ (2 _\u03c0_ )\n\n2 _\u03c3r_ [2]\n\n_._ (14)\n\ufffd\n\nWe now derive a 4th order Edgeworth expansion for _p_ ( _v_ _[r]_ ) and its score function, which expands _p_ ( _v_ _[r]_ ) in terms\nof the limiting distribution _g_ ( _v_ _[r]_ ) plus remainder terms. Our proof reveals that 3rd order cumulants control all\nterms in the expansion that decay at rate _O_ \ufffd ~~_\u221a_~~ 1 _r_ \ufffd. As 3rd order cumulants are all zero due to symmetry in\n\nAssumption 1, the overall decay rate is controlled by _O_ \ufffd 1 _r_ \ufffd terms associated with 4th order cumulants. It is\n\nfor this reason that we obtain a faster convergence rate than the standard central limit theorem.\n**Lemma 2.** _Let Assumption 1 hold and let v_ _[r]_ = _vec_ ( _Z_ _[r]_ ) _and ui be defined in Eq._ (10) _. Let g_ ( _v_ _[r]_ ) _denote_\n_limiting Gaussian density in Eq._ (14) _. Then, the Egdeworth expansion of p_ ( _v_ _[r]_ ) _is:_\n\n_p_ ( _v_ _[r]_ ) = _g_ ( _v_ _[r]_ ) + [1]\n\n_\u03ba_ [4]\n\n\ufffd _i,j,k,l_ _[H][i,j,k,l]_ [(] _[v][r][, \u03c3][r]_ [) +] _[ \u03c1]_ [(] _[v][r]_ [)] _[,,]_\n\n_i,j,k,l_\n\n4! _r_ _[g]_ [(] _[v][r]_ [)] \ufffd\n\n_where:_\n\n_\u2225vr\u2225_ 2\n_Hi,j,k,l_ ( _v_ _[r]_ _, \u03c3r_ ) := exp\n\ufffd 2 _\u03c3r_ [2]\n\n_\u2202_ [4]\n\ufffd _\u2202vi_ _[r][\u2202v][r]_\n\n_\u2202_ [4]\n\n_\u2212_ _[\u2225][v][r][\u2225]_ [2]\nexp\n_\u2202vi_ _[r][\u2202v]_ _j_ _[r][\u2202v]_ _k_ _[r][\u2202v]_ _l_ _[r]_ \ufffd 2 _\u03c3r_ [2]\n\n2 _\u03c3r_ [2]\n\n\ufffd\n\n_is a 4th order Hermite polynomial for g_ ( _v_ _[r]_ ) _(Laplace, 1811; Hall, 1992; Withers, 2000) and \u03c1_ ( _v_ _[r]_ ) _is o_ \ufffd 1 _r_ \ufffd _._\n\n22\n\n_Proof._ We denote the characteristic function of _p_ ( _u_ _[r]_ _i_ [)][ as:]\n\n_\u03c6U r_ ( _\u03c9_ ) = exp \ufffd _\u2212i\u03c9_ _[\u22a4]_ _u_ \ufffd _p_ ( _u_ ) _du._\n\ufffd\n\nNow, make the transformation of variables _x_ _[r]_ _i_ [=] ~~_\u221a_~~ _u_ _[r]_ _ir_, hence _p_ ( _x_ _[r]_ _i_ [) =] _[ \u221a]_ ~~_[r]_~~ _[p]_ [(] _[u][r]_ _i_ [=] _[ \u221a]_ ~~_[r]_~~ _[x][r]_ _i_ [)][ and denote the]\ncharacteristic function of _x_ _[r]_ _i_ [as] _[ \u03c6][X]_ _[r]_ [(] _[\u03c9]_ [)][. Using the time-scaling property of the Fourier transform, we can]\nderive the characteristic function of _x_ _[r]_ _i_ [in terms of] _[ \u03c6][U][ r]_ [(] _[\u03c9]_ [)][:]\n\n_\u03c6X_ _[r]_ ( _\u03c9_ ) = exp \ufffd _\u2212i\u03c9_ _[\u22a4]_ _x_ \ufffd _p_ ( _x_ ) _dx,_\n\ufffd\n\n= _[\u221a]_ _r_ exp \ufffd _\u2212i\u03c9_ _[\u22a4]_ _x_ \ufffd _p_ ( _u_ = _[\u221a]_ _rx_ ) _dx,_\n\ufffd\n\n1\n= _[\u221a]_ _r_ exp _\u2212i\u03c9_ _[\u22a4]_ _[u]_ _p_ ( _u_ ) _du,_\n\ufffd ~~_\u221ar_~~ \ufffd ~~_\u221ar_~~ \ufffd\n\n= exp _\u2212i\u03c9_ _[\u22a4]_ _[u]_\n\ufffd \ufffd ~~_\u221ar_~~\n\n_p_ ( _u_ ) _du,_\n\ufffd\n\n_\u03c9_\n= _\u03c6U r_ _._ (15)\n\ufffd ~~_\u221ar_~~ \ufffd\n\nNow, as _v_ _[r]_ = [\ufffd] _[r]_ _i_ =1 _[x]_ _i_ _[r]_ [, we use the property that the density of a sum of] _[ r]_ [ independent random variables]\nis given by the _r_ -fold convolution of the individual PDFs. As convolution in the spatial domain is equal to\nmultiplication in the frequency domain, the density _p_ ( _v_ _[r]_ ) is:\n\n_p_ ( _v_ _[r]_ ) = _F_ _[\u2212]_ [1] [( _\u03c6X_ _r_ ) _[r]_ ] ( _v_ _[r]_ ) _,_\n\n= _F_ _[\u2212]_ [1] [exp ( _r_ log _\u03c6X_ _[r]_ )] ( _v_ _[r]_ ) _,_\n\n= _F_ _[\u2212]_ [1] [exp ( _rKX_ _[r]_ [)] (] _[v][r]_ [)] _[,]_ (16)\n\nwhere _KX_ _[r]_ [(] _[\u03c9]_ [) := log] _[ \u03c6][X]_ _[r]_ [(] _[\u03c9]_ [)][ is the] _[ cumulant generating function]_ [ (also known as the second characteristic]\nfunction) for _Xi_ _[r]_ [. From Eq. (][15][):]\n\n_KX_ _[r]_ [(] _[\u03c9]_ [) = log] _[ \u03c6][X]_ _[r]_ [(] _[\u03c9]_ [)] _[,]_\n\n_\u03c9_\n= log _\u03c6U r_\n\ufffd ~~_\u221ar_~~\n\n_,_\n\ufffd\n\n_\u03c9_\n= _KU_ _[r]_ _,_\n\ufffd ~~_\u221ar_~~ \ufffd\n\nwhere _KU r_ ( _\u00b7_ ) is the cumulant generating function for _u_ _[r]_ _i_ [. We now make a 4th order Taylor series expansion]\n\nof _rKU r_ \ufffd ~~_\u221a_~~ _\u03c9r_ \ufffd about _\u03c9_ = 0:\n\n_\u03c9_\n_rKU r_ = _r KU r_ (0)\n\ufffd ~~_\u221ar_~~ \ufffd \ufffd ~~\ufffd~~ \ufffd ~~\ufffd~~\n\n=log 1=0\n\n_\u2202KU_ _[r]_ ( _\u03c9_ = 0)\n_\u03c9i_ _\u2202\u03c9i_\n_i_\n\n+ _[\u221a]_ _r_ \ufffd\n\n_[r]_ ( _\u03c9_ = 0) + [1]\n\n_\u2202\u03c9i_ 2!\n\n2!\n\n_\u2202\u03c9i\u2202\u03c9j_\n\n_\u2202_ [2] _KU_ _[r]_ ( _\u03c9_ = 0)\n\n\ufffd _\u03c9i\u03c9j_ _\u2202\u03c9i\u2202\u03c9j_\n\n_i,j_\n\n\ufffd\n\n1\n+\n3! ~~_[\u221a]_~~ ~~_r_~~\n\n_\u2202_ [3] _KU r_ ( _\u03c9_ = 0)\n\n\ufffd _\u03c9i\u03c9j\u03c9k_ _\u2202\u03c9i\u2202\u03c9j\u2202\u03c9k_\n\n_i,j,k_\n\n\ufffd\n\n_KU r_ ( _\u03c9_ = 0) + [1]\n\n_\u2202\u03c9i\u2202\u03c9j\u2202\u03c9k_ 4!\n\n4! _r_\n\n_KU r_ ( _\u03c9_ = 0) 1\n\n+ _o_\n_\u2202\u03c9i\u2202\u03c9j\u2202\u03c9k_ \ufffd _r_\n\n_r_\n\n_\u2202_ [4] _KU r_ ( _\u03c9_ = 0)\n\n\ufffd _\u03c9i\u03c9j\u03c9k\u03c9l_ _\u2202\u03c9i\u2202\u03c9j\u2202\u03c9k_\n\n_i,j,k,l_\n\n\ufffd\n\n_,_\n\ufffd\n\n_\u03c9i\u03ba_ [1] _i_ [+] 2! [1]\n_i_\n\n\ufffd _\u03c9i\u03c9j\u03c9k\u03ba_ [3] _i,j,k_\n\n_i,j,k_\n\n= _[\u221a]_ _r_\n\ufffd\n\n2!\n\n1\n\n\ufffd _\u03c9i\u03c9j\u03ba_ [2] _i,j_ [+] 3! ~~_[\u221a]_~~ ~~_r_~~\n\n_i,j_\n\n+ [1]\n\n4! _r_\n\n1\n\n\ufffd _\u03c9i\u03c9j\u03c9k\u03c9l\u03ba_ [4] _i,j,k,l_ [+] _[ o]_ \ufffd _r_\n\n_i,j,k,l_\n\n\ufffd\n\n_r_\n\n_,_\n\ufffd\n\n_\u03c9i\u03ba_ [1] _i_ [+] 2! [1]\n_i_\n\n\ufffd _\u03c9i\u03c9j\u03c9k\u03ba_ [3] _i,j,k_ [+] _[ R]_ [(] _[\u03c9]_ [)] _[,]_ (17)\n\n_i,j,k_\n\n= _[\u221a]_ _r_\n\ufffd\n\n2!\n\n1\n\n\ufffd _\u03c9i\u03c9j\u03ba_ [2] _i,j_ [+] 3! ~~_[\u221a]_~~ ~~_r_~~\n\n_i,j_\n\n23\n\n[(] _[\u03c9]_ [=0][)]\nwhere _\u03ba_ _[n]_ _\u00b7_ [is an] _[ n]_ [-th order cumulant (i.e.] _[ \u03ba][n]_ _i_ [:=] _[ \u2202K][U]_ _\u2202\u03c9_ _[r]_ _i_ ) and\n\n_R_ ( _\u03c9_ ) := [1]\n\n4! _r_\n\n1\n\n\ufffd _\u03c9i\u03c9j\u03c9k\u03c9l\u03ba_ [4] _i,j,k,l_ [+] _[ o]_ \ufffd _r_\n\n_i,j,k,l_\n\n\ufffd\n\n_r_\n\n_,_\n\ufffd\n\n1\ndenotes the 4th order polynomial and _o_ \ufffd _r_\n\n1\ndenotes the 4th order polynomial and _o_ \ufffd _r_ \ufffd denotes the higher order remainder terms. We now state some facts\n\nabout cumulants to simplify the above expansion. For a symmetric random vector like _u_ _[r]_ under Assumption 3,\nall odd-order cumulants are zero, i.e. _\u03ba_ _[i]_ _\u00b7_ [= 0][ for odd] _[ i]_ [ (][McCullagh][,][ 2013][;][ 1987][). The second-order cumulant]\nterm coincides with the covariance matrix product: [\ufffd] _[\u03c9][i][\u03c9][j][\u03ba]_ [2] [=] _[ \u2212][\u03c9][\u22a4]_ [\u03a3] _[r][\u03c9]_ [ (][McCullagh][,][ 1987][). Moreover,]\n\nterm coincides with the covariance matrix product: [\ufffd] _i,j_ _[\u03c9][i][\u03c9][j][\u03ba]_ [2] _i,j_ [=] _[ \u2212][\u03c9][\u22a4]_ [\u03a3] _[r][\u03c9]_ [ (][McCullagh][,][ 1987][). Moreover,]\n\nas 4th order moments of _u_ _[r]_ exist from Lemma 1 then all cumulants up to including order 4 exist since\ncumulants are polynomial functions of the moments (Leonov & Shiryaev, 1959; McCullagh, 1987). Finally,\nwe use _\u03c9_ _[\u22a4]_ \u03a3 _[r]_ _\u03c9_ = _\u2225\u03c9\u2225_ [2] _\u03c3r_ [2] [from Lemma][ 1][. Substituting these results into Eq. (][17][) and using] _[ R]_ [(] _[\u03c9]_ [)][ to denote]\nyields:\n\n_\u03c9_\n_rKU r_\n\ufffd ~~_\u221ar_~~\n\n= _\u2212_ _[\u2225][\u03c9][\u2225]_ [2] _[\u03c3]_ _r_ [2] + _R_ ( _\u03c9_ ) _,_\n\ufffd 2\n\n= _\u21d2_ exp ( _rKX_ _[r]_ [(] _[\u03c9]_ [)) = exp] _\u2212_ _[\u2225][\u03c9][\u2225]_ [2] _[\u03c3]_ _r_ [2] + _R_ ( _\u03c9_ ) _,_\n\ufffd 2 \ufffd\n\n_\u2212_ _[\u2225][\u03c9][\u2225]_ [2] _[\u03c3]_ _r_ [2]\n= exp\n\ufffd 2\n\n_\u2212_ _[\u2225][\u03c9][\u2225]_ [2] _[\u03c3]_ _r_ [2]\n= exp\n\ufffd 2\n\n_\u2212_ _[\u2225][\u03c9][\u2225]_ [2] _[\u03c3]_ _r_ [2]\n= exp\n\ufffd 2\n\n1 + [1]\n\uf8ed 4!\n\nexp ( _R_ ( _\u03c9_ )) _,_\n\ufffd\n\n1\n1 + _R_ ( _\u03c9_ ) + _o_\n\ufffd\ufffd \ufffd _r_\n\n_,_\n\ufffd\ufffd\n\n\ufffd [\uf8eb]\n\n_r_\n\n_,_\n\n\ufffd [\uf8f6]\n\n\uf8f8\n\n\ufffd [\uf8f6]\n\n4! _r_\n\n1\n\n\ufffd _\u03c9i\u03c9j\u03c9k\u03c9l\u03ba_ [4] _i,j,k,l_ [+] _[ o]_ \ufffd _r_\n\n_i,j,k,l_\n\n\ufffd\n\nwhere we have used the Taylor series expansion of exp( _x_ ) about _x_ = 0 to derive the fourth line. Substituting\ninto Eq. (16) yields:\n\n_p_ ( _v_ _[r]_ ) = exp( _i\u03c9_ _[\u22a4]_ _v_ _[r]_ ) exp ( _rKX_ _[r]_ [(] _[\u03c9]_ [))] _[ d\u03c9,]_\n\ufffd\n\n\ufffd [\uf8eb]\n\n= exp( _i\u03c9_ _[\u22a4]_ _v_ _[r]_ ) exp _\u2212_ _[\u2225][\u03c9][\u2225]_ [2] _[\u03c3]_ _r_ [2]\n\ufffd \ufffd 2\n\n= exp( _i\u03c9_ _[\u22a4]_ _v_ _[r]_ ) exp _\u2212_ _[\u2225][\u03c9][\u2225]_ [2] _[\u03c3]_ _r_ [2]\n\ufffd \ufffd 2\n\n1 + [1]\n\uf8ed 4!\n\n_r_\n\n_d\u03c9,_\n\n\ufffd [\uf8f6]\n\n\uf8f8\n\n\ufffd [\uf8f6]\n\n4! _r_\n\n1\n\n\ufffd _\u03c9i\u03c9j\u03c9k\u03c9l\u03ba_ [4] _i,j,k,l_ [+] _[ o]_ \ufffd _r_\n\n_i,j,k,l_\n\n\ufffd\n\n_d\u03c9_\n\ufffd\n\n+ [1]\n\n4! _r_\n\n_\u03ba_ [4]\n\n\ufffd _i,j,k,l_\n\n_i,j,k,l_\n\n\ufffd\n\n_\u03c9i\u03c9j\u03c9k\u03c9l_ exp( _i\u03c9_ _[\u22a4]_ _v_ _[r]_ ) exp _\u2212_ _[\u2225][\u03c9][\u2225]_ [2] _[\u03c3]_ _r_ [2]\n\ufffd \ufffd 2\n\n1\n_d\u03c9_ + _o_ _,_\n\ufffd \ufffd _r_ \ufffd\n\n= exp( _i\u03c9_ _[\u22a4]_ _v_ _[r]_ ) exp _\u2212_ _[\u2225][\u03c9][\u2225]_ [2] _[\u03c3]_ _r_ [2] _d\u03c9_\n\ufffd \ufffd 2 \ufffd\n\n1\n_d\u03c9_ + _o_\n\ufffd \ufffd _r_\n\n_._\n\ufffd\n\n+ [1]\n\n4! _r_\n\n_\u2202_ [4]\n_\u03ba_ [4]\n_i,j,k,l_ \ufffd _i,j,k,l_ _\u2202vi_ _[r][\u2202v]_ _j_ _[r]_\n\n\ufffd\n\n_\u2202vi_ _[r][\u2202v]_ _j_ _[r][\u2202v]_ _k_ _[r][\u2202v]_ _l_ _[r]_\n\nexp( _i\u03c9_ _[\u22a4]_ _v_ _[r]_ ) exp _\u2212_ _[\u2225][\u03c9][\u2225]_ [2] _[\u03c3]_ _r_ [2]\n\ufffd \ufffd 2\n\nNow, recognising that exp _\u2212_ _[\u2225][\u03c9][\u2225]_ 2 [2] _[\u03c3]_ _r_ [2] is the characteristic function for _g_ ( _v_ _[r]_ ) yields our desired result:\n\ufffd \ufffd\n\n_p_ ( _v_ _[r]_ ) = _F_ _[\u2212]_ [1] [ _F_ [ _g_ ]]( _v_ _[r]_ ) + [1]\n\n4! _r_\n\n_\u2202_ [4]\n_\u03ba_ [4]\n_i,j,k,l_ \ufffd _i,j,k,l_ _\u2202vi_ _[r][\u2202v]_ _j_ _[r]_\n\n\ufffd\n\n_\u2202_ [4] 1\n\n_F_ _[\u2212]_ [1] [ _F_ [ _g_ ]]( _v_ _[r]_ ) + _o_\n_\u2202vi_ _[r][\u2202v]_ _j_ _[r][\u2202v]_ _k_ _[r][\u2202v]_ _l_ _[r]_ \ufffd _r_\n\n_r_\n\n_,_\n\ufffd\n\n= _g_ ( _v_ _[r]_ ) + [1]\n\n4! _r_\n\n_\u2202_ [4]\n_\u03ba_ [4]\n_i,j,k,l_ \ufffd _i,j,k,l_ _\u2202vi_ _[r][\u2202v]_ _j_ _[r]_\n\n\ufffd\n\n_\u2202_ [4] 1\n\n_g_ ( _v_ _[r]_ ) + _o_\n_\u2202vi_ _[r][\u2202v]_ _j_ _[r][\u2202v]_ _k_ _[r][\u2202v]_ _l_ _[r]_ \ufffd _r_\n\n_r_\n\n_,_\n\ufffd\n\n= _g_ ( _v_ _[r]_ ) + [1]\n\n_\u03ba_ [4]\n\n\ufffd _i,j,k,l_ _[H][i,j,k,l]_ [(] _[v][r][, \u03c3][r]_ [) +] _[ \u03c1]_ [(] _[v][r]_ [)] _[,]_\n\n_i,j,k,l_\n\n4! _r_ _[g]_ [(] _[v][r]_ [)] \ufffd\n\n24\n\nwhere _\u03c1_ ( _v_ _[r]_ ) denotes the remainder terms.\n\nWe now provide uniform bounds on the remainder term of the expansion using the techniques of (Bhattacharya\n& Ranga Rao, 1976, Theorem 19.2) and analyse convergence rate of the limiting Gaussian distribution to a\nunit Gaussian:\n**Lemma 3.** _Let \u03c3_ 0 [2] = 1 _, let g_ ( _v_ _[r]_ ) _be the limiting Gaussian from Eq._ (14) _, let f_ ( _v_ _[r]_ ) _be a bounded function and_\n_let g\u221e_ ( _v_ _[r]_ ) = _N_ (0 _, Imn_ ) _. Let Assumptions 1 and 2 hold and let v_ _[r]_ = _vec_ ( _Z_ _[r]_ ) _and ui be defined in Eq._ (10) _._\n_Define the Edgeworth remainder as:_\n\n_R_ ( _v_ _[r]_ ) = _p_ ( _v_ _[r]_ ) _\u2212_ _g_ ( _v_ _[r]_ ) _._\n\n_Then:_\n\n1\n_f_ ( _v_ _[r]_ ) _v_ _[r]_ _R_ ( _v_ _[r]_ ) _dv_ _[r]_ = _O_\n\ufffd \ufffd _r_\n\n1\n_f_ ( _v_ _[r]_ ) _v_ _[r]_ _R_ ( _v_ _[r]_ ) _dv_ _[r]_ = _O_ _,_\n\ufffd _r_ \ufffd\n\n1\n_v_ _[r]_ _f_ ( _v_ _[r]_ ) _g_ ( _v_ _[r]_ ) _dv_ _[r]_ = _v_ _[r]_ _f_ ( _v_ _[r]_ ) _g\u221e_ ( _v_ _[r]_ ) _dv_ _[r]_ + _O_\n\ufffd \ufffd \ufffd _r_\n\n_r_\n\n_r_\n\n_._\n\ufffd\n\n_Proof._ We start by bounding \ufffd _f_ ( _v_ _[r]_ ) _v_ _[r]_ _R_ ( _v_ _[r]_ ) _dv_ _[r]_ . From Lemma 2, we have shown that the Edgeworth\nexpansion for _p_ ( _v_ _[r]_ ) is controlled by 4th order cumulants and higher, that is;\n\n_p_ ( _v_ _[r]_ ) = _g_ ( _v_ _[r]_ ) + [1]\n\n\ufffd _\u03ba_ [4] _i,j,k,l_ _[H][i,j,k,l]_ [(] _[v][r][, \u03c3][r]_ [) +] _[ \u03c1]_ [(] _[v][r]_ [)] _[.]_ (18)\n\n_i,j,k,l_\n\n4! _r_ _[g]_ [(] _[v][r]_ [)] \ufffd\n\nWe show that the three assumptions needed to apply Bhattacharya & Ranga Rao (1976, Theorem 20.1) to\nobtain our result using Eq. (18). Firstly, the boundedness assumption of the integrand holds:\n\n_\u2225f_ ( _v_ _[r]_ ) _v_ _[r]_ _\u2225_\nsup _\u2264_ sup\n_v_ _[r]_ 1+ _\u2225v_ _[r]_ _\u2225_ _v_ _[r][ f]_ [(] _[v][r]_ [)] _[ <][ \u221e][.]_\n\nSecondly, the sampling regularity assumption that _ui_ (as defined in Eq. (10)) is zero-mean i.i.d. (satisfied\nunder Assumption 1) with finite 4th order moments (satisfied from Lemma 1) holds. Let _\u03c6U_ ( _\u03c9_ ) denote\nthe characteristic function of _p_ ( _u_ ), then the final assumption we need to verify is Cramer\u2019s condition:\nlim sup _\u2225\u03c9\u2225\u2192\u221e_ _\u03c6U_ ( _\u03c9_ ) _<_ 1, which is satisfied from the Riemann-Lebesgue lemma Folland (1999, Theorem 8.22) because _p_ ( _u_ ) is absolutely continuous under Assumption 1). Our first result thus follows from\napplying Bhattacharya & Ranga Rao (1976, Theorem 20.1):\n\n1\n_f_ ( _v_ _[r]_ ) _v_ _[r]_ _R_ ( _v_ _[r]_ ) _dv_ _[r]_ = _O_ _._\n\ufffd \ufffd _r_ \ufffd\n\n_r_\n\n_._\n\ufffd\n\nWe now derive our second result. Let _d_ := _[mn]_\n\n2 [. As] _[ g][\u221e]_ [(] _[v][r]_ [)][ and] _[ g]_ [(] _[v][r]_ [)][ are both Gaussians:]\n\n1\n_g\u221e_ ( _v_ _[r]_ ) _[\u2212]_ [1] _g_ ( _v_ _[r]_ ) =\n\ufffd _\u03c3r_\n\n_d_ _\u2225vr\u2225_ 2\nexp\n\ufffd \ufffd 2\n\n_r\u2225_ 2 _\u2212_ _[\u2225][v][r][\u2225]_ [2]\n\n2 2 _\u03c3_ [2]\n\n2 _\u03c3r_ [2]\n\n_,_\n\ufffd\n\n_r_\n\n=\n\ufffd _r_ + _\u03c3\u03f5_ [2]\n\n_r_\n\n=\n\ufffd _r_ + _\u03c3\u03f5_ [2]\n\n_r_\n\n=\n\ufffd _r_ + _\u03c3\u03f5_ [2]\n\n_d_ _\u2225vr\u2225_ 2\nexp\n\ufffd \ufffd 2\n\n_d_ 2\n1 + exp _\u03c3\u03f5_ _[\u2225][v][r][\u2225]_ [2]\n\ufffd \ufffd \ufffd 2( _r_ + _\u03c3\u03f5_ [2] )\n\n2 _r\u2225_ 2 _\u2212_ _rr\u2225_ + _v_ _[r]_ _\u03c3\u2225_ [2]\n\n2( _r_ + _\u03c3\u03f5_ [2] )\n\n_,_\n\ufffd\n\n_d_ 2\nexp _\u03c3\u03f5_ _[\u2225][v][r][\u2225]_ [2]\n\ufffd \ufffd 2( _r_ + _\u03c3\u03f5_ [2] )\n\n_d_ 2\nexp _\u03c3\u03f5_ _[\u2225][v][r][\u2225]_ [2]\n\ufffd \ufffd 2( _r_ + _\u03c3\u03f5_ [2]\n\n_,_\n\ufffd\n\n_\u2212_ 1 _,_\n\ufffd \ufffd\n\n2\nwhere we have substituted for _\u03c3r_ [2] = 1 + _[\u03c3]_ _r_ _[\u03f5]_ from Lemma 1, hence:\n\n_v_ _[r]_ _f_ ( _v_ _[r]_ ) _g_ ( _v_ _[r]_ ) _dv_ _[r]_ = _v_ _[r]_ _f_ ( _v_ _[r]_ ) _g\u221e_ ( _v_ _[r]_ ) _g\u221e_ ( _v_ _[r]_ ) _[\u2212]_ [1] _g_ ( _v_ _[r]_ ) _dv_ _[r]_ _,_\n\ufffd \ufffd\n\n_\u2212_ 1 _dv_ _[r]_ _._\n\ufffd \ufffd\n\n_r_\n= _v_ _[r]_ _f_ ( _v_ _[r]_ ) _g\u221e_ ( _v_ _[r]_ )\n\ufffd \ufffd _r_ + _\u03c3\u03f5_ [2]\n\n25\n\n_d_ 2\n1 + exp _\u03c3\u03f5_ _[\u2225][v][r][\u2225]_ [2]\n\ufffd \ufffd \ufffd 2( _r_ + _\u03c3\u03f5_ [2] )\n\n_d_\nExpanding \ufffd _r_ + _r\u03c3\u03f5_ [2] \ufffd using the binomial expansion:\n\n_d_\n\n_r_ _\u03c3\u03f5_ [2]\n\n= 1 _\u2212_\n\n\ufffd _r_ + _\u03c3\u03f5_ [2] \ufffd \ufffd _r_ + _\u03c3\u03f5_ [2]\n\n_d_\n\n_,_\n\ufffd\n\n_d\u03c3\u03f5_ [2] _d\u03c3\u03f5_ 2\n\n_r_ + _\u03c3\u03f5_ [2] [+] _[ o]_ \ufffd _r_ + _\u03c3_\n\n_d\u03c3\u03f5_ [2]\n= 1 _\u2212_\n\n_r_ + _\u03c3\u03f5_ [2]\n\n_,_\n\ufffd\n\n1\n= 1 + _O_\n\ufffd _r_\n\n_,_\n\ufffd\n\nhence:\n\n_vf_ ( _v_ _[r]_ ) _g_ ( _v_ _[r]_ ) _dv_ _[r]_ = _v_ _[r]_ _f_ ( _v_ _[r]_ ) _g\u221e_ ( _v_ _[r]_ ) _g\u221e_ ( _v_ _[r]_ ) _[\u2212]_ [1] _g_ ( _v_ _[r]_ ) _dv_ _[r]_ _,_\n\ufffd \ufffd\n\n2\n_v_ _[r]_ _f_ ( _v_ _[r]_ ) _g\u221e_ ( _v_ _[r]_ ) 1 + exp _\u03c3\u03f5_ _[\u2225][v][r][\u2225]_ [2]\n\ufffd\ufffd\ufffd \ufffd \ufffd 2( _r_ + _\u03c3\u03f5_ [2] )\n\n1\n= 1 + _O_\n\ufffd \ufffd _r_\n\n_\u2212_ 1 _dv_ _[r]_ _._\n\ufffd \ufffd\n\nIt therefore suffices to show:\n\n2\n_v_ _[r]_ _f_ ( _v_ _[r]_ ) _g_ ( _v_ _[r]_ ) exp _\u03c3\u03f5_ _[\u2225][v][r][\u2225]_ [2]\n\ufffd \ufffd \ufffd 2( _r_ + _\u03c3\u03f5_ [2] )\n\nIt therefore suffices to show:\n\n2\n_v_ _[r]_ _f_ ( _v_ _[r]_ ) _g_ ( _v_ _[r]_ ) exp _\u03c3\u03f5_ _[\u2225][v][r][\u2225]_ [2]\n\ufffd \ufffd \ufffd 2( _r_ + _\u03c3\u03f5_ [2]\n\n1\n_\u2212_ 1 _dv_ _[r]_ = _O_\n\ufffd \ufffd \ufffd _r_\n\n_._\n\ufffd\n\nUsing the identity exp( _x_ ) _\u2212_ 1 _\u2264_ _x_ exp( _x_ ) for _x \u2265_ 0:\n\n2\n_v_ _[r]_ _f_ ( _v_ _[r]_ ) _g_ ( _v_ _[r]_ ) exp _\u03c3\u03f5_ _[\u2225][v][r][\u2225]_ [2] _\u2212_\n\ufffd \ufffd \ufffd 2( _r_ + _\u03c3\u03f5_ [2] ) \ufffd\n\n\ufffd\ufffd\ufffd\ufffd\ufffd\n\n2\n_v_ _[r]_ _f_ ( _v_ _[r]_ ) _g_ ( _v_ _[r]_ ) exp _\u03c3\u03f5_ _[\u2225][v][r][\u2225]_ [2]\n\ufffd \ufffd \ufffd 2( _r_ + _\u03c3\u03f5_ [2]\n\n2( _r_ + _\u03c3\u03f5_ [2] )\n\n_\u2212_ 1 _dv_ _[r]_\n\ufffd \ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\n\n_\u03f5_ _[\u2225][v][r][\u2225]_ [2]\n_\u2264_ _|f_ ( _v_ _[r]_ ) _|\u2225v\u2225g\u221e_ ( _v_ _[r]_ ) _[\u03c3]_ [2]\n\ufffd 2( _r_ + _\u03c3_ [2]\n\n2( _r_ + _\u03c3\u03f5_ [2] )\n\n2\n\n_\u03f5_ _[\u2225][v][r][\u2225]_ [2] _\u03c3\u03f5_ _[\u2225][v][r][\u2225]_ [2]\n\n_[\u03c3]_ [2]\n\n2( _r_ + _\u03c3\u03f5_ [2] ) [exp] \ufffd 2( _r_ + _\u03c3\u03f5_ [2]\n\n_dv_ _[r]_\n\ufffd\n\n= _g\u221e_ ( _v_ _[r]_ ) _[\u03c3]_ _\u03f5_ [2] _[|][f]_ [(] _[v][r]_ [)] _[|\u2225][v][r][\u2225]_ [3]\n\ufffd 2( _r_ + _\u03c3_ [2] )\n\n2\n\n_[f]_ [(] _[v][r]_ [)] _[|\u2225][v][r][\u2225]_ [3] exp _\u03c3\u03f5_ _[\u2225][v][r][\u2225]_ [2]\n\n2( _r_ + _\u03c3\u03f5_ [2] ) \ufffd 2( _r_ + _\u03c3\u03f5_ [2]\n\n2( _r_ + _\u03c3\u03f5_ [2] )\n\n_dv_ _[r]_\n\ufffd\n\n= 1 _\u03c3\u03f5_ [2] _[|][f]_ [(] _[v][r]_ [)] _[|\u2225][v][r][\u2225]_ [3]\n\ufffd (2 _\u03c0_ ) _[d]_ 2( _r_ + _\u03c3\u03f5_ [2] )\n\n_[f]_ [(] _[v][r]_ [)] _[|\u2225][v][r][\u2225]_ [3] _\u2212_ _[\u2225][v][r][\u2225]_ [2]\n\nexp\n2( _r_ + _\u03c3\u03f5_ [2] ) \ufffd 2\n\n_[r][\u2225]_ [2] _\u03f5_ _[\u2225][v][r][\u2225]_ [2]\n\n+ _[\u03c3]_ [2]\n2 _r_ + _\u03c3_ [2]\n\n2( _r_ + _\u03c3\u03f5_ [2] )\n\n_,_\n\ufffd\n\n_\u03f5_\n_\u2264_ sup [(][2] _[\u03c0]_ [)] _[d][\u03c3]_ [2]\n_v_ _[|][f]_ [(] _[v][r]_ [)] _[|]_ 2( _r_ + _\u03c3\u03f5_ [2] )\n\n_\u2225v_ _[r]_ _\u2225_ [3] exp _\u2212_ _[\u2225][v][r][\u2225]_ [2]\n\ufffd \ufffd 2\n\n_r_\n\ufffd _r_ + _\u03c3\u03f5_ [2]\n\n_dv_ _[r]_\n\ufffd\ufffd\n\n_,_\n\n\ufffd ~~\ufffd~~ \ufffd ~~\ufffd~~\n= _O_ (1)\n\n1\n= _O_\n\ufffd _r_\n\n_,_\n\ufffd\n\nwhere our final line follows from the fact that fixed polynomials are integrable under the Gaussian distribution\nand _f_ ( _v_ _[r]_ ) is bounded by assumption, hence:\n\n1 1\n_vf_ ( _v_ _[r]_ ) _g_ ( _v_ _[r]_ ) _dv_ _[r]_ = 1 + _O_ _v_ _[r]_ _f_ ( _v_ _[r]_ ) _g\u221e_ ( _v_ _[r]_ ) _dv_ _[r]_ + _O_ _,_\n\ufffd \ufffd \ufffd _r_ \ufffd\ufffd\ufffd\ufffd \ufffd _r_ \ufffd\ufffd\n\n_r_\n\n1\n_v_ _[r]_ _f_ ( _v_ _[r]_ ) _g\u221e_ ( _v_ _[r]_ ) _dv_ _[r]_ + _O_\n\ufffd\ufffd\ufffd\ufffd \ufffd _r_\n\n_r_\n\n_,_\n\ufffd\ufffd\n\n1\n= _v_ _[r]_ _f_ ( _v_ _[r]_ ) _g\u221e_ ( _v_ _[r]_ ) _dv_ _[r]_ + _O_\n\ufffd \ufffd _r_\n\n_,_\n\ufffd\n\nas required.\n\nUsing Lemma 3, we have all ingredients needed derive our main about the convergence result, which follows\nafter some algebra on the norm:\n**Theorem 2.** _Let Assumptions 1 and 2 hold and set \u03c3_ 0 = 1 _, then:_\n\n\u02c6 1\n_\u2225g_ True _\u2212_ _g_ LR _[r]_ _[\u2225][F]_ [=] _[ O]_\n\ufffd _r_\n\n26\n\n_._\n\ufffd\n\n_Proof._ We start by converting the norm to vector form using Eq. (11):\n\n_\u03c3_ ( _g_ True _\u2212_ _g_ LR _[r]_ [) =][ E] _[E]_ [[] _[E][ \u00b7][ f]_ [(] _[M]_ [ =] _[ \u00b5]_ [ +] _[ \u03c3E]_ [)]] _[ \u2212]_ [E] _[Z][r]_ [ [] _[Z]_ _[r][f]_ [(] _[M]_ [ =] _[ \u00b5]_ [ +] _[ \u03c3Z]_ [)]] _[,]_\n\n= E _v_ [ _vf_ ( _\u00b5, v_ )] _\u2212_ E _vr_ [ _v_ _[r]_ _f_ ( _\u00b5, v_ _[r]_ )] _,_ (19)\n\nwhere _f_ ( _\u00b5, v_ ) := _f_ ( _M_ = _\u00b5_ + _\u03c3_ mat( _v_ )) and _v_ = vec( _E_ ) is the vectorisation of variable _E_, which is distributed\nas _v \u223c_ _g\u221e_ ( _v_ ) =: _N_ (0 _, Imn_ ). Let _R_ ( _v_ _[r]_ ) denote the difference between _p_ ( _v_ _[r]_ ) and the limiting Gaussian\n\n1 _\u2212_ _[\u2225][v][r][\u2225]_ [2]\n\n(2 _\u03c0_ ) _[mn]_ _\u03c3r_ [2] [exp] \ufffd 2 _\u03c3r_ [2]\n\n:\n\ufffd\n\ndistribution _g_ ( _v_ _[r]_ ) := 1\n~~_\u221a_~~ (2 _\u03c0_ )\n\n2 _\u03c3r_ [2]\n\n_R_ ( _v_ _[r]_ ) := _p_ ( _v_ _[r]_ ) _\u2212_ _g_ ( _v_ _[r]_ ) _._\n\nUsing this notation:\n\nE _vr_ [ _v_ _[r]_ _f_ ( _\u00b5, v_ _[r]_ )] = _p_ ( _v_ _[r]_ ) _v_ _[r]_ _f_ ( _\u00b5, v_ _[r]_ ) _dv_ _[r]_ _,_\n\ufffd\n\n= ( _g_ ( _v_ _[r]_ ) + _R_ ( _v_ _[r]_ )) _v_ _[r]_ _f_ ( _\u00b5, v_ _[r]_ ) _dv_ _[r]_ _,_\n\ufffd\n\n= _g_ ( _v_ _[r]_ ) _v_ _[r]_ _f_ ( _\u00b5, v_ _[r]_ ) _dv_ _[r]_ + _R_ ( _v_ _[r]_ ) _v_ _[r]_ _f_ ( _\u00b5, v_ _[r]_ ) _dv_ _[r]_ _,_\n\ufffd \ufffd\n\nwhere _g_ ( _v_ _[r]_ ) is the limiting Gaussian density from Eq. (14) and _R_ ( _v_ _[r]_ ) is the Edgeworth remainder term as\ndefined in Lemma 2. Now, applying Lemma 3 to both integrals:\n\n1\nE _v_ _[r]_ [ _v_ _[r]_ _f_ ( _\u00b5, v_ _[r]_ )] = _g\u221e_ ( _v_ _[r]_ ) _v_ _[r]_ _f_ ( _\u00b5, v_ _[r]_ ) _dv_ _[r]_ + _O_\n\ufffd \ufffd _r_\n\n1\n=E _v_ [ _vf_ ( _\u00b5, v_ )] + _O_ _._\n\ufffd _r_ \ufffd\n\nSubstituting into Eq. (19) yields our desired result:\n\n_,_\n\ufffd\n\n1\n_\u03c3_ ( _g_ True _\u2212_ _g_ LR _[r]_ [) =] _[ O]_\n\ufffd _r_\n\n_,_\n\ufffd\n\n_._\n\ufffd\n\n\u02c6 1\n= _\u21d2\u2225g_ True _\u2212_ _g_ LR _[r]_ _[\u2225][F]_ [=] _[ O]_\n\ufffd _r_\n\n**B** **Mean Field Score Function Approximator**\n\n**B.1** **Bessel Functions**\n\nWe will use _n_ th order Bessel functions of the second kind _Kn_ ( _z_ ) (Basset, 1892; Macdonald, 1899; Watson,\n1944), which are conveniently represented by the integral equations:\n\n_\u221e_\n_Kn_ ( _z_ ) = exp( _\u2212z_ cosh _\u03b8_ ) cosh( _n\u03b8_ ) _d\u03b8._\n\ufffd0\n\nBessel functions are the solutions to systems of differential equations that occur naturally in phenomena\nwhere there is strong radial symmetry, typically involving the propagation of spherical waves from points\nlike the ripples formed from water droplets (Whitham, 1999). For our setting, Bessel functions describe the\nprobability density of the product of rotationally invariant random variables, whose solution is analogous to\nthe interference pattern of two spherical wave propagators.\n\nUsing the representation, we find the derivative of the zeroth order function takes the recursive form:\n\n_dK_ 0( _z_ )\n\nexp( _\u2212z_ cosh _\u03b8_ ) cosh( _\u03b8_ ) _d\u03b8_ = _\u2212K_ 1( _z_ ) _._ (20)\n0\n\n_\u221e_\n_dz_ 0( _z_ ) = _\u2212_ \ufffd0\n\nMore generally, the derivative of the _n_ th order Bessel function is Watson (1944, Section 3.71, Eq. 4):\n\n_dKn_ ( _z_ )\n\n_z_ _[K][n]_ [(] _[z]_ [)] _[ \u2212]_ _[K][n]_ [+1][(] _[z]_ [)] _[.]_ (21)\n\n_n_ _z_\n\n= _[n]_\n_dz_ _z_\n\n27\n\n**B.2** **Derivation of Mean-field Approximators**\n\nTo derive a mena-field approximation, we assume that the elements of _A_ and _B_ are drawn independently from\nthe set of generalised Gaussian distributions (GGDs):\n**Assumption 3.** _Assume each element ai,j \u223cGG_ ( _s, p_ ) _and bi,j \u223cGG_ ( _s, p_ ) _of A and B be independently_\n_distributed according to the zero-mean generalised Gaussian distribution GG_ ( _s, p_ ) _with density:_\n\n_p_ exp _\u2212_ _x_\n\n2 _s_ \u0393 1 \ufffd \ufffd\ufffd\ufffd _s_\n~~\ufffd~~ _p_ ~~\ufffd~~\n\n_p_\n_GG_ ( _x|s, p_ ) =\n\n_s_\n\n_p_ [\ufffd]\n\n_,_\n\ufffd\ufffd\ufffd\n\n_where_ 0 _< s < \u221e_ _is the scale parameter, p >_ 0 _the shape parameter and_ \u0393( _\u00b7_ ) _is the gamma function._\n\nWe observe common distributions emerge from the set of GGDs including the Laplace for _p_ = 1, the Gaussian\nfor _p_ = 2 and the uniform over [ _\u2212s,_ + _s_ ] in the limit _p \u2192\u221e_ .\n\nIf we make the assumption that all elements of _Z_ are independent (this is true as _r_ grows) then we can write\n_p_ ( _Z_ _[r]_ ) _\u2248_ _p_ \u02c6( _Z_ ) := [\ufffd] _[m]_ _i_ =1 \ufffd _nj_ =1 _[p]_ [(] _[z][i,j]_ [)][ as the product of the marginal distributions. Under this approximation,]\nthe score function can be defined element-wise as:\n\n[ _\u2207Z_ log _p_ ( _Z_ _[r]_ )] _i,j \u2248_ _S_ [\u02c6] ( _zi,j_ ) := _\u2202zi,j_ log _p_ ( _zi,j_ ) _._\n\nUsing this approximation we apply the score function _S_ [\u02c6] ( _\u00b7_ ) element-wise to the matrix _Z_ :\n\n\u02c6\n_g_ LR _\u2248_ _g_ MF := _\u2212\u03c3_ E _Z\u223cp_ ( _Zr_ ) \ufffd _f_ ( _y_ = ( _\u00b5_ + _\u03c3Z_ ) _x_ ) _\u00b7_ _S_ [\u02c6] _\u2299_ ( _Z_ )\ufffd _._\n\nFor _r_ = 1, _S_ [\u02c6] ( _\u00b7_ ) has a convenient analytic form for all members of the set of GGDs with _\u03c3\u03f5 \u2248_ 0:\n**Theorem 3.** _Let Assumption 3 hold with \u03c3\u03f5_ = 0 _and r_ = 1 _. Then the distribution over marginals p_ ( _zi,j_ ) _is:_\n\n_p_\n\n_p_ 2 _|zi,j|_ 2\n_p_ ( _zi,j_ ) = 2 _[K]_ 0\n_s_ \u0393 1 \ufffd _s_ _[p]_\n~~\ufffd~~ ~~\ufffd~~ _p_ ~~\ufffd\ufffd~~\n\n\ufffd\n\n_,_ (22)\n\n_where K_ 0 ( _\u00b7_ ) _is the zeroth-order modified Bessel function of the second kind and the marginal score function is_\n_defined element-wise as:_\n\n\ufffd\n\n_p_ _p_\n\n_\u00b7_ _[p]_ 2 _|zi,j|_ 2 _[\u2212]_ [1] sign( _zi,j_ )\n~~\ufffd~~ _s_ _[p]_\n\n_K_ 1\n\n\u02c6\n_S_ ( _zi,j_ ) = _\u2212_\n\n_K_ 0\n\n_p_\n2 _|zi,j_ _|_ 2\n\n_s_ _[p]_\n\ufffd\n\n_p_\n2 _|zi,j_ _|_ 2\n\n_s_ _[p]_\n~~\ufffd~~\n\n_._\n_s_ _[p]_\n\n_Proof._ For _r_ = 1, we denote the elements of vector _A_ as _ai_ and elements of vector _B_ as _bj_, then the elements\nof matrix _Z_ = _AB_ _[\u22a4]_ are: _zi,j_ = _aibj_ . We now derive the distribution the unnormalised variables: _zi,j_ using\nthe formula for the distribution of the product of two independent random variables (Rohatgi, 1976; Grimmett\n& Stirzaker, 1993):\n\n_\u221e_\n\n_p_ ( _ai_ ) _p_ _bj_ = _[z][i][,j]_\n_\u2212\u221e_ \ufffd _ai_\n\n_\u221e_\n_p_ ( _zi,j_ ) =\n\ufffd\n\n_ai_\n\n1\n\ufffd _|ai|_ _[da][i][,]_\n\n2\n\n_\u221e_\n\ufffd\n\n\ufffd\ufffd\ufffd\ufffd\n\n_p_ [\ufffd] 1\n\n_|ai|_ _[da][i][,]_\n\n_p_ [\ufffd] 1\n\n_|a_\n\n=\n\n\uf8eb\n\n_p_\n\n2 _s_ \u0393\n\n\uf8ed\n\n2 _s_ \u0393 1\n~~\ufffd~~ _p_ ~~\ufffd~~\n\n\uf8f6\n\n\uf8f8\n\nexp _\u2212_ _ai_\n_\u2212\u221e_ \ufffd \ufffd\ufffd\ufffd _s_\n\n_s_\n\n_p_ [\ufffd] exp _\u2212_ _zi,j_\n\ufffd\ufffd\ufffd \ufffd \ufffd\ufffd\ufffd\ufffd _ais_\n\n_p_ [\ufffd] 1\n\n_|ai|_ _[da][i][,]_\n\n_p_ [\ufffd] 1\n\n_|a_\n\n=2\n\n\uf8eb\n\n_p_\n\n2 _s_ \u0393\n\n\uf8ed\n\n2 _s_ \u0393 1\n~~\ufffd~~ _p_ ~~\ufffd~~\n\n\uf8f6\n\n\uf8f8\n\n2\n\n_\u221e_\n\ufffd0\n\nexp _\u2212_ _ai_\n0 \ufffd \ufffd\ufffd\ufffd _s_\n\n_s_\n\n_p_ [\ufffd] exp _\u2212_ _zi,j_\n\ufffd\ufffd\ufffd \ufffd \ufffd\ufffd\ufffd\ufffd _ais_\n\n\ufffd\ufffd\ufffd\ufffd\n\n28\n\nwhere we have used symmetry of the integrand about 0 to derive the final line. Now, making the substitution\n_x_ = \ufffd _asi_ \ufffd _p_, we have:\n\n1\n\n_dai_ _p_ _[\u2212]_ [1]\n\n_dx_ [=] _[ sx]_\n\n_dai_\n\n_p_ 1\n\n_,_ _ai_ = _sx_ _p_\n_p_\n\nhence:\n\n1\n\ufffd _x_ _[dx.]_\n\n_|zi,j|_ _[p]_\n\n[1]\n\n_x_ _s_ [2] _[p]_\n\n_p_ 1\n_p_ ( _zi,j_ ) = _s_ \u0393 1 2 2\n~~\ufffd~~ ~~\ufffd~~ _p_ ~~\ufffd\ufffd~~\n\n\ufffd0 _\u221e_\n\n_\u221e_\n\nexp _\u2212x \u2212_ [1]\n0 \ufffd _x_\n\n_s_ [2] _[p]_\n\nNow, we use the identity (Temme, 1996, Theorem 9.42):\n\n1\n\ufffd _x_ _[dx,]_\n\n1\n\ufffd _x_\n\n_K_ 0( _x_ ) = [1]\n\n2\n\n\ufffd0 _\u221e_\n\n_\u221e_\n\nexp _\u2212x \u2212_ _[z]_ [2]\n0 \ufffd 4 _x_\n\n4 _x_\n\n_p_\n2\nwith _z_ = [2] _[|][z]_ _s_ _[i][,j][p]_ _[|]_ to yield:\n\n_p_\n\n_p_ 2 _|zi,j|_ 2\n_p_ ( _zi,j_ ) = 2 _[K]_ 0\n_s_ \u0393 1 \ufffd _s_ _[p]_\n~~\ufffd~~ ~~\ufffd~~ _p_ ~~\ufffd\ufffd~~\n\n\ufffd\n\n_,_\n\nas required for Eq. (22). Now we derive the marginal score function by applying the chain rule:\n\n\ufffd\n\n_,_\n\n\ufffd\n\n_\u2202zi,j_ log _p_ ( _zi,j_ ) = _\u2202zi,j_ log _K_ 0\n\n_p_\n2 _|zi,j|_ 2\n\n_s_ _[p]_\n\ufffd\n\n= _\u2202z_ log _K_ 0\n\n= _\u2202z_ log _K_ 0\n\n_p_\n2\n_z_ = [2] _[|][z][i][,j][|]_\n_s_ _[p]_\n\n\ufffd\n\n_p_\n2\n_z_ = [2] _[|][z][i][,j][|]_\n_s_ _[p]_\n\n\ufffd\n\n_p_\n_p|zi,j|_ 2 _[\u2212]_ [1] sign( _zi,j_ )\n\n_,_\n_s_ _[p]_\n\n\ufffd\n\n_p_\n_\u2202zi,j_ 2 _|zi,j|_ 2 _,_\n_s_ _[p]_\n\n_p_\n2\n_z_ = [2] _[|][z][i][,j]_ _[|]_\n_s_ _[p]_\n\ufffd\n\n\ufffd\n\n_\u2202zK_ 0\n\n=\n\n_p_\n\n_K_ 0 _z_ = [2] _[|][z]_ _s_ _[i][,j][p]_ _[|]_ 2\n\n~~\ufffd~~\n\n_p_\n\n_\u00b7_ _[p][|][z][i][,j][|]_ 2 _[\u2212]_ [1] sign( _zi,j_ )\n~~\ufffd~~ _s_ _[p]_\n\n_,_\n_s_ _[p]_\n\n_p_\n\n_K_ 1 2 _|zsi,j_ _[p]_ _|_ 2\n= _\u2212_ \ufffd _p_\n\n_K_ 0 2 _|zsi,j_ _[p]_ _|_ 2\n\n~~\ufffd~~\n\n\ufffd\n\n_p_\n\n_\u00b7_ _[p][|][z][i][,j][|]_ 2 _[\u2212]_ [1] sign( _zi,j_ )\n~~\ufffd~~ _s_ _[p]_\n\n_,_\n_s_ _[p]_\n\nwhere we have used the identity _\u2202zK_ 0( _x_ ) = _\u2212K_ 1( _x_ ) from Eq. (20)\n\nFor _r >_ 1 we can derive _S_ [\u02c6] ( _\u00b7_ ) for the Gaussian sampling case:\n**Theorem 4.** _Let Assumption 3 hold with \u03c3\u03f5_ = 0 _and p_ = 2 _. Then the distribution over marginals p_ ( _zi,j_ ) _is:_\n\n_r\u2212_ 1\n\n2\n\n_p_ ( _zi,j_ ) = [2] _[\u221a]_ ~~_[r]_~~ _[|\u221a]_ ~~_[r]_~~ _[z][i][,j][|]_ _r_\n\n2\n\n_._\n\ufffd\n\n2\n\n_r_ _\u00b7 K r\u2212_ 2 1\n\n2 ~~\ufffd~~\n\n2 _|\u221a_ ~~_r_~~ _zi,j|_\n\ufffd _s_ [2]\n\n_s_ _[r]_ [+1] ~~_[\u221a]_~~ ~~_\u03c0_~~ \u0393 _r_\n~~\ufffd~~ 2\n\n_and the score function is:_\n\n2 _|_ ~~_[\u221a]_~~ _rzi,j_ _|_\n\ufffd _s_ [2] \ufffd\n\n_[ \u2212]_ [1]\n\n_\u2212_ [2] _[\u221a]_ ~~_[r]_~~ [si][g][n][(] _[z][i][,j]_ [)]\n_zi,j_ _s_ [2]\n\n_K r\u2212_ 1\n\n_S_ \u02c6( _zi,j_ ) = _[r][ \u2212]_ [1]\n\n[g][n][(] _[z][i][,j]_ [)] _K_ _r_ +2 1\n\n_s_ [2]\n\n2\n\n2 _|_ ~~_[\u221a]_~~ _rzi,j_ _|_ _, ._\n~~\ufffd~~ _s_ [2] ~~\ufffd~~\n\n2\n\n29\n\n_Proof._ Let _ui,j_ = ~~_\u221a_~~ 1 _r_ _zi,j_ . Using the transformation of variables, we derive the density for _zi,j_ from _p_ ( _ui,j_ )\nusing: _p_ ( _zi,j_ ) = _[\u221a]_ ~~_r_~~ _p_ ( _ui,j_ = _[\u221a]_ ~~_r_~~ _zi,j_ ). Each element _ui,j_ is the sum of _r_ independent variables _ui,j,l_ := _ai,lbj,l_\ndistributed according to Eq. (22) with _p_ = 2:\n\n_r_\n\ufffd _ui,j,l_\n\n_l_ =1\n\n_ui,j_ =\n\n_r_ 1\n\ufffd _l_ =1 _ai,lbj,l_ = ~~_\u221ar_~~\n\nFrom Eq. (22), the distribution of each _ui,j,l_ is:\n\n2\n_p_ ( _ui,j,l_ ) =\n_s_ [2] _\u03c0_ _[K]_ [0]\n\n2 _|ui,j,l|_\n\ufffd _s_ [2]\n\n\ufffd\n\nWe use the fact that the PDF of a sum of _r_ independent random variables is given by the _r_ -fold convolution of\nthe individual PDFs. As convolution in the spatial domain is equal to multiplication in the frequency domain,\nthe PDF _p_ ( _ui,j_ ) follows by taking Fourier transform of _p_ ( _ui,j,l_ ), taking the power _r_ and then taking the inverse\nFourier transform:\n\n2 _|\u00b7|_\n\ufffd _s_ [2]\n\n_r_\n( _ui,j_ ) _,_\n\ufffd\ufffd \ufffd\n\n2\n_p_ ( _ui,j_ ) =\n\ufffd _s_ [2] _\u03c0_\n\n2\n_p_ ( _ui,j_ ) =\n\ufffd _s_ [2]\n\n_r_\n_F_ _[\u2212]_ [1] _F_ _K_ 0\n\ufffd \ufffd \ufffd\n\nwhere _F_ [ _f_ ( _\u00b7_ )]( _\u03c9_ ) and _F_ _[\u2212]_ [1] [ _f_ [\u02c6] ( _\u00b7_ )]( _x_ ) denotes the Fourier and inverse Fourier transforms. Taking the Fourier\ntransform of the Bessel function:\n\n_F_ _K_ 0\n\ufffd\n\n2 _|\u00b7|_\n\ufffd _s_ [2]\n\n( _\u03c9_ ) = exp( _\u2212i\u03c9_ _[\u22a4]_ _x_ ) _K_ 0\n\ufffd\ufffd \ufffd\n\n2 _|x|_\n\ufffd _s_ [2]\n\n_dx,_\n\ufffd\n\n2 _|x|_\n\ufffd _s_ [2]\n\n_dx,_\n\ufffd\n\n= cos( _\u03c9x_ ) _K_ 0\n\ufffd\n\n= cos( _\u03c9x_ ) _K_ 0\n\ufffd\n\n2 _|x|_\n\n_dx \u2212_ _i_ sin( _\u03c9x_ ) _K_ 0\n\n\ufffd _s_ [2] \ufffd \ufffd\n\n2 _|x|_\n\n_dx,_\n\n\ufffd _s_ [2] \ufffd\n\n_dx,_ (23)\n\ufffd\n\n_\u221e_\n= 2 cos( _\u03c9x_ ) _K_ 0\n\ufffd0\n\n2 _x_\n\ufffd _s_ [2]\n\n2 _|x|_\nwhere we have used the fact thatsecond line is zero. Using a standard result, we can evaluate the integral in Eq. ( _K_ 0 \ufffd _s_ [2] \ufffd is an even function of _x_ and so its integral with23) Gradshte sin(\u02d8\u0131n et al. _\u03c9x_ ) ( in the2015,\n\n6.671 Integral 14):\n\n_F_ _K_ 0\n\ufffd\n\n2 _|\u00b7|_\n\ufffd _s_ [2]\n\n_\u03c0_\n( _\u03c9_ ) =\n\ufffd\ufffd _\u03c9_ [2] +\n~~\ufffd~~\n\n2\n_\u03c9_ [2] + ~~\ufffd~~ _s_\n\n_s_ 2 [2] ~~\ufffd~~ 2 _[,]_\n\n30\n\nhence:\n\n_s_ 2 [2] ~~\ufffd~~ 2 ~~[\ufffd]~~ ~~_[r]_~~ 2\n\n2\n_p_ ( _ui,j_ ) =\n\ufffd _s_ [2] _\u03c0_\n\n_r_\n_F_ _[\u2212]_ [1]\n\ufffd\n\n\uf8ee\n\n_\u03c0_ _[r]_\n\n2\n\uf8ef\uf8f0 ~~\ufffd~~ _\u03c9_ [2] + ~~\ufffd~~ _s_ [2]\n\n\uf8f9\n\n\uf8fa ( _ui,j_ ) _,_\n\uf8fb\n\n\uf8f9\n\n2\n\n\uf8fb ( _ui,j_ ) _,_\n\n2 [\ufffd] _[\u2212]_ _[r]_ 2\n\ufffd\n\n_r_\n_F_ _[\u2212]_ [1]\n\ufffd\n\n\uf8ee\n\n\uf8f0\n\n\ufffd\n\n_[r]_\n\n2 [\uf8f9]\n\n2\n\n=\n\ufffd _s_ [2]\n\n2\n\n=\n\ufffd _s_ [2]\n\n2\n_\u03c9_ [2] +\n\ufffd _s_ [2]\n\n_s_ [2]\n\n2 [\ufffd] _[\u2212]_ _[r]_ 2\n\n_d\u03c9,_\n\n\ufffd\n\n2 [\ufffd] _[\u2212]_ _[r]_ 2\n\ufffd\n\n_r_ 1\n\ufffd 2 _\u03c0_\n\n\ufffd\n\n2\n\n=\n\ufffd _s_ [2]\n\n2\n\n=\n\ufffd _s_ [2]\n\nexp( _i\u03c9ui,j_ )\n\ufffd\n\n2\n_\u03c9_ [2] +\n\ufffd _s_ [2]\n\n_s_ [2]\n\n_r_ 1\n\ufffd 2 _\u03c0_\n\ncos( _\u03c9ui,j_ )\n\ufffd\ufffd\n\n\ufffd\n\n2\n_\u03c9_ [2] +\n\ufffd _s_ [2]\n\n2\n\n=\n\ufffd _s_ [2]\n\n2\n\n=\n\ufffd _s_ [2]\n\n_s_ [2]\n\n2 [\ufffd] _[\u2212]_ _[r]_ 2\n\n_d\u03c9_\n\n\ufffd\n\n\ufffd\n\n2 [\ufffd] _[\u2212]_ _[r]_ 2\n\n_d\u03c9_\n\n\ufffd\n\n2 [\ufffd] _[\u2212]_ _[r]_ 2\n\ufffd\n\n+ _i_ sin( _\u03c9ui,j_ )\n\ufffd\n\n2\n_\u03c9_ [2] +\n\ufffd _s_ [2]\n\n_s_ [2]\n\n_,_\n\n\ufffd\n\n\ufffd\n\n2 [\ufffd] _[\u2212]_ _[r]_ 2\n\n_d\u03c9,_\n\n\ufffd\n\n2 [\ufffd] _[\u2212]_ _[r]_ 2\n\ufffd\n\n_r_ 1\n\ufffd 2 _\u03c0_\n\n2\n\n=\n\ufffd _s_ [2]\n\n2\n\n=\n\ufffd _s_ [2]\n\ncos( _\u03c9ui,j_ )\n\ufffd\n\n\ufffd\n\n2\n_\u03c9_ [2] +\n\ufffd _s_ [2]\n\n_s_ [2]\n\n_r_ 1\n\ufffd _\u03c0_\n\n2\n\n=\n\ufffd _s_ [2]\n\n2\n\n=\n\ufffd _s_ [2]\n\n2 [\ufffd] _[\u2212]_ _[r]_ 2\n\n_d\u03c9,_ (24)\n\n\ufffd\n\n2 [\ufffd] _[\u2212]_ _[r]_ 2\n\ufffd\n\n_\u221e_\n\ncos( _\u03c9ui,j_ )\n\n\ufffd0\n\n\ufffd0 _\u221e_\n\n\ufffd\n\n2\n_\u03c9_ [2] +\n\ufffd _s_ [2]\n\n_s_ [2]\n\nwhere we have used the fact that the integrand is an even function and so its integral with sin( _\u03c9zi,j_ ) is zero\nto derive the penultimate line. To evaluate the integral in Eq. (24) we apply Gradshte\u02d8\u0131n et al. (2015, 3.771\nIntegral 2):\n\n_r_ 1\n\n_\u00b7_\n\ufffd ~~_\u221a\u03c0_~~ \u0393 ~~\ufffd~~ 2 _r_\n\n2\n_p_ ( _ui,j_ ) =\n\ufffd _s_ [2]\n\n2\n_p_ ( _ui,j_ ) =\n\ufffd _s_ [2]\n\n2\n_s_ _|ui,j|_\n2 _r_ ~~\ufffd~~ \ufffd 4\n\n2\n\n_\u00b7 K r\u2212_ 1\n\n2 _|ui,j|_\n\n_,_\n\n\ufffd _s_ [2] \ufffd\n\n4\n\n\ufffd _[r][\u2212]_ 2 [1]\n\n2\n\n_r\u2212_ 1\n= 2 _|ui,j|_ 2\n\n_r_ _\u00b7 K r\u2212_ 2 1\n\n2 ~~\ufffd~~\n\n2 _|ui,j|_\n\n_._\n\n\ufffd _s_ [2] \ufffd\n\n_s_ _[r]_ [+1] ~~_[\u221a]_~~ ~~_\u03c0_~~ \u0393 _r_\n~~\ufffd~~ 2\n\n2\n\nUsing the transformation of variables yields our desired results:\n\n_p_ ( _zi,j_ ) = _[\u221a]_ _rp_ ( _ui,j_ = _[\u221a]_ _rzi,j_ ) _,_\n\n_r\u2212_ 1\n\n2\n\n= [2] _[\u221a]_ ~~_[r]_~~ _[|\u221a]_ ~~_[r]_~~ _[z][i][,j][|]_\n\n2\n\n_r_ _\u00b7 K r\u2212_ 2 1\n\n2 ~~\ufffd~~\n\n2 _|\u221a_ ~~_r_~~ _zi,j|_\n\n_._\n\n\ufffd _s_ [2] \ufffd\n\n_s_ _[r]_ [+1] ~~_[\u221a]_~~ ~~_\u03c0_~~ \u0393 _r_\n~~\ufffd~~ 2\n\n2\n\nNow, we derive the score function:\n\n_,_\n\ufffd\n\n_\u2202zi,j_ log _p_ ( _zi,j_ ) = _[r][ \u2212]_ [1]\n\n2\n\n2 _\u00b7 \u2202zi,j_ log _|_ _[\u221a]_ _rzi,j|_ + _\u2202zi,j_ log _K r\u2212_ 2 1\n\n2 _|\u221a_ ~~_r_~~ _zi,j|_\n\ufffd _s_ [2]\n\n_x_ = [2] _[|]_ ~~_[\u221a]_~~ _[rz][i][,j]_ _[|]_\n\ufffd _s_ [2] \ufffd\n\n_[ \u2212]_ [1]\n\n+ [2] _[\u2202][z][i,j]_ _[|\u221a]_ ~~_[r]_~~ _[z][i,j][|]_\n2 _zi,j_ _s_ [2]\n\n_K r\u2212_ 1\n\n= _[r][ \u2212]_ [1]\n\n_\u2202xK r\u2212_ 1\n\n_[|\u221a]_ ~~_[r]_~~ _[z][i,j][|]_ 2\n\n_s_ [2]\n\n2\n\n2 _|_ ~~_[\u221a]_~~ _rzi,j_ _|_ _,_\n~~\ufffd~~ _s_ [2] ~~\ufffd~~\n\n2\n\n_x_ = [2] _[|]_ ~~_[\u221a]_~~ _[rz][i][,j]_ _[|]_\n\ufffd _s_ [2] \ufffd\n\n_[ \u2212]_ [1]\n\n+ [2] _[\u221a]_ ~~_[r]_~~ [si][g][n][(] _[z][i][,j]_ [)]\n2 _zi,j_ _s_ [2]\n\n_K r\u2212_ 1\n\n= _[r][ \u2212]_ [1]\n\n[g][n][(] _[z][i][,j]_ [)] _\u2202xK r\u2212_ 2 1\n\n_s_ [2]\n\n2\n\n2 _|_ ~~_[\u221a]_~~ _rzi,j_ _|_ _,_\n~~\ufffd~~ _s_ [2] ~~\ufffd~~\n\n2\n\n31\n\nNow, from Eq. (21):\n\n_[\u2212]_ [1]\n\n2 [(] _[x]_ [)] _[ \u2212]_ _[K][ r]_ [+] 2 [1]\n\n_\u2202xK r\u2212_ 1\n\n2 [(] _[x]_ [)]\n\n_r\u2212_ 1\n\n_r\u2212_\n\n2 _x_ _[K][ r][\u2212]_ [1]\n\n[+][1]\n\n2 [(] _[x]_ [)]\n\n=\n\n_K r\u2212_ 1\n\n2 [(] _[x]_ [)]\n\n_,_\n\n_K r\u2212_ 1\n\n2 [(] _[x]_ [)]\n\n_[ \u2212]_ [1] _\u2212_ _K_ _r_ +2 1\n\n2 _x_ _K r\u2212_ 1\n\n= _[r][ \u2212]_ [1]\n\n+1\n\n2 [(] _[x]_ [)]\n\n_\u2212_ 1\n\n2 [(] _[x]_ [)] _[,]_\n\n_K r\u2212_ 1\n\n2 _|_ ~~_[\u221a]_~~ _rzi,j_ _|_\n\ufffd _s_ [2] \ufffd\n\n_K r\u2212_ 1\n\n_[ \u2212]_ [1]\n\n+ [(] _[r][ \u2212]_ [1][)][si][g][n][(] _[z][i][,j]_ [)]\n2 _zi,j_ 2 _|zi,j|_\n\n[g][n][(] _[z][i][,j]_ [)] _K_ _r_ +2 1\n\n_s_ [2]\n\n= _\u21d2_ _\u2202zi,j_ log _p_ ( _zi,j_ ) = _[r][ \u2212]_ [1]\n\n[1][)][si][g][n][(] _[z][i][,j]_ [)]\n\n_\u2212_ [2] _[\u221a]_ ~~_[r]_~~ [si][g][n][(] _[z][i][,j]_ [)]\n2 _|zi,j|_ _s_ [2]\n\n2\n\n2 _|_ ~~_[\u221a]_~~ _rzi,j_ _|_ _,_\n~~\ufffd~~ _s_ [2] ~~\ufffd~~\n\n2\n\n2 _|_ ~~_[\u221a]_~~ _rzi,j_ _|_\n\ufffd _s_ [2] \ufffd\n\n_K r\u2212_ 1\n\n_[ \u2212]_ [1]\n\n+ [(] _[r][ \u2212]_ [1][)]\n2 _zi,j_ 2 _zi,j_\n\n[g][n][(] _[z][i][,j]_ [)] _K_ _r_ +2 1\n\n_s_ [2]\n\n= _[r][ \u2212]_ [1]\n\n_[ \u2212]_ [1][)]\n\n_\u2212_ [2] _[\u221a]_ ~~_[r]_~~ [si][g][n][(] _[z][i][,j]_ [)]\n2 _zi,j_ _s_ [2]\n\n2\n\n2 _|_ ~~_[\u221a]_~~ _rzi,j_ _|_ _,_\n~~\ufffd~~ _s_ [2] ~~\ufffd~~\n\n2\n\n_[ \u2212]_ [1]\n\n_\u2212_ [2] _[\u221a]_ ~~_[r]_~~ [si][g][n][(] _[z][i][,j]_ [)]\n_zi,j_ _s_ [2]\n\n_K r\u2212_ 1\n\n= _[r][ \u2212]_ [1]\n\n[g][n][(] _[z][i][,j]_ [)] _K_ _r_ +2 1\n\n_s_ [2]\n\n2\n\n2 _|_ ~~_[\u221a]_~~ _rzi,j_ _|_\n\ufffd _s_ [2] \ufffd\n\n2 _|_ ~~_[\u221a]_~~ _rzi,j_ _|_ _,_\n~~\ufffd~~ _s_ [2] ~~\ufffd~~\n\n2\n\nas required.\n\n32\n\n**C** **EGG Architecture**\n\nIn the following section, we detail the design of our EGG model, which follows the high-level structure of\nmodern pre-layernorm decoder-only language models, but replaces self-attention with a modified minGRU and\nstandard layernorms with a custom variant to enable pure integer training. See Algorithm 2 for an overview of\nthe forward pass of the EGG architecture.\n\n**Algorithm 2** EGG forward pass\n\n**Require:** Input token _t \u2208_ U8, input state _s \u2208_ I _[l]_ 8 _[\u00d7][D]_, network parameters _\u03b8_\n**Ensure:** Output vector _y \u2208_ I _[D]_ 8 [and output state] _[ s][\u2032][ \u2208]_ [I] _[l]_ 8 _[\u00d7][D]_\n_s_ _[\u2032]_ _\u2190_ I _[l]_ 8 _[\u00d7][D]_ initialized to 0\n_y \u2190_ EMBED( _\u03b8_ emb _, t_ )\n**for** _i \u2208{_ 0 _, . . ., l \u2212_ 1 _}_ **do**\n\n_y_ _[\u2032]_ _, s_ _[\u2032]_ _i_ _[\u2190]_ [GRU][(] _[\u03b8]_ [gru] _[,i][,]_ [ LN][(] _[\u03b8]_ [ln1] _[,i][, y]_ [)] _[, s][i]_ [)]\n_y \u2190_ I8(I32( _y_ _[\u2032]_ ) + I32( _y_ ))\n_y_ _[\u2032]_ _\u2190_ MLP( _\u03b8_ mlp _,i,_ LN( _\u03b8_ ln2 _,i, y_ ))\n_y \u2190_ I8(I32( _y_ _[\u2032]_ ) + I32( _y_ ))\n**end for**\n_y \u2190_ LN( _\u03b8_ lnout _,i, y_ )@ _\u03b8_ head _[T]_\n\n**C.1** **Notation and Operations**\n\nWe use the constant _l \u2208_ E [+] to denote the number of layers of the model and _D_ = 4 _[d]_ as the hidden dimension\nof the model, where _d \u2208_ E [+] .\n\nWe use I _n_ to denote an _n_ -bit signed integer and U _n_ to denote an _n_ -bit unsigned integer. We denote casting\nvector _\u20d7x_ to format I _n_ as I _n_ ( _\u20d7x_ ), which implicitly includes clipping to the bounds of the datatype. To ensure\nsymmetry between positive and negative values of each datatype, we consider the value _\u2212_ 2 _[n][\u2212]_ [1] to be invalid\nfor datatype I _n_ ; for instance, for 8-bit signed integers we only allows value from -127 to 127.\n\nWe use the following operations:\n\n    - _\u20d7x_ @ _M_ indicating scaled vector-matrix multiplication of I _[n]_ 8 _[\u00d7]_ [I] _[n,m]_ 8 _\u2192_ I _[m]_ 8 [, corresponding to int8 tensor]\ncore multiplication with int32 accumulation and scaling. The details of this operation are described\nin the next subsection.\n\n    - _a \u00b7 b_ indicates dot product with int32 accumulation, I _[n]_ 8 _[\u00d7]_ [ I] _[n]_ 8 _[\u2192]_ [I][32][, and] _[ a][ \u2299]_ _[b]_ [ indicates the Hadamard]\n(elementwise) product.\n\n    - Standard integer operations: + for addition, _\u2212_ for subtraction, and _\u2299_ for element-wise multiplication.\n\n    - _|x|_ indicates taking the element-wise absolute value of _x_, I _[n]_ _\u2192_ I _[n]_ .\n\n    - sign( _x_ ) indicates taking the element-wise sign of _x_, giving 1 for positive values, -1 for negative\nvalues, and 0 for zero.\n\n   - sum( _x_ ) indicates taking the sum of all elements in _x_ (casting to I32 to prevent overflow): I _[n]_ _\u2192_ I32.\n\n    - _x \u226b_ _n_ indicates an elementwise bitwise right shift by _n_, which is typically equivalent to 2 _[\u2212][n]_ _x_ .\nSimilarly, _x \u226a_ _n_ indicates a bitwise left shift by _n_, which is typically equivalent to 2 _[n]_ _x_ .\n\n    - Square-bracket indexing. For instance _M_ [ _x, y_ ] extracts the element at index _x_ in axis 0 and index _y_\nin axis 1, following the zero-based indexing convention.\n\n**C.2** **Parameter Initialization**\n\nThe standard initialization for matrix parameters in our model is rounding 16 times a sample from the standard\nnormal, and casting to I8. This can be precomputed on a CPU since this is only done once at the start of\ntraining.\n\n33\n\nThe egg model has the following parameters (where an additional subscript of _i_ indicates that there is a version\nof this parameter for each layer of the model):\n\n    - _\u03b8_ emb _\u2208_ I [256] 8 _[\u00d7][D]_, following standard initialization.\n\n    - _\u03b8_ head _\u2208_ I [256] 8 _[\u00d7][D]_, following standard initialization.\n\n    - _\u03b8_ lnout _\u2208_ I _[D]_ 8 [, initialized to 16 for each element.]\n\n    - _\u03b8_ ln1 _,i, \u03b8_ ln2 _,i \u2208_ I _[D]_ 8 [, initialized to 16 for each element]\n\n    - _\u03b8_ mlp _,i,_ 1 _\u2208_ I [4] 8 _[D][\u00d7][D]_ and _\u03b8_ mlp _,i,_ 2 _\u2208_ I _[D]_ 8 _[\u00d7]_ [4] _[D]_, following standard initialization.\n\n    - _\u03b8_ GRU _,i,_ [Wf,Uf,Wh,Uh] _\u2208_ I _[D]_ 8 _[\u00d7][D]_, following standard initialization.\n\n    - _\u03b8_ GRU _,i,_ [bfm bh] _\u2208_ I _[D]_ 8 [, initialized to 0 for each element.]\n\nIn total there are 513 _D_ + _l_ (4 _D_ + 12 _D_ [2] ) parameters in the model.\n\n**C.3** **Matrix Multiplication**\n\nTensor cores in GPUs are able to calculate fast vector-matrix multiplications with int32 accumulation as\n_xM \u2208_ I _[m]_ 32 [where] _[ x][ \u2208]_ [I] _[n]_ 8 [and] _[ M][ \u2208]_ [I] _[n]_ 8 _[\u00d7][m]_ . For our purposes, we define _x_ @ _M_ as a scaled multiplication:\n\n_x_ @ _M_ := I8( _[xM]_\n\n16 ~~_[\u221a]_~~ ~~_n_~~ [)] _[.]_\n\nNote that when _n_ = 4 _[d]_, the division operation just becomes a right-shift by 4 + _d_, which is fast to calculate.\n\nWe choose this scaled matrix multiplication because we initialize _M_ to 16 times standard normal samples for\neach element, so dividing by 16 _[\u221a]_ ~~_n_~~ preserves the magnitude of _x_ for the output. In particular, if all elements\nof _x_ and _M_ are drawn from independently from the standard normal distribution multiplied by 16, the central\nlimit theorem tells us that the expected value per element of the output will be 256 _[\u221a]_ ~~_n_~~ ~~,~~ so dividing by 16 _[\u221a]_ ~~_n_~~\npreserves the standard deviation of 16.\n\n**C.4** **Embedding**\n\nOur embedding function takes as input an embedding matrix _\u03b8_ emb _\u2208_ I [256] 8 _[\u00d7][D]_ and an input token _t \u2208_ U8, and\nsimply outputs the vector corresponding to that token: _\u03b8_ emb[ _t_ ] _\u2208_ I _[D]_ 8 [.]\n\n**C.5** **Layer Normalization (LN)**\n\nOur layer normalization operation involves multiplying our input _x \u2208_ I _[D]_ 8 [with a weight] _[ \u03b8]_ [ln] _[\u2208]_ [I] _[D]_ 8 [before]\ndividing by the mean absolute value of _x_ .\n\nWe decide to divide by the mean absolute value of the input instead of the more more common root-meansquared since square roots are expensive on integers. Note that the _L_ 1 norm after dividing the input by the\nmean absolute value (when using real numbers) is _D_ instead of 1, which we intentionally choose to preserve\nmore bits of information given the limited range of I8.\n\nWe calculate the mean absolute value of input _x_ as:\n\n_x_ mav = I8(sum( _|x|_ ) _\u226b_ (2 _d_ )) _,_\n\nNote that we can safely cast the mean absolute value to an I8 without overflow given the properties of the\nmean of a set, though we lose precision due to truncating the fractional component.\n\nThe output of layernorm is calculated as:\n\nDIVIDE(I16( _x_ ) _\u2299_ I16( _\u03b8_ ln) _, x_ mav) _._\n\nSince division is an expensive operation, we precompute it using a lookup table. Note that the product of two\nI8 values will always remain in the dynamic range of I16, so our lookup table will be of shape 2 [16] _\u00d7_ 2 [8] .\n\n34\n\n**C.6** **MLP**\n\nEach MLP block consists of two weight parameters: _\u03b8_ 1 _\u2208_ I [4] 8 _[D][\u00d7][D]_ and _\u03b8_ 2 _\u2208_ I _[D]_ 8 _[\u00d7]_ [4] _[D]_ . Given an input _x \u2208_ I _[D]_ 8 [,]\nwe calculate the output as:\n\n( _x_ @ _\u03b8_ 1 _[T]_ [)@] _[\u03b8]_ 2 _[T]_ _[.]_\n\nNote that we do not use an activation function, because the @ operation is already nonlinear due to the saturated\nconversion from I32 to I8\n\n**C.7** **GRU**\n\nEach GRU block accepts an input vector and state _x, s_ _\u2208_ I _[D]_ 8 consists of 6 weight parameters:\n_\u03b8_ Wf _, \u03b8_ Uf _, \u03b8_ Wh _, \u03b8_ Uh _\u2208_ I _[D]_ 8 _[\u00d7][D]_ and _\u03b8_ bf _, \u03b8_ bh _\u2208_ I _[D]_ 8 [.]\n\nUsing these weight matrices, we calculate the following vectors:\n\n_f_ = _\u03c3_ (I8(I32( _x_ @ _\u03b8_ Wf _[T]_ [) +][ I][32][(] _[s]_ [@] _[\u03b8]_ Uf _[T]_ [) +][ I][32][(] _[\u03b8]_ [bf][)))] _[,]_\n\n\u02c6\n_f_ = I8(((I32( _f_ ) + 127) _\u2299_ I32( _s_ )) _\u226b_ 8) _,_\n\n\u02c6\n_h_ = _\u03d5_ (I8(I32( _x_ @ _\u03b8_ Wh _[T]_ [) +][ I][32][( \u02c6] _[f]_ [@] _[\u03b8]_ Uh _[T]_ [) +][ I][32][(] _[\u03b8]_ [bh][)))] _[,]_\n\n_h_ = _s_ + I8(((I32( _f_ ) + 127) _\u2299_ (I32( _h_ [\u02c6] ) _\u2212_ I32( _s_ ))) _\u226b_ 8) _,_\n\nwhere _h_ is the output and the new hidden state. In the typical GRU, _\u03c3_ stands for the sigmoid function while _\u03d5_\nstands for the hyperbolic tangent, but we find that setting these as identity operations is sufficient due to the\nnonlinearity already present in the clipped addition. One can view this clipped addition operation as scaled\nand shifted version of the \u201chard\" tanh and sigmoid operators.\n\nTo explain why we perform these operations, we can analyze this relative to the original GRU. The _f_ vector\nfor the standard GRU has all elements between 0 and 1 due to the sigmoid, but our elements are between -127\nand 127. Therefore, to calculate _f_ [\u02c6] (which is typically just _f \u2299_ _s_ ), we first add 127 to _f_, getting the range\nbetween 0 and 254 before multiplying by _s_ before bit-shifting right by 8 again to bring our values back to the\nI8 dynamic range. We apply similar logic to calculate the final _h_, which is typically just _h_ = _s_ + _f \u2299_ ( _h_ [\u02c6] _\u2212_ _s_ )\nbut needs to be rescaled to keep the int8 dynamic range.\n\n**C.8** **Fitness Calculation in Integer Types**\n\nThe \u201cfitness\u201d used in language model pretraining is the log-likelihood of correctly generating the next token,\ntreating the outputs of the language model as logits (unnormalized log probabilities). If _t_ _[\u2032]_ _\u2208_ U8 is the next\ntoken to predict and _y \u2208_ I [256] 8 are the logits, we can calculate the log likelihood as follows:\n\n_y_ _[\u2032]_ = I32( _y_ ) + 128 _,_\n\n_o_ = _y_ _[\u2032]_ [ _t_ _[\u2032]_ ] _\u2212_ LOG2[sum(EXP2[ _y_ _[\u2032]_ ])] _,_\n\nwhere _o_ is the loss for one token. We implement EXP2 and LOG2 as lookup tables, where\n\nEXP2[ _i_ ] = 16 _\u00d7_ 2 _[i/]_ [16] _,_\n\nLOG2[ _i_ ] = 16 _\u00d7_ log2( _i/_ 16) _._\n\nNote that each element in EXP2 for any U8 input requires at most 20 bits, so the sum of exponents across all\npossible choices is at most 28 bits, meaning we have to precompute LOG2 for 2 [28] values.\n\n**D** **EGG Pretraining with Integer EGGROLL**\n\nThe core ideas of EGGROLL still apply in this integer-based training setting, but we have to make some\nmodifications to ensure it only uses integer operations.\n\n35\n\n**D.1** **Adding EGGROLL Perturbations**\n\nFor parameter _\u03b8 \u2208_ I _[m]_ 8 _[\u00d7][n]_ that represents a matrix multiplication, we first sample rank-1 perturbation vectors\nfor each index in the batch: _A \u2208_ I _[m]_ 8 [and] _[ B][ \u2208]_ [I] _[n]_ 8 [. We sample these vectors from the standard random normal]\nmultiplied by 16 and rounded to the nearest I8 (clipping if necessary). To prevent the use of floating-point\narithmetic on the accelerator, we pre-generate a large matrix of these random values, randomly indexing into it\nto get the perturbation vectors.\n\nGiven an input _x \u2208_ I _[n]_ 8 [, instead of calculating] _[ x]_ [@] _[\u03b8][T]_ [, we calculate]\n\nI8( _[x\u03b8][T]_ [ + ][((] _[x][ \u00b7][ B]_ [)][I][32][(] _[A]_ [)] _[ \u226b]_ [(][4 + \u02c6] _[\u03c3]_ [))] ) _._\n\n16 ~~_[\u221a]_~~ ~~_n_~~\n\nThe value of \u02c6 _\u03c3_ is a hyperparameter, related to the _\u03c3_ in the main paper as _\u03c3_ = 2 _[\u2212][\u03c3]_ [\u02c6] . Note that the batched\nforward pass remains efficient since it still simply performs a batched vector-vector dot product in int8 (with\nint32 accumulate) and a batched vector-scalar product in int32.\n\nWe apply this same logic to the embedding matrix, since we can interpret _\u03b8_ [ _t_ ] as one_hot( _t_ ) _\u03b8_ and still apply\nour rank-1 updates in that context. In practice, this means replacing _x \u00b7 B_ with _B_ [ _t_ ].\n\n**D.2** **Fitness Shaping**\n\nWe employ a simple fitness shaping scheme based on antithetical pairs. Specifically, given raw fitnesses\n_s_ [+] _, s_ _[\u2212]_, for the positive and negative sample of the antithetical pair respectively, the transformed fitness for the\nnoise is:\nsign( _s_ [+] _\u2212_ _s_ _[\u2212]_ ) _,_\n\nNote that the only possible values for the fitness after shaping are _{\u2212_ 1 _,_ 0 _,_ 1 _}_ .\n\n**D.3** **Parameter Update**\n\nFor parameter _\u03b8 \u2208_ I _[m]_ 8 _[\u00d7][n]_ that represents a matrix multiplication (or embedding vector), suppose the sampled\nbatch of rank-1 perturbation vectors are _A \u2208_ I _[N]_ 8 _[\u00d7][m]_ and _B \u2208_ I _[N]_ 8 _[\u00d7][n]_, and let the fitnesses after shaping be\n_F \u2208_ I _[N]_ 8 [. Then we calculate an intermediate value] _[ E][ \u2208]_ [I] _[m]_ 32 _[\u00d7][n]_ as:\n\n_E_ = ( _F \u2299_ _A_ ) _[T]_ _B._\n\nWe use _E_ to determine if each element of _\u03b8_ should be increased or decreased. In particular, when the absolute\nvalue of _E_ is above a pre-specified threshold we move _\u03b8_ by one discrete bin in the direction of the sign of\n_E_ . Since there are only 255 unique values for each element in I8, restricting updates to single bins improves\nstability without compromising the ability for a parameter to get to any other value with relatively few\nupdates.\n\nWe currently do not incorporate any momentum or other optimizer states, but this remains critical future work\nto improve the speed of convergence for pure integer training.\n\n**E** **EGG Ablations**\n\n**E.1** **Data Efficient Training**\n\nIn our main experiments, we give a unique training sequence to each antithetical pair. However, after fitness\nshaping only 1 bit is extracted per antithetical pair, so we may be able to reuse training sequences across\nmultiple members of the population. When we let each sequence be used for at most 512 members of the\npopulation, we get the loss curves in Fig. 6. At large population sizes, at 4096 and above, we see similar\nperformance despite using 256 times less data. At small population sizes, we see instability, likely due to the\nfact that reusing the same training sequence across the entire population would lead to \u201coverfitting\u201d to the\ntraining sequence instead of generalizing.\n\n36\n\nData Efficient Pure Integer Pretraining: Test Loss\n\n8\n\n7\n\n10 [5]\n\n10 [4]\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-36-0.png)\n\n6\n\n10 [3]\n\n5\n\n10 [2]\n\n4\n\n10 [1]\n\n0 200 400 600 800 1000\nTraining Step\n\nFigure 6: Test loss curves when reusing each sequence across 512 members of the population. Dashed lines indicate the\noriginal experiments where each antithetical pair receives a different sequence.\n\n**E.2** **Controlling For Data Across Population Sizes**\n\nTo determine whether the benefits of large population sizes are due to receiving more data or more unique\nperturbations, we run an experiment where smaller population sizes receive the same amount of data per step\nas the default for 32768. For instance, for a population size of 2, this means there is just one antithetical\npair of perturbations but each is evaluated across 16384 data sequences. In principle, this means that each\nperturbation could have a cleaner fitness estimate. We plot results for population sizes of 2, 64, 512, 4096, and\n32768 in Fig. 7.\n\nData Controlled Pure Integer Pretraining: Test Loss\n\n10 [5]\n\n8\n\n10 [4]\n\n7\n\n10 [3]\n\n6\n\n10 [2]\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-36-1.png)\n\n10 [1]\n\n5\n\n10 [0]\n\n4\n\n10 [\u22121]\n\n0 200 400 600 800 1000\nTraining Step\n\nFigure 7: Test losses when different population sizes receive the same amount of data.\n\nWe find that maximizing the number of unique perturbations is critical for pretraining performance. Training\nis incredibly unstable at a population size of 2, which is analogous to zeroth order optimization, implying that\npretraining would have been infeasible with prior zeroth order optimization approaches for LLM training or\nother ES works. In particular, Qiu et al. (2025) manages to get satisfactory speed with na\u00efve ES in reasoning\ntasks by reusing the same perturbation hundreds of times and having a small population size, but Fig. 7\ndemonstrates that this strategy would not work for pretraining from scratch.\n\n37\n\n**F** **EGGROLL Speed**\n\nAll timings were done on a single GPU on a GH200 (equivalent to a single H100) for a linear model with\ndimension 8192 in bfloat16, allowing a maximum batch size of 1024. For the graph in Fig. 2a, we pre-generate\nthe noises instead of integrating the noise generation into the forward pass.\n\n100\n\n80\n\n60\n\n40\n\n20\n\nNormalized Training Speeds\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-37-0.png)\n\n0\nEGGROLL PPO OpenES\n\nFigure 8: Relative speed of EGGROLL, when including jax noise regeneration.\n\nIn Fig. 8, we consider the impact of regenerating noises on-the-fly using jax PRNG. The darker area and value\nin parenthesis for EGGROLL and OpenES indicate the speed when regenerating noises on-the-fly, while the\nfull bar indicates the speed when the noises are already generated.\n\nWe regenerate noises on the fly in our primary jax codebase, but pre-generating the EGGROLL perturbations\nbeforehand is also a practical possibility since low-rank perturbations only require a small amount of memory,\nproportional to the square root of the size of the original parameter matrices.\n\n**G** **Experimental Details**\n\n**G.1** **Reinforcement Learning Experiments**\n\nWe present here the hyperparameter ranges we used for hyperparameter optimization, as well as all hyperparameter settings for all the experiments. All RL experiments were run on an NVIDIA L40S GPU. For PPO,\nwe use the same methodology to tune the hyperparameters as we did for OpenES and EGGROLL as described\nin Section 6.2. We report the ranges and the final hyperparameters here. We train PPO agents using Rejax\n(Liesen et al., 2024).\n\n38\n\n|CartPole-v1|Col2|\n|---|---|\n|||\n||EggRoll<br>OpenES<br>PPO|\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-38-1.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-38-5.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-38-9.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-38-12.png)\n\n0 2 4\nSteps 1e8\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.0\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.0\n\n1.75\n\n1.50\n\n1.25\n\n1.00\n\n0.75\n\n0.50\n\n0.25\n\n0.00\n\nPendulum-v1\n\n0 2 4\nSteps 1e8\n\nCraftax Classic\n\n0 2 4\nSteps 1e8\n\nJumanji Snake\n\n0 2 4\nSteps 1e8\n\nNavix DoorKey (8x8)\n\n0 2 4\nSteps 1e8\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.0\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.0\n\n1.00\n\n0.75\n\n0.50\n\n0.25\n\n0.00\n\n\u22120.25\n\n\u22120.50\n\n\u22120.75\n\nBrax Ant\n\n0 2 4\nSteps 1e8\n\nCraftax Symbolic\n\n0 2 4\nSteps 1e8\n\nKinetix Hard Pinball (l)\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.0\n\n0 2 4\nSteps 1e8\n\nBrax Inverted Double Pendulum\n\n1.2\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.0\n\n0 2 4\nSteps 1e8\n\nJumanji Knapsack\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0 2 4\nSteps 1e8\n\nKinetix Thrust Over Ball (s)\n\n1.04\n\n1.02\n\n1.00\n\n0.98\n\n0.96\n\n0 2 4\nSteps 1e8\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.0\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.0\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.0\n\n0 2 4\nSteps 1e8\n\nNavix Dynamic Obstacles Random (6x6)\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.0\n\nBrax Humanoid\n\n0 2 4\nSteps 1e8\n\nJumanji 2048\n\n0 2 4\nSteps 1e8\n\nKinetix Thrust Control Left (m)\n\n0 2 4\nSteps 1e8\n\nNavix FourRooms (8x8)\n\n0 2 4\nSteps 1e8\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-38-0.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-38-2.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-38-3.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-38-4.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-38-6.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-38-7.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-38-8.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-38-10.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-38-11.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-38-13.png)\n\nFigure 9: Comparison of reinforcement learning results: Mean returns for each environment and algorithm across 10 random\nseeds. The returns are evaluated using the mean of the parameters. HPO was conducted for each algorithm/environment\npair. The shaded region is the standard error of the mean.\n\n**G.2** **Multi Agent Reinforcement Learning Experiments**\n\nWe train on three cooperative Multi Particle Environments (MPEs) (Lowe et al., 2017) implemented in\nJaxMARL (Rutherford et al., 2023) with feed-forward networks of width 64 and depth 3, performing Bayesian\nhyperparameter optimization for each environment and algorithm. All runs were executed on NVIDIA\nA100-SXM4-40GB GPUs. We find that the optimal batch size is consistent across algorithms on the same\nenvironment. Figure 11 shows that EGGROLL with rank 1 trains up to 2.4 times faster than OpenES for large\nbatch sizes while staying competitive in performance.\n\n39\n\nCartPole-v1\n\n400\n\n300\n\n200\n\n100\n\n0\nEggRoll OpenES\n\nBrax Inverted Double Pendulum\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-39-1.png)\n\n300\n\n200\n\n100\n\n0\nEggRoll OpenES\n\nCraftax Classic\n\nPendulum-v1\n\nBrax Ant\n\nBrax Humanoid\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-39-2.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-39-3.png)\n\n10000\n\n5000\n\n0\nEggRoll OpenES\n\nCraftax Symbolic\n\n30000\n\n10000\n\n7500\n\n5000\n\n2500\n\n0\nEggRoll OpenES\n\nJumanji 2048\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-39-0.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-39-4.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-39-5.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-39-6.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-39-7.png)\n\n3000\n\n2000\n\n1000\n\n0\nEggRoll OpenES\n\nJumanji Knapsack\n\n15000\n\n10000\n\n5000\n\n0\nEggRoll OpenES\n\nJumanji Snake\n\n600\n\n400\n\n200\n\n0\nEggRoll OpenES\n\nKinetix Thrust Control Left (m)\n\n20000\n\n10000\n\n0\nEggRoll OpenES\n\nKinetix Hard Pinball (l)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-39-8.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-39-9.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-39-10.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-39-11.png)\n\n1500\n\n1000\n\n4000\n\n3000\n\n2000\n\n1000\n\n15000\n\n10000\n\n5000\n\n0\nEggRoll OpenES\n\nNavix Dynamic Obstacles Random (6x6)\n\n500\n\n0\nEggRoll OpenES\n\nKinetix Thrust Over Ball (s)\n\n4000\n\n40000\n\n20000\n\n0\nEggRoll OpenES\n\nNavix DoorKey (8x8)\n\n0\nEggRoll OpenES\n\nNavix FourRooms (8x8)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-39-12.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-39-13.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-39-14.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-39-15.png)\n\n1500\n\n1000\n\n500\n\n0\nEggRoll OpenES\n\n3000\n\n2000\n\n1000\n\n0\nEggRoll OpenES\n\n3000\n\n2000\n\n1000\n\n0\nEggRoll OpenES\n\n400\n\n200\n\n0\nEggRoll OpenES\n\nFigure 10: Comparison of reinforcement learning results: Mean and standard deviation of training time.\n\nTable 1: Hyperparameter Ranges for EGGROLL and OpenES\n\n**Hyperparameter** **Values**\npop_size 512, 1024, 2048, 4096\nn_parallel_evaluations 1, 4, 8\nrank 1, 2, 4\noptimizer adamw, sgd, adam\nlearning_rate 1e-3, 1e-2, 1e-1\nlr_decay 0.995, 0.999, 0.9995, 1.0\nsigma 0.05, 0.2, 0.5\nsigma_decay 0.995, 0.999, 0.9995, 1.0\nrank_transform true, false\ndeterministic_policy true, false\n\n40\n\nTable 2: Hyperparameter Ranges for PPO\n\n**Hyperparameter** **Values**\nclip_eps 0.1, 0.2, 0.3\nent_coef 0, 0.0001, 0.001\ngae_lambda 0.9, 0.95, 0.98\ngamma 0.95, 0.99, 0.995, 0.999\nlearning_rate 0.0001, 0.0003, 0.001\nmax_grad_norm 0.5, 1, 2\nlayer_size 256\nn_layers 3\nnormalize_observations true\nnormalize_rewards false\nnum_envs 64, 128, 256\nnum_epochs 4, 8, 16\nnum_minibatches 16, 32, 64\nnum_steps 64, 128, 256\nreward_normalization_discount 0.99\nskip_initial_evaluation false\nvf_coef 0.5, 0.75, 1\n\nTable 3: CartPole-v1\n\nHyperparameter eggroll open_es\nactivation pqn pqn\ndeterministic_policy false true\nlearning_rate 0.1 0.1\nlr_decay 0.9995 0.9995\nlayer_size 256 256\nn_layers 3 3\nn_parallel_evaluations 1 4\npop_size 2048 512\noptimizer sgd adamw\nrank 4 /\n\nrank_transform false true\nsigma 0.2 0.5\nsigma_decay 0.999 0.9995\n\n**G.3** **Reasoning Fine-tuning Experiments: Countdown**\n\nTable 4: Pendulum-v1\n\nHyperparameter eggroll open_es\nactivation pqn pqn\ndeterministic_policy false true\nlearning_rate 0.01 0.01\nlr_decay 0.995 0.995\nlayer_size 256 256\nn_layers 3 3\nn_parallel_evaluations 1 4\npop_size 4096 4096\noptimizer adam adamw\nrank 4 /\n\nrank_transform false false\nsigma 0.05 0.05\nsigma_decay 0.995 1\n\nWe ran a Bayesian hyper-parameter sweep (Snoek et al., 2012) for both GRPO and EGGROLL and used\nthe best set found to run the experiments in figure 5a. For GRPO we swept over sampling temperature and\nlearning rate, whereas for EGGROLL we swept over the standard deviation of the ES sampling ( _\u03c3_ ) and the\nlearning rate scale. The best hyper-parameters found are detailed on tables 26 (EGGROLL) and 27 (GRPO).\nAll of the experiments run in 8 hours on a NVIDIA H200 GPU.\n\nWe also run an experiment where we increase the number of GPUs to 8 and use a bigger model, RWKV 7g7B,\non the Countdown task, allowing for stronger final performance. Notably, we compare to the results reported\nby Qiu et al. (2025) on Countdown. Figure 12 shows that starting from our significantly weaker model (RWKV\n7g7B v.s. Qwen 2.5-7B), we are able to train to a higher validation accuracy (72.9%), v.s. the ones reported\nfor training with GRPO (52.8%) and Open ES (66.8%). Qiu et al. (2025) do not report the wall clock time or\nthe hardware used for their experiments which makes it difficult to establish a fair comparison.\n\n41\n\nTable 5: brax/ant\n\nHyperparameter eggroll open_es\nactivation pqn pqn\ndeterministic_policy false false\nlearning_rate 0.01 0.1\nlr_decay 0.9995 0.995\nlayer_size 256 256\nn_layers 3 3\nn_parallel_evaluations 1 8\npop_size 2048 512\noptimizer adam adam\nrank 1 /\n\nrank_transform false false\nsigma 0.05 0.05\nsigma_decay 0.9995 0.9995\n\nTable 7: brax/inverted_double_pendulum\n\nHyperparameter eggroll open_es\nactivation pqn pqn\ndeterministic_policy true true\nlearning_rate 0.1 0.1\nlr_decay 1 0.995\nlayer_size 256 256\nn_layers 3 3\nn_parallel_evaluations 1 1\npop_size 2048 4096\noptimizer adam adam\nrank 2 /\n\nrank_transform true true\nsigma 0.5 0.05\nsigma_decay 0.995 1\n\nTable 9: craftax/Craftax-Symbolic-AutoReset-v1\n\nHyperparameter eggroll open_es\nactivation pqn pqn\ndeterministic_policy false false\nlearning_rate 0.01 0.1\nlr_decay 0.999 0.995\nlayer_size 256 256\nn_layers 3 3\nn_parallel_evaluations 1 4\npop_size 512 1024\noptimizer sgd adam\nrank 4 /\n\nrank_transform true false\nsigma 0.05 0.5\nsigma_decay 0.999 1\n\nTable 6: brax/humanoid\n\nHyperparameter eggroll open_es\nactivation pqn pqn\ndeterministic_policy true false\nlearning_rate 0.1 0.1\nlr_decay 1 0.995\nlayer_size 256 256\nn_layers 3 3\nn_parallel_evaluations 8 8\npop_size 4096 1024\noptimizer adam sgd\nrank 1 /\n\nrank_transform true true\nsigma 0.2 0.2\nsigma_decay 0.9995 0.995\n\nTable 8: craftax/Craftax-Classic-Symbolic-AutoReset-v1\n\nHyperparameter eggroll open_es\nactivation pqn pqn\ndeterministic_policy false false\nlearning_rate 0.01 0.001\nlr_decay 0.995 0.995\nlayer_size 256 256\nn_layers 3 3\nn_parallel_evaluations 4 8\npop_size 2048 4096\noptimizer sgd adamw\nrank 1 /\n\nrank_transform false false\nsigma 0.05 0.05\nsigma_decay 1 0.995\n\nTable 10: jumanji/Game2048-v1\n\nHyperparameter eggroll open_es\nactivation pqn pqn\ndeterministic_policy false true\nlearning_rate 0.1 0.01\nlr_decay 1 0.999\nlayer_size 256 256\nn_layers 3 3\nn_parallel_evaluations 4 4\npop_size 1024 1024\noptimizer adamw adamw\nrank 1 /\n\nrank_transform false true\nsigma 0.5 0.05\nsigma_decay 0.9995 0.9995\n\n42\n\nTable 11: jumanji/Knapsack-v1\n\nHyperparameter eggroll open_es\nactivation pqn pqn\ndeterministic_policy false false\nlearning_rate 0.1 0.01\nlr_decay 0.999 1\nlayer_size 256 256\nn_layers 3 3\nn_parallel_evaluations 4 1\npop_size 1024 2048\noptimizer sgd adamw\nrank 4 /\n\nrank_transform true true\nsigma 0.05 0.5\nsigma_decay 1 0.995\n\nTable 13: kinetix/l/hard_pinball\n\nHyperparameter eggroll open_es\nactivation pqn pqn\ndeterministic_policy true true\nlearning_rate 0.01 0.01\nlr_decay 0.995 1\nlayer_size 256 256\nn_layers 3 3\nn_parallel_evaluations 8 1\npop_size 2048 512\noptimizer sgd sgd\nrank 4 /\n\nrank_transform true true\nsigma 0.05 0.5\nsigma_decay 0.999 0.9995\n\nTable 15: kinetix/s/h1_thrust_over_ball\n\nHyperparameter eggroll open_es\nactivation pqn pqn\ndeterministic_policy false false\nlearning_rate 0.1 0.01\nlr_decay 0.995 0.995\nlayer_size 256 256\nn_layers 3 3\nn_parallel_evaluations 1 1\npop_size 512 2048\noptimizer adamw sgd\nrank 1 /\n\nrank_transform true true\nsigma 0.5 0.05\nsigma_decay 0.9995 1\n\n**G.4** **Reasoning Fine-tuning Experiments: GSM8K**\n\nTable 12: jumanji/Snake-v1\n\nHyperparameter eggroll open_es\nactivation pqn pqn\ndeterministic_policy false false\nlearning_rate 0.001 0.001\nlr_decay 0.9995 1\nlayer_size 256 256\nn_layers 3 3\nn_parallel_evaluations 8 1\npop_size 4096 2048\noptimizer adam sgd\nrank 1 /\n\nrank_transform true false\nsigma 0.05 0.2\nsigma_decay 0.9995 1\n\nTable 14: kinetix/m/h17_thrustcontrol_left\n\nHyperparameter eggroll open_es\nactivation pqn pqn\ndeterministic_policy false false\nlearning_rate 0.1 0.001\nlr_decay 0.9995 1\nlayer_size 256 256\nn_layers 3 3\nn_parallel_evaluations 4 1\npop_size 512 1024\noptimizer sgd adam\nrank 4 /\n\nrank_transform true true\nsigma 0.5 0.5\nsigma_decay 1 0.999\n\nTable 16: navix/Navix-DoorKey-8x8-v0\n\nHyperparameter eggroll open_es\nactivation pqn pqn\ndeterministic_policy false false\nlearning_rate 0.01 0.01\nlr_decay 0.9995 1\nlayer_size 256 256\nn_layers 3 3\nn_parallel_evaluations 1 8\npop_size 1024 2048\noptimizer adamw adam\nrank 1 /\n\nrank_transform false true\nsigma 0.05 0.05\nsigma_decay 1 1\n\nWe used the hyper-parameters found for Countdown as a starting point and reduced the learning rates for\nboth GRPO and EGGROLL using linear search until we found the best performing one on the validation set.\n\n43\n\nTable 17: navix/Navix-Dynamic-Obstacles-6x6-Randomv0\n\nHyperparameter eggroll open_es\nactivation pqn pqn\ndeterministic_policy false false\nlearning_rate 0.01 0.01\nlr_decay 0.999 1\nlayer_size 256 256\nn_layers 3 3\nn_parallel_evaluations 4 1\npop_size 512 4096\noptimizer adam adam\nrank 2 /\n\nrank_transform false false\nsigma 0.05 0.2\nsigma_decay 1 0.995\n\nTable 18: navix/Navix-FourRooms-v0\n\nHyperparameter eggroll open_es\nactivation pqn pqn\ndeterministic_policy false false\nlearning_rate 0.01 0.001\nlr_decay 0.999 0.9995\nlayer_size 256 256\nn_layers 3 3\nn_parallel_evaluations 4 4\npop_size 2048 2048\noptimizer sgd adam\nrank 4 /\n\nrank_transform true false\nsigma 0.05 0.05\nsigma_decay 0.9995 0.9995\n\nTable 19: PPO Hyperparameters (Set 1)\n\nHyperparameter CartPole Pendulum Ant Humanoid IDP CraftaxClassic CraftaxSymbolic Game2048\nactivation pqn pqn pqn pqn pqn pqn pqn pqn\nclip_eps 0.2 0.1 0.2 0.3 0.1 0.2 0.2 0.3\nent_coef 0.0001 0.001 0 0.0001 0.0001 0.0001 0 0.001\ngae_lambda 0.9 0.95 0.95 0.9 0.98 0.98 0.9 0.9\ngamma 0.995 0.999 0.995 0.95 0.99 0.95 0.95 0.99\nlearning_rate 0.0003 0.0003 0.0003 0.0001 0.001 0.001 0.0003 0.0003\nmax_grad_norm 0.5 1 0.5 2 2 2 2 2\nlayer_size 256 256 256 256 256 256 256 256\nn_layers 3 3 3 3 3 3 3 3\nnormalize_obs true true true true true true true true\nnormalize_rew false false false false false false false false\nnum_envs 256 256 64 256 64 128 256 64\nnum_epochs 4 16 8 4 4 4 4 8\nnum_minibatches 32 16 32 64 64 32 32 16\nnum_steps 128 256 128 64 128 128 64 64\nrew_norm_discount 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99\nskip_initial_eval false false false false false false false false\nvf_coef 0.5 1 1 0.75 1 0.5 0.75 0.75\n\nOur experiments for GSM8K run on 8 NVIDIA H200 GPUS for 8 hours each. We also increase the standard\ndeviation, _\u03c3_, parameter for ES (from 7 _\u00d7_ 10 _[\u2212]_ [4] to 2 _\u00d7_ 10 _[\u2212]_ [3] ) as the significantly bigger population sizes (8096\nv.s. 512) allow for much more stable training and aggressive exploration.\n\n44\n\nTable 20: PPO Hyperparameters (Set 2)\n\nHyperparameter Knapsack Snake HardPinball ThrustLeft ThrustBall DoorKey DynamicObs FourRooms\nactivation pqn pqn pqn pqn pqn pqn pqn pqn\nclip_eps 0.1 0.3 0.1 0.2 0.2 0.1 0.1 0.1\nent_coef 0.0001 0.001 0.0001 0.0001 0.0001 0.0001 0.001 0.001\ngae_lambda 0.9 0.95 0.9 0.9 0.95 0.98 0.98 0.9\ngamma 0.99 0.999 0.99 0.995 0.999 0.95 0.999 0.99\nlearning_rate 0.0001 0.0001 0.0001 0.0001 0.0001 0.0003 0.001 0.001\nmax_grad_norm 0.5 0.5 1 2 0.5 0.5 1 1\nlayer_size 256 256 256 256 256 256 256 256\nn_layers 3 3 3 3 3 3 3 3\nnormalize_obs true true true true true true true true\nnormalize_rew false false false false false false false false\nnum_envs 256 128 256 256 64 64 128 256\nnum_epochs 4 4 16 16 16 16 4 8\nnum_minibatches 64 16 16 32 16 64 16 32\nnum_steps 128 128 64 128 64 256 128 256\nrew_norm_discount 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99\nskip_initial_eval false false false false false false false false\nvf_coef 0.75 0.75 0.5 0.5 0.5 0.75 0.5 0.75\n\nSimple Reference\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-44-2.png)\n\n0 2 4\nSteps 1e8\n\nIPPO OpenES EGGROLL\n(Batch Size = 4096)\n\nSimple Speaker Listener\n\n0 2 4\nSteps 1e8\n\nIPPO OpenES EGGROLL\n(Batch Size = 512)\n\n\u221240\n\n\u221250\n\n\u221260\n\n2.0\n\n1.5\n\n1.0\n\n0.5\n\n0.0\n\n\u221240\n\n\u221260\n\n\u221280\n\n15\n\n10\n\n5\n\n0\n\nSimple Spread\n\n0 2 4\nSteps 1e8\n\nIPPO OpenES EGGROLL\n(Batch Size = 128)\n\n\u221250\n\n\u2212100\n\n\u2212150\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-44-0.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-44-1.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-44-3.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-44-4.png)\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-44-5.png)\n\nFigure 11: Training curves and wall clock times for cooperative Multi Particle Environments. Hyperparameter optimization\nyielded equal batch sizes for all algorithms on the same environment. All EGGROLL runs used rank 1 perturbations.\nShaded regions are standard errors of mean values.\n\nTable 21: Hyperparameter Ranges Used in MPE Sweeps\nfor EGGROLL and OpenES\n\nHyperparameter Values\nactivation pqn, tanh\npop_size 128, 512, 1024, 2048, 4096\nlearning_rate 0.01, 0.05, 0.1, 0.5\nlr_decay 0.3, 0.7, 1.0\nsigma 0.1, 0.2, 0.3, 0.4, 0.5\nrank_transform true, false\n\nTable 22: Hyperparameter Ranges Used in MPE Sweeps\nfor IPPO\n\nHyperparameter Values\nactivation relu, tanh\npop_size 128, 512, 1024, 2048, 4096\nlearning_rate 5e-5, 1e-4, 2.5e-4, 1e-3\nentropy_coef 0.001, 0.005, 0.01\n\n45\n\nTable 23: MPE Simple Spread v3\n\nHyperparameter eggroll open_es ippo\nactivation tanh tanh tanh\n\ndeterministic_policy true true false\nlearning_rate 0.01 0.01 0.001\nlr_decay 0.7 0.7 linear\nlayer_size 64 64 64\nn_layers 3 3 3\npop_size 128 128 128\noptimizer adamw adamw adam\nrank 1 1 \nrank_transform false false sigma 0.5 0.5 n_minibatches - - 4\nupdate_epochs - - 4\ngamma - - 0.99\ngae_lambda - - 0.95\nepsilon_clip - - 0.2\nentropy_coef - - 0.01\nvalue_coef - - 0.5\nmax_grad_norm - - 0.5\n\nTable 24: MPE Simple Speaker Listener v4\n\nHyperparameter eggroll open_es ippo\nactivation tanh tanh relu\n\ndeterministic_policy true true false\nlearning_rate 0.01 0.01 0.001\nlr_decay 0.7 0.3 linear\nlayer_size 64 64 64\nn_layers 3 3 64\npop_size 512 512 512\noptimizer adamw adamw adam\nrank 1 1 \nrank_transform true true sigma 0.5 0.5 n_minibatches - - 4\nupdate_epochs - - 4\ngamma - - 0.99\ngae_lambda - - 0.95\nepsilon_clip - - 0.2\nentropy_coef - - 0.005\nvalue_coef - - 0.5\nmax_grad_norm - - 0.5\n\nTable 25: MPE Simple Reference v3\n\nHyperparameter eggroll open_es ippo\nactivation pqn tanh relu\ndeterministic_policy true true false\nlearning_rate 0.01 0.01 0.001\nlr_decay 0.3 0.3 linear\nlayer_size 64 64 64\nn_layers 3 3 3\npop_size 4096 4096 4096\noptimizer adamw adamw adam\nrank 1 1 \nrank_transform false true sigma 0.1 0.3 n_minibatches - - 4\nupdate_epochs - - 4\ngamma - - 0.99\ngae_lambda - - 0.95\nepsilon_clip - - 0.2\nentropy_coef - - 0.01\nvalue_coef - - 0.5\nmax_grad_norm - - 0.5\n\n46\n\nHyperparameter Value\n\nModel RWKV 7g1.5B\nOptimizer Gradient descent\nES standard deviation _\u03c3_ 7 _\u00d7_ 10 _[\u2212]_ [4]\n\nRank _r_ 1\n\nLearning-rate scale _\u03b7_ scale 0.125\nPopulation size 256\nParallel generations per GPU 1536\nPrompts per epoch 6\nGeneration / thinking length 1000 tokens\nTrain / val temperature 0 / 0\nParallel validations 128\n\nTable 26: Key hyperparameters for EGGROLL training on Countdown with FastRWKV-7g1.5B.\n\nHyperparameter Value\n\nModel RWKV 7g1.5B\nOptimizer Radam\nLearning rate _\u03b7_ 3 _\u00d7_ 10 _[\u2212]_ [6]\n\nGenerations per prompt _G_ 8\nParallel generations per GPU 64\nPrompts per epoch 8\nGeneration length 1000 tokens\nNumber of minibatches 4\n\nPPO clip parameter _\u03f5_ clip 0.2\nTrain / val temperature 1 / 0\nParallel validations 128\n\nTable 27: Key hyperparameters for GRPO training on Countdown with AssociativeScanRWKV-7g1.5B.\n\n0.7\n\n0.6\n\n0.5\n\n0.4\n\n0.3\n\n![](papers/images/Evolution-Strategies-at-the-Hyperscale.pdf-46-0.png)\n\n0.2\n\n0 100 200 300 400 500\n\nEpoch\n\nEGGROLL: RWKV-g0:7g7B\n\nFigure 12: Validation score training curve of an RWKV 7g7B model on countdown using 8 GPUS and a population size of\n8096. Notably, we are able to outperform a stronger base model trained with GRPO and OpenES.\n\n47\n\nHyperparameter Value\n\nModel RWKV 7g7B\nES standard deviation _\u03c3_ 2 _\u00d7_ 10 _[\u2212]_ [3]\n\nRank _r_ 1\n\nLearning-rate scale _\u03b7_ scale 0.06\nGenerations per prompt _G_ 512\nParallel generations per GPU 1024\nTotal parallel generations 8192\nPrompts per epoch 16\nGeneration length 1000 tokens\nNoise reuse factor 1\n\nFreeze non-LoRA params True\nTrain / val temperature 0 / 0\nParallel validations 128\n\nTable 28: Key hyperparameters for multi-GPU EGGROLL training on GSM8K with FastRWKV-7g7B.\n\nHyperparameter Value\n\nModel RWKV 7g7B\nLearning rate _\u03b7_ 1 _\u00d7_ 10 _[\u2212]_ [6]\n\nGenerations per prompt _G_ 8\nParallel generations per GPU 32\nTotal parallel generations 256\nPrompts per epoch 32\nGeneration length 1000 tokens\nNumber of minibatches 16\n\nNumber of workers (processes) 8\nPPO clip parameter _\u03f5_ clip 0.2\nTrain / val temperature 1 / 0\nParallel validations 128\n\nTable 29: Key hyperparameters for multi-GPU GRPO training on GSM8K with AssociativeScanRWKV-7g7B.\n\n48"}, {"role": "assistant", "content": "**Analogy:** Imagine you and your friends are trying to find the best way to make a giant sandcastle. Instead of everyone carefully measuring every grain of sand (which is super slow!), you each try making tiny, quick changes to your own sandcastle - like adding a bucket here or smoothing a wall there. Then, you look at all the sandcastles and learn from the changes that made the best ones. The computer learns like this, making lots of tiny, simple 'guesses' very fast. Also, instead of counting with tricky tiny bits of sand, it only uses whole, easy-to-count rocks (like 1, 2, 3) so it's much faster and needs less energy, like using LEGO bricks instead of messy clay.\n\n**Explanation:** This paper shows how we can teach smart computer programs to do things, like understanding stories or playing games, much faster and using less electricity. They do this by making the computer count with simple whole numbers, like the numbers we use to count our fingers, instead of really long, tricky numbers with lots of tiny pieces. And for learning, instead of complex calculations, the computer makes many small, quick 'tries' (like our sandcastle example) to see if it's getting better at its job. If a 'try' makes it better, it keeps that change! This makes learning quicker and cheaper."}]}
